\documentclass[11pt,a4paper,openright,oneside]{book}
\usepackage{amsfonts, amsmath, amssymb,latexsym,amsthm, mathrsfs, enumerate}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{epsfig}
\usepackage{csquotes}
\usepackage{biblatex}

\addbibresource{refs.bib}
\addbibresource{man-refs.bib}

\parskip=5pt
\parindent=15pt
\usepackage[margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{parskip}

\setcounter{page}{0}



\numberwithin{equation}{section}
\newtheorem{defn0}{Definition}[chapter]
\newtheorem{prop0}[defn0]{Proposition}
\newtheorem{thm0}[defn0]{Theorem}
\newtheorem{lemma0}[defn0]{Lemma}
\newtheorem{corollary0}[defn0]{Corollary}
\newtheorem{example0}[defn0]{Example}
\newtheorem{remark0}[defn0]{Remark}
\newtheorem{conjecture0}[defn0]{Conjecture}

\newenvironment{definition}{ \begin{defn0}}{\end{defn0}}
\newenvironment{proposition}{\bigskip \begin{prop0}}{\end{prop0}}
\newenvironment{theorem}{\bigskip \begin{thm0}}{\end{thm0}}
\newenvironment{lemma}{\bigskip \begin{lemma0}}{\end{lemma0}}
\newenvironment{corollary}{\bigskip \begin{corollary0}}{\end{corollary0}}
\newenvironment{example}{ \begin{example0}\rm}{\end{example0}}
\newenvironment{remark}{ \begin{remark0}\rm}{\end{remark0}}
\newenvironment{conjecture}{\begin{conjecture0}}{\end{conjecture0}}

\newcommand{\defref}[1]{Definition~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\exref}[1]{Example~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}
\newcommand{\conjref}[1]{Conjecture~\ref{#1}}


\DeclareMathOperator{\vectorize}{vec}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\unfolding}{unfold}
\DeclareMathOperator{\IN}{IN}
\DeclareMathOperator{\OUT}{OUT}
\DeclareMathOperator{\TNS}{TNS}

% --------------------------------------------------
\usepackage{fancyhdr}

\lhead{}
\lfoot{}
\rhead{}
\cfoot{}
\rfoot{\thepage}

\begin{document}

\bibstyle{plain}

\thispagestyle{empty}

\begin{titlepage}
\begin{center}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=6cm]{matematiquesinformatica-pos-rgb.png}
\end{center}
\end{figure}

\vspace*{1cm}
\textbf{\LARGE GRAU DE MATEM\`{A}TIQUES } \\
\vspace*{.5cm}
\textbf{\LARGE Treball final de grau} \\

\vspace*{1.5cm}
\rule{16cm}{0.1mm}\\
\begin{Huge}
\textbf{OPTIMAL LOW-RANK APPROXIMATION USING TENSOR NETWORK STRUCTURE SEARCH} \\
\end{Huge}
\rule{16cm}{0.1mm}\\

\vspace{1cm}

\begin{flushright}
\textbf{\LARGE Autor: Aran Roig}

\vspace*{2cm}

\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ll}
\textbf{\Large Director:} & \textbf{\Large Dr. Nahuel Statuto} \\
\textbf{\Large Realitzat a:} & \textbf{\Large  Departament de Matemàtiques   } \\
 & \textbf{\Large i Informàtica} \\
\\
\textbf{\Large Barcelona,} & \textbf{\Large \today }
\end{tabular}

\end{flushright}

\end{center}



\end{titlepage}


\newpage
\pagenumbering{roman} 

\section*{Abstract}

Goldbach's weak conjecture asserts that every odd integer greater than 5 is the sum of three primes. We study that problem and the proof of it presented by H. A. Helfgott and D. Platt. We focus on the circle method. Finally, we describe a computation that confirms Goldbach's weak conjecture up to $10^{28}$.
With the misco's theorem, we can achieve great results from approximating all digits of $\pi^2$

\section*{Resum}
La conjectura feble de Goldbach afirma que tot nombre enter imparell major que 5 \'es la suma de tres nombres primers. En aquest treball estudiem aquest problema i la seva prova presentada per HA Helfgott i D. Platt. Ens centrem en el m\`etode del cercle. Finalment, describim un c\`alcul que confirma la conjectura feble de Goldbach fins a $10^{28}$.
Amb el teorema d'en Misco, podem aproximar i dir clarament quines són les normes del servidor de Discord


% TODO: Omplir això
% 15A69 - Multilinear Algebra, Tensor Calculus, Graph Theory, no se
{\let\thefootnote\relax\footnote{2020 Mathematics Subject Classification. 11G05, 11G10, 14G10}}



\newpage 


\section*{Agra\"{\i}ments}

Vull agrair a ... 
\newpage

\tableofcontents

\newpage

\pagenumbering{arabic} 
\setcounter{page}{1}
\chapter{Introduction}

A well known problem in science and engineering is to retrieve a function given some data. It may be
for example the solution of a partially differentiable equation given some boundary conditions or initial data or
even a target function to be learned from some training set data. \cite{yeTensorNetworkRanks2019}

Explicar que normalment es que les funcions que resulten d'aixo segurament viuen en un espai molt gran, segurament
amb una dimensió molt alta i que una cosa
que s'hi sol fer es assumir per exemple que la nostra funció és pot reescriure com una de low-rank, és a dir, que es pot escriure com la suma
d'unes altres funcions.

A partir d'això entren en joc un concepte originat de part de la física, les tensor networks. Les tensor networks no són més
que representacions de tensors, normalment d'ordre alt, que es poden recuperar a partir de contraure diferents tensors d'ordres més
petits. Resulta que les tensor networks, si representem les dades de la nostra funció com un tensor d'ordre alt, no acaben sent més que
aproximacions low-rank del nostre problema.

L'objectiu final d'aquest treball es intentar donar una solució óptima a aquest problema utilitzant xarxes tensorials i després mostrar
alguna aplicació en diferents camps de la física i del machine learning.

En resum hauria d'explicar el problema sencer

\section{Objectives}

\section{Thesis structure}

\section{Preliminaries}


\begin{definition}[Graph]
    \normalfont{\cite{wilsonIntroductionGraphTheory2009}}
    A \textbf{graph} $G$ is defined as a tuple $G = (V,E)$ where $V = V(G)$ is a set of elements called \textbf{vertices} and 
    $E = E(G) \subset \{\{u, v\} : u, v \in V\}$ is a set of elements called \textbf{edges}.
\end{definition}

\begin{definition}[Directed Graph]
    A \textbf{directed graph} $D$ is a tuple $D = (V, \bar{E})$ where $V = V(D)$ are its vertices and
    $\bar{E} = \bar{E}(D) \subset \{(u,v) : u,v \in V\}$
\end{definition}

\begin{definition} Given a directed graph $G=(V,\bar{E})$ and a vertex $i \in V$ we define
    $$\IN(i) = \{j \in V : (j, i) \in \bar{E}\} \qquad \OUT(i) = \{j \in V : (i, j) \in \bar{E} \}$$
\end{definition}
\iffalse
\begin{definition}[Graph isomorphisms]
    We say that two graphs $G, H$ are \textbf{isomorphic} if 
\end{definition}


\begin{definition}[Outer product]
    We define the \textbf{outer product} of $n$ vectors 
    $a_1 \in \mathbb{R}^{N_1}, a_2 \in \mathbb{R}^{N_2}, \dots, a_n \in \mathbb{R}^{N_n}$
    as the $n$th-order tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times N_2 \times \dots \times N_n}$ with its entries being
    $$\mathcal{T}(i_1, \dots, i_n) = a_1(i_1) a_2(i_2) \cdots a_n(i_n)$$
\end{definition}

\begin{example}
The outer product of two vectors $a \in \mathbb{R}^I$ and $b \in \mathbb{R}^J$ is denoted by $a \circ b$ and it results
as a matrix $M = a \circ b \in \mathbb{R}^{I \times J}$ with its entries defined as $M_{ij} = a_i b_j$.
\end{example}


\begin{definition}[Inner product]
The inner product of two tensors $\mathcal{X}, \mathcal{Y} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ is defined by
$$\langle \mathcal{X},\mathcal{Y} \rangle = \sum_{i_1, \dots, i_N}^{N_1, \dots, N_n} \mathcal{X}_{i_1, \dots, i_n} \mathcal{Y}_{i_1, \dots, i_n} = 
\vectorize(\mathcal{X})^T \vectorize(\mathcal{Y}) = \langle \vectorize(\mathcal{X}), \vectorize(\mathcal{Y}) \rangle$$
\end{definition}


\begin{definition}[Kronecker product]
    \normalfont{\cite{panagakisTensorMethodsComputer2021}} Given two matrices $A \in \mathbb{R}^{N_1 \times N_2}$ and $B \in \mathbb{R}^{M_1 \times M_2}$,
    their kronecker product is defined as the matrix $A \otimes B \in \mathbb{R}^{N_1 \cdot M_1 \times N_2 \cdot M_2}$ with
    $$A \otimes B = \begin{bmatrix}
        a_{11}B & \cdots & a_{1N_2}B \\ 
        \vdots & \ddots & \vdots \\
        a_{N_1 1}B & \cdots & a_{N_1 N_2}B \\
    \end{bmatrix}$$
\end{definition}

\begin{definition}[Khatri-Rao product]
    \normalfont{\cite{panagakisTensorMethodsComputer2021}} Given two matrices $A \in \mathbb{R}^{N \times R}$ and $B \in \mathbb{R}^{M \times R}$ their
Khatri-Rao, also known as column-wise Kronecker product is defined as $A \odot B \in \mathbb{R}^{N \cdot M \times R}$
    $$ A \odot B = \begin{bmatrix} A_{:,1} \otimes B_{:,1} & A_{:,2} \otimes B_{:,2} & \cdots & A_{:,R} \otimes B_{:,R}  \end{bmatrix}$$
        

\end{definition}

\begin{definition}[Frobenius norm]
The \textbf{Frobenius norm} of a tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ is given by
$$\|\mathcal{T}\|_F = \sqrt{\langle \mathcal{T}, \mathcal{T} \rangle} = \sqrt{\sum_{i_1, \dots, i_n}^{N_1, \dots, N_n}
\mathcal{T}_{i_1 \dots i_n}^2}$$
\end{definition}

\begin{definition}[Tensor transposition]
    \normalfont{\cite{zhengFullyConnectedTensorNetwork2021}} Let $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$
    be an $n$th-order tensor and $p$ a permutation of the vector $(1, 2, \dots, n)$. We define the \textbf{vector $p$ based tensor
    transposition of $\mathcal{T}$} as the tensor $\overrightarrow{\mathcal{T}_p} \in \mathbb{R}^{N_{p_1} \times \cdots \times N_{p_n}}$ with its entries defined as follows:
    $$\overrightarrow{\mathcal{T}_p}(i_1, i_2, \cdots, i_n) = (i_{p_1}, i_{p_2}, \cdots, i_{p_n})$$
\end{definition}

\begin{definition}[Tensor contraction]
    \normalfont{\cite{zhengFullyConnectedTensorNetwork2021}} Suppose that $p$ and $q$ are reorderings of the vectors
    $(1,2,\dots,n)$ and $(1,2,\dots,m)$ respectively, and let ${\mathcal{X} \in \mathbb{R}^{N_1 \times \cdots \times N_n}}$ 
    and $\mathcal{Y} \in \mathbb{R}^{M_1 \times \cdots \times M_m}$ two tensors with $N_{p_i} = M_{q_i}$ for all $i = 1,2,\dots,d$
    with $d \leqslant \min{(n, m)}$. We define the tensor contraction along the $p_{1:d}$-modes of $\mathcal{X}$ and the $q_{1:d}$-modes
    of $\mathcal{Y}$ as the tensor $\mathcal{Z}$ of order $n + m - 2d$
$$\mathcal{Z} = \mathcal{X} \times_{p_{1:d}}^{q_{1:d}} \mathcal{Y} \in \mathbb{R}^{N_{p_{d+1}} \times \cdots \times N_{p_{n}} \times N_{q_{d+1}} \times \cdots \times N_{q_m}}$$
whose elements are defined by:
$$\mathcal{Z}(i_{p_{d+1}}, \cdots, i_{p_n}, j_{q_{d+1}}, \cdots, j_{q_m}) = $$$$ \sum_{i_{p_1} = 1}^{N_1} \sum_{i_{p_2} = 1}^{N_2} \cdots \sum_{i_{p_d} = 1}^{N_d}
\overrightarrow{\mathcal{X}_p}(i_{p_1}, \cdots, i_{p_d}, i_{p_{d+1}}, \cdots, i_{p_n}) \overrightarrow{\mathcal{Y}_q}(i_{p_1}, \cdots, i_{p_d}, j_{q_{d+1}}, \cdots, j_{q_m})$$
\end{definition}



\fi



\chapter{Tensors}

In this chapter we will introduce the basics of tensor algebra and we will present some results that will be useful in the
following chapters

We will denote $\mathbb{V}_1, \dots, \mathbb{V}_n$ as finite vector spaces over a field $\mathbb{K}$ ($\mathbb{C}$ if unspecified) of dimension $\dim{\mathbb{V}_i} = N_i \; \forall i = 1, \dots, n$.
We denote $\mathcal{B}_1, \dots, \mathcal{B}_n$ with ${\mathcal{B}_i = \{e_1^{(i)}, \dots, e_{N_i}^{(i)}\}, i = 1, \dots, n}$ the canonical basis of $\mathbb{V}_1, \dots, \mathbb{V}_n$

\begin{definition}[Tensor]
    A \textbf{tensor} is a multilineal map ${T: \mathbb{V}_1 \times \dots \times \mathbb{V}_n \rightarrow \mathbb{K}}$
\end{definition}

\begin{definition}[Order of a tensor] Given a tensor $T \in \mathbb{V}_1 \times \cdots \times \mathbb{V}_n \rightarrow \mathbb{K}$ We define the \textbf{order} of the tensor $T$ as $n$.
\end{definition}

We will allow that some $\mathbb{V}_i$ are dual spaces of some other vector spaces $\mathbb{W}_i$, i.e $\mathbb{V}_i = \mathbb{W}_i^*$.
\begin{definition}
    Let $T \in \mathbb{V}_1 \times \cdots \times \mathbb{V}_p \times \mathbb{W}_1^* \times \cdots \times \mathbb{W}_q^* \rightarrow \mathbb{K}$.
    We say that the tensor $T$ is $p$-times covariant and $q$-times contravariant.
\end{definition}

\begin{definition}[Tensor product] Let $L$ be the vector space generated by the base ${V_1 \times \dots \times V_n}$, i.e the
    set of linear combinations of the elements $(v_1, \dots, v_n), v_i \in \mathbb{V}_i$. Let $\mathcal{R}$ be the linear subspace of
    $L$ generated by the relation $R$ defined by:
    $$(v_1, \dots, \alpha v_i, \dots, v_n) \sim \alpha(v_1, \dots, v_n) \; \forall i = 1, \dots, n, \forall \alpha \in \mathbb{K}$$
    $$(v_1, \dots, v_i + u_i, \dots, v_n) \sim (v_1, \dots, v_i \dots, v_n) + (v_1, \dots, u_i, \dots, v_n) \; \forall i = 1, \dots, n$$
    The tensor product $\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n$ is defined as the quotient $L / \mathcal{R}$ and its called 
    \textbf{tensor product space}. The image of
    $(v_1, \dots, v_n)$ by the quotient is denoted by $v_1 \otimes \dots \otimes v_n$
\end{definition}

The following theorem gives us a correspondance between each tensor ${T: \mathbb{V}_1 \times \dots \times \mathbb{V}_n \rightarrow \mathbb{K}}$
and each element of $\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n$:

\begin{theorem}[Universal property of the tensor product]
    \normalfont{\cite{romanTensorProducts2008}}
    The tensor product of two vector spaces $\mathbb{V} \otimes \mathbb{W}$
    for every bilinear map $h: \mathbb{V} \times \mathbb{W} \rightarrow X$ there exists an unique bilinear map $\tilde{h}: \mathbb{V} \otimes
    \mathbb{W} \rightarrow X$ such that the following diagram commutes:

    \centering
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJWXFx0aW1lcyBXIl0sWzEsMCwiViBcXG90aW1lcyBXIl0sWzEsMSwiWCJdLFswLDEsIlxcdmFycGhpIl0sWzAsMiwiaCIsMl0sWzEsMiwiXFx0aWxkZSBoIl1d
\begin{tikzcd}
	{\mathbb{V}\times \mathbb{W}} & {\mathbb{V} \otimes \mathbb{W}} \\
	& X
	\arrow["\varphi", from=1-1, to=1-2]
	\arrow["h"', from=1-1, to=2-2]
	\arrow["{\tilde h}", from=1-2, to=2-2]
\end{tikzcd}

\end{theorem}

We can construct a basis for $\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n$. We define
$$\mathcal{B}_{\otimes} = \{e_{i_1}^{(1)} \otimes \cdots \otimes e_{i_n}^{(n)} : 1 \leqslant i_j \leqslant N_j, 1 \leqslant j \leqslant n\}$$
Where $\{e_1^{(i)}, e_2^{(i)}, \dots, e_{N_i}^{(i)}\}$ is the canonical basis for $\mathbb{V}_i$.
Constructed this way, $\mathcal{B}_\otimes$ is a (canonical) basis of ${\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n}$.

\begin{remark} \label{rem:tenbase} The dimension
    of ${\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n}$ is ${\dim{\mathbb{V}_1} \cdot \dim{\mathbb{V}_2} \cdots \dim{\mathbb{V}_n}}$ and its elements can be expressed as
    \begin{equation} \label{eq:base-representation}
T = \sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} T_{s_1, \dots, s_n} \cdot  e_{s_1}^{(1)} \otimes \cdots \otimes e_{s_n}^{(n)}
\end{equation}
\end{remark}


% (POSAR COSES DEL KROENKER PRODUCT, PRESENTARLO)
\begin{definition}[Tensor product] Given two tensors $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ and ${U \in \mathbb{W}_1 \otimes \cdots \otimes
    \mathbb{W}_m}$ with $\dim \mathbb{V}_i = N_i$, $i = 1, \dots, n$ and $\dim \mathbb{W}_j = M_j$, $j = 1, \dots, m$ 
    and canonical basis $\{v_1^{(i)}, \dots, v_{N_i}^{(i)}\}$ for each $\mathbb{V}_i$ and $\{w_1^{(j)}, \dots, w_{M_j}^{(j)}\}$ for each $\mathbb{W}_j$
    we define the tensor product
    $T \otimes U$ as
    $$ T \otimes U = \sum_{i_1, \dots, i_n}^{N_1, \dots, N_n} \sum_{j_1, \dots, j_m}^{M_1, \dots, M_m} T_{i_1, \dots, i_n} U_{j_1, \dots, j_m} \cdot
    v_{i_1}^{(1)} \otimes \cdots \otimes v_{i_n}^{(n)} \otimes w_{j_1}^{(1)} \otimes \cdots \otimes w_{j_m}^{(m)}$$
    Note that $T \otimes U \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \otimes \mathbb{W}_1 \otimes \cdots \otimes \mathbb{W}_m$

\end{definition}

From remark \ref{rem:tenbase}, a tensor $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ can be identified as a "$n$-dimensional array" $\mathcal{T}$, i.e,
a discrete function
$$\begin{align}
    \mathcal{T}: \prod_{i=1}^n \{1, \dots, N_i\} & \longrightarrow \mathbb{K} \\
    T(i_1, \dots, i_n) & \longmapsto T_{i_1, \dots, i_n}
\end{align}$$

From now on, we will write the discrete function $\mathcal{T}$ as
an element of ${\mathbb{K}^{N_1 \times \cdots \times N_n}}$ \cite{yokotaVeryBasicsTensors2024}.
of $T$ as $\mathcal{T}$. Now we will define some definitions from the underlying definition of the tensor viewed as a discrete function:

Now we will define some definitions that will help us establish a formalized method of mapping $n$-dimensional arrays with
vectors and matrices:

\begin{definition}[Linearization]
    Fixed $N_1, \dots, N_n \in \mathbb{N}$, given $i_1, \dots, i_n \in \mathbb{N}$ such that $1 \leqslant i_1, \leqslant N_1, \dots, 1 \leqslant i_n \leqslant N_n$,
    we define the Linearization of the indices $i_1, \dots, i_n$ as the mapping ${\prod_{i=1}^n \{1, \dots, N_i\} \rightarrow \{1, \dots, \prod_{i=1}^n N_i\}}$:
    $$\overline {i_1, i_2, \dots, i_n} = \sum_{j=2}^{n} \left( (i_j - 1) \prod_{k=1}^j N_k \right) + i_1$$
\end{definition}

\begin{lemma}
    The linearization mapping is bijective
\end{lemma}

\begin{definition}[Vectorization]
    Given a tensor $\mathcal{T} \in \mathbb{K}^{N_1 \times \cdots \times N_n}$, we define the \textbf{vectorization} of $\mathcal{T}$
    as the first order tensor $\mathcal{V} \in \mathbb{K}^{N_1 N_2 \cdots N_n}$ defined by:
    $$\mathcal{V}(\overline{i_1 i_2 \dots i_n}) = \mathcal{T}(i_1, i_2, \dots, i_n)$$
    We will write $\mathcal{V} = \vectorize{\mathcal{T}}$
\end{definition}


\begin{definition}[Tensor unfolding]
Let $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$, $n \geqslant 2$, $1 \leqslant d \leqslant n$ and $p$ a permutation of the vector $(1,2,\dots, n)$. We define the
\textbf{generalized unfolding} of the tensor $\mathcal{T}$ as the $2$nd-order tensor 
$\mathcal{U} \in \mathbb{R}^{\prod_{i=1}^d N_{p_i} \times \prod_{i=d+1}^n N_{p_i}}$:
$$ \mathcal{U} (\overline{i_{p_1}, \dots, i_{p_d}}, \overline{i_{p_{d+1}}, \dots, i_{p_n}}) = \mathcal{T}(i_1, \dots, i_n)$$

We will write $\mathcal{U} = \unfolding{(\mathcal{T}, (p_1, \dots, p_d), (p_{d+1}, \dots, p_n))}$. 
We also define $\unfolding_d T := \unfolding{(\mathcal{T}, (1, \dots, d), (d+1, \dots, n))}$
\end{definition}



\begin{definition}[Tensor slices]
%Consider a matrix $A = [\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_J] = [\mathbf{\tilde a}_1, \mathbf{\tilde a}_2, \dots, \mathbf{\tilde a}_I]^T \in \mathbb{R}^{I \times J}$. We define the slices of a matrix
%as $$A(:, j) = \mathbf{a}_j \in \mathbb{R}^I \qquad A(i, :) = \mathbf{\tilde{a}}_j \in \mathbb{R}^J$$

    Let $\mathcal{T} \in \mathbb{K}^{N_1\times \cdots \times N_n}$. Consider $S \subset \{1, \dots, n\}$ a subset of modes (dimensions)
    of $\mathcal{T}$. Let $\mathbf{i}_S = (i_k)_{k \in S}$. We define the \textbf{slice of $\mathcal{T}$} as the tensor $\mathcal{T}_{\mathbf{i}_S}$
    of order $n - \#S$
    $$\begin{align}
        \mathcal{T}_{\mathbf{i}_S} : \prod_{k \not\in S} \{1, \dots, N_k\} & \longrightarrow \mathbb{K} \\
        \mathcal{T}_{\mathbf{i}_S}(i_{j_1}, \dots, i_{j_m}) & \longmapsto \mathcal{T}(i_1, \dots, i_n)
    \end{align}$$
    Where $\{j_1, \dots, j_m\} = \{1, \dots, n\} \setminus S$. 

    Sometimes we will also implicitly specify $\mathbf{i}_S$ by writing $\mathcal{T}(a_1, \dots, a_n)$ and replacing $a_j$ with $i_j$ if $j \in S$ and "$:$" otherwise

\end{definition}

\begin{example}
    Consider $\mathcal{M} \in \mathbb{K}^{N_1 \times N_2}$ a second order tensor. We can see this tensor as a
    bidimensional array (matrix). The slice $\mathcal{M}(i, :)$ results in the $i$-th row of $\mathcal{M}$.
    The slice $\mathcal{M}(:, j)$ results in the $j$-th column of $\mathcal{M}$
\end{example}

\begin{example}
    Consider a $4$th-order tensor $\mathcal{T} \in \mathbb{K}^{N_1 \times N_2 \times N_3 \times N_4}$. 
    Fixed $i_2, i_3$, the tensor slice of $2$th-order $\mathcal{T} = \mathcal{A}(:, i_2, i_3, :) \in \mathbb{K}^{N_1 \times N_4}$ 
    with its entries defined by ${\mathcal{T}(i_1, i_4) = \mathcal{A}(i_1, i_2, i_3, i_4)}$
\end{example}

\begin{definition}[Rank of a tensor] 
    We say that a tensor is of rank $r$ and we write $\rank{T} = r$
    with $r \in \mathbb{N}$ being the minimum value such that we can write $T$ as
$$T= \sum_{p=1}^r v_p^{(1)} \otimes \cdots \otimes v_p^{(n)}$$
where $v_1^{(i)}, \dots, v_r^{(i)} \in \mathbb{V}_i, i = 1, \dots, n$
\end{definition}

One can easily see that $\rank{T} \leqslant \prod_{i=1}^n N_i$.
Unlike matrices, determining the rank of a tensor is an NP-hard problem. \cite{hillarMostTensorProblems2013}. Even
finding the maximum rank, (i.e determining $\max_{T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n} \rank{T}$) this is still an unresolved problem.
We will now present an slightly better upper bound for the tensor rank:
\begin{proposition}
    \begin{equation} \label{eq:rank-dimensional-bound}
        \rank{T} \leqslant \left\lfloor \frac{\prod_{i=1}^n N_i}{\sum_{i=1}^n N_i} \right\rfloor
    \end{equation}

\begin{proof}
    Let $r = \rank{T}$. We can write $T = \sum_{p=1}^r v_p^{(1)} \otimes \cdots \otimes v_p^{(n)}$. Now, each term of this
    sum has $\sum_{i=1}^n N_i$ adjustable parameters, since each $v_p^{(i)}$ is a vector of $\mathbb{V}_i$ with its dimension being $N_i$.
    So, in total we will have $r \sum_{i=1}^n N_i$ adjustable parameters in our decomposition. Since our tensor $T$ is completly
    determined by $\prod_{i=1}^n N_i$ parameters, we can impose that $r \sum_{i=1}^n N_i \leqslant \prod_{i=1}^n N_i$ 
\end{proof}
\end{proposition}



\begin{definition}
    \normalfont{\cite{dodsonTensorsMultilinearForms1991}}
    Let $T \in \left( \bigotimes_{i=1}^{k-1} \mathbb{V}_i \otimes \mathbb{V}_k \otimes \bigotimes_{i=k+1}^{p} \mathbb{V}_i \right) \otimes 
    \left( \bigotimes_{i=1}^{l-1} \mathbb{W}_{i}^* \otimes \mathbb{W}_l^* \otimes 
    \bigotimes_{i=l+1}^q \mathbb{W}_i^* \right)$ With $\mathbb{V}_k = \mathbb{W}_l$. Consider the mapping
    $$
    \mathcal{C}_k^l: 
    \left(\bigotimes_{i=1}^{p} \mathbb{V}_i \right) \otimes \left( \bigotimes_{i=1}^{q} \mathbb{W}_i^* \right) \longrightarrow 
    \left( \bigotimes_{i=1}^{k-1} \mathbb{V}_i \otimes \bigotimes_{i=k+1}^p \mathbb{V}_i \right) \otimes \left( \bigotimes_{i=1}^{l-1} \mathbb{W}^*_i \otimes \bigotimes_{i=l+1}^q \mathbb{W}^*_i \right)
$$
$$
\mathcal{C}_k^l \left( \bigotimes\nolimits_{i=1}^p v_i \otimes \bigotimes\nolimits_{i=1}^q f_q \right) 
= \left(\bigotimes\nolimits_{i=1}^{k-1} v_i \otimes \bigotimes\nolimits_{i=k+1}^p v_{i} \otimes
    \bigotimes\nolimits_{i=1}^{l-1} f_i \otimes \bigotimes_{i=l+1}^q f_i \right) f_l(v_k)$$
$\mathcal{C}_k^l$ is defined as the tensor contraction mapping of $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_p \otimes \mathbb{V}_{p+1}^* \otimes
\cdots \otimes \mathbb{V}_q^*$ by the indices $(k,l)$. We call $\mathcal{C}_k^l(T)$ the contraction of $T$ by indices $(k,l)$
\end{definition}

\begin{definition}

    Let $X \in \left( \bigotimes_{i=1}^{k-1} \mathbb{V}_i \otimes \mathbb{V}_k \otimes \bigotimes_{i=k+1}^{p} \mathbb{V}_i \right)$ and
    $Y \in \left( \bigotimes_{i=1}^{l-1} \mathbb{W}_{i} \otimes \mathbb{W}_l^* \otimes 
    \bigotimes_{i=l+1}^q \mathbb{W}_i \right)$ with $\mathbb{V}_k = \mathbb{W}_l$. We define the contraction between $X$ and $Y$ by the
    indices $(k,l)$ as $$X \times^l_k Y := \mathcal{C}_k^l(X \otimes Y)$$
\end{definition}

Fixing bases for $\mathbb{V}_1, \dots, \mathbb{V}_p, \mathbb{W}_1, \dots, \mathbb{W}_q$ and representing $X, Y$
as discrete functions by its representations in those basis, we get a way for computing $\mathcal{C}_k^l(X \otimes Y)$:

\begin{definition}[Base dependant tensor contraction]
    \normalfont{\cite{zhengFullyConnectedTensorNetwork2021}} Given $\mathcal{X}, \mathcal{Y}$ as the discrete function representations
    of $X, Y$ from earlier, we can write $\mathcal{C}_k^l(X \otimes Y)$ element-wise as:
    $$\begin{align}\mathcal{C}_k^l & (i_1, \dots, i_{k-1}, i_{k+1}, \dots, i_p, j_1, \dots, j_{l-1}, j_{l+1}, \dots, j_q) \\  =& \sum_{s=1}^{N_k}
        \mathcal{X}(i_1, \cdots, i_{k-1}, s, i_{k+1}, \cdots, i_p) \mathcal{Y}(j_1, \cdots, j_{l-1}, s, j_{l+1}, \cdots, j_q)
\end{align}$$
    \end{definition}

Since the notation for making a single contraction is already very tedious to write, from now on
we will use the Penrose notation.

\section{Penrose notation}
The Penrose an intuitive graphical language to represent tensor contractions that
dates back from at least the early 1970s \cite{rogerPenroseApplications}

Given an $n$th-order tensor $\mathcal{T} \in \mathbb{K}^{N_1 \times \dots \times N_n}$ the way we draw it using the
Penrose notation is as a circle with as many edges as the order of the tensor, as seen in Fig. \ref{pen:tens}

\begin{figure}[h]
\centering
\tikz {
    \node(T)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{T}$};
    \node(i) at (-1, 0) {$i_1$};
    \node(i2) at (-0.7, 0.8) {$i_2$};
    \node(id) at (0.7, 0.8) {$\dots$};
    \node(in) at (1, 0) {$i_n$};
    \draw (T) -- (i);
    \draw (T) -- (i2);
    \draw (T) -- (id);
    \draw (T) -- (in);
}
\caption{
    Representation of a tensor $\mathcal{T} \in \mathbb{K}^{N_1 \times \cdots \times N_n}$ using the Penrose notation
}
\label{pen:tens}
\end{figure}

A lot of times we will not write explicitly the name of the indexes, since we only care about their order. The order
of the indexes will be determined by their orientation respect to the circle: to get the order we will start from the left and then
following a clockwise rotation. The order in which we encounter the edges will be the order of the indexes. For example,
in Fig. $\ref{pen:tens}$, the order would be $i_1, i_2, \dots, i_n$

We represent the contraction between two tensors $\mathcal{X} \in \mathbb{K}^{N_1 \times \cdots \times N_n}, \mathcal{Y} \in \mathbb{K}^{M_1 \times \cdots \times M_m}$
as their representation in the Penrose notation with the edges that represent the indexes that are contracting by joining them. For example, the contraction
shown in Fig. \ref{pen:tencon}, results in the tensor $\mathcal{Z} = \mathcal{X} \times_3^1 \mathcal{Y} \in \mathbb{K}^{N_1 \times N_2 \times M_2 \times M_3}$

\begin{figure}
\centering
\tikz {
    \node(X)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{X}$};
    \node(Y)[draw, shape=circle, minimum size=0.8cm] at (2,0) {$\mathcal{Y}$};
    \node(i1) at (-1, 0) {$i_1$};
    \node(i2) at (0, 1) {$i_2$};
    \node(j2) at (2, 1) {$j_2$};
    \node(j3) at (3, 0) {$j_3$};
    \draw (X) -- (i1);
    \draw (X) -- (i2);
    \draw (Y) -- (j2);
    \draw (Y) -- (j3);
    \draw (X) -- (Y) 
        node[above, pos=0.8] {$j_1$}
        node[above, pos=0.2] {$i_3$};
}
\tikz {
    \node(R) at (-1.75, 0) {$\equiv$}
    \node(Z)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{Z}$};
    \node(i1) at (-1, 0) {$i_1$};
    \node(i2) at (-0.4, 1) {$i_2$};
    \node(j2) at (0.4, 1) {$j_2$};
    \node(j3) at (1, 0) {$j_3$};
    \draw (Z) -- (i1);
    \draw (Z) -- (i2);
    \draw (Z) -- (j2);
    \draw (Z) -- (j3);
}
\caption{
    Representation in the Penrose notation of the contraction between two tensors $\mathcal{X} \in \mathbb{K}^{N_1 \times N_2 \times N_3}, \mathcal{Y} \in \mathbb{K}^{M_1 \times M_2 \times M_3}$
    by their indices $i_3$ and $j_1$ with $N_1, N_2, N_3, M_1, M_2, M_3 \in \mathbb{N}$ and $N_3 = M_1$
}
\label{pen:tencon}
\end{figure}



















\chapter{Tensor Networks}

The concept of tensor networks originated from a physics background. Roger Penrose described how its
diagrammatic language could be used in various applications of physics \cite{rogerPenroseApplications}.

Later, in 1992, Steven R. White developed de Density Matrix Renormalization Group (DRMG) algorithm for
quantum lattice systems. It was considered the first successfull tensor network application \cite{whiteDensityMatrixFormulation1992}.

% Canviar aquest paràgraf
The commonly use terminology "tensor decomposition" (TD) is equivalent to "tensor network" to some extent. After
several years of progress accross different research fields, there is no significant distinction between tensor decomposition
and tensor networks. TD was employed primarly in signal processing fields \cite{wangTensorNetworksMeet2023}. Traditional TD models can be viewed
as basic kinds of tensor networks. In this thesis we will study some of the properties of different tensor decomposition methods,
and their effectivity.

In this chapter, we will define a mathematic definition of a tensor decomposition based on \cite{yeTensorNetworkRanks2019}, we will
see some common examples of tensor decompositions and we will define the 
tensor network structure space.

\section{Tensor networks and tensor network states}

Informally, the way we construct a tensor network consists of picking a directed graph $G = (V, \bar{E})$, and for each vertex $i \in V$ we assign
a vector space $\mathbb{V}_i$ and for each edge $(i,j) \in \bar{E}$ we assign a vector space $\mathbb{E}_i$ to the tail of the edge and its 
dual covector space $\mathbb{E}_i^*$ to the head of the edge. We will also demand that the graph $G$ is connected.

More formally, let $\mathbb{V}_1, \dots, \mathbb{V}_d$ be vector spaces with $\dim{\mathbb{V}_i} = N_i, i = 1, \dots, d$. Let
$\mathbb{E}_1, \dots, \mathbb{E}_c$ be finite vector spaces with $\dim{\mathbb{E}_i} = R_i, i = 1, \dots, c$. For each $i \in V$ we associate
the tensor product space
$$\left( \bigotimes\nolimits_{j \in \IN (i)} \mathbb{E}_j \right) \otimes \mathbb{V}_i \otimes 
\left( \bigotimes\nolimits_{j \in \OUT (i)} \mathbb{E}_j^*  \right)$$

and a contraction map $\kappa_G$ defined by contracting factors in $\mathbb{E}_j$ with factors of $\mathbb{E}^*_j$

$$\kappa_G : \bigotimes\nolimits_{i=1}^d \left[ \left( \bigotimes\nolimits_{j \in \IN (i)} \mathbb{E}_j \right) \otimes \mathbb{V}_i \otimes 
\left( \bigotimes\nolimits_{j \in \OUT (i)} \mathbb{E}_j^*  \right)  \right] \rightarrow \bigotimes\nolimits_{i=1}^d \mathbb{V}_i$$

Since every directed edge $(i,j)$ must point out of a vertex $i$ and point into a vetex $j$, each copy of $\mathbb{E}_j$ is paired with one
copy of $\mathbb{E}^*_$, so the contraction $\kappa_G$ is well defined.

\begin{figure}[h]
    \centering

    \begin{minipage}{0.3\textwidth}
\begin{tikzpicture}[scale=0.85, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-3,1.5) {$1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (-3,-1.5) {$2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$3$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (3,0) {$4$};
    \node(i1) at (-3, 2.5) {$\mathbb{V}_1$};
    \node(i2) at (-4, -1.5) {$\mathbb{V}_2$};
    \node(i3) at (0, 1) {$\mathbb{V}_3$};
    \node(i4) at (3, 1) {$\mathbb{V}_4$};


    \draw (1) -> (2)
    node[left, pos=0.8] {$\mathbb{E}_1$}
    node[left, pos=0.15] {$\mathbb{E}_1^*$};
    \draw (2) -> (3)
    node[above, pos=0.75] {$\mathbb{E}_2$}
    node[above, pos=0.1] {$\mathbb{E}_2^*$};
    \draw (3) -> (1)
    node[above, pos=0.75] {$\mathbb{E}_3$}
    node[above, pos=0.15] {$\mathbb{E}_3^*$};
    \draw (3) -> (4)
    node[above, pos=0.8] {$\mathbb{E}_4$}
    node[above, pos=0.15] {$\mathbb{E}_4^*$};
    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.28\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(2.5, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\kappa_G(\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_d)$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.25\textwidth}
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at(0,0) {$T$};
   \node(i1) at (-1, 0) {$\mathbb{V}_1$};
    \node(i2) at (-0.4, 1) {$\mathbb{V}_2$};
    \node(i3) at (0.4, 1) {$\mathbb{V}_3$};
    \node(i4) at (1, 0) {$\mathbb{V}_4$};
    \draw (T) -- (i1);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i4);
    \end{tikzpicture}
\end{minipage}
\caption{
    Example of the correspondance between some directed graph $G$ and the vector spaces $\mathbb{V}_1, \dots, \mathbb{V}_d, \mathbb{E}_1, \dots, \mathbb{E}_c, \mathbb{E}_1^*,
    \dots, \mathbb{E}_c^*$ using the Penrose notation
}
\label{tn:graph}
\end{figure}

\begin{definition}[Tensor network state]
    If a tensor $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ can be written as $T = \kappa_G(\mathcal{G}_1 \otimes \cdots \otimes \mathcal{G}_n)$ where
    $$\mathcal{G}_i \in \left( \bigotimes\nolimits_{j \in \IN (i)} \mathbb{E}_j \right) \otimes \mathbb{V}_i \otimes 
\left( \bigotimes\nolimits_{j \in \OUT (i)} \mathbb{E}_j^*  \right)$$
then we will say that $T$ is a \textbf{tensor network state} associated to the graph $G$ with cores $\mathcal{G}_i \in \mathbb{V}_i, i = 1, \dots, n$
\end{definition}

\begin{definition}[Tensor network]
    We will define all the resulting tensors that are possible by varying $\mathcal{G}_1, \dots, \mathcal{G}_n$ and then contracting through $\kappa_G$ as the tensor network
associated to $G$ and the vector spaces $\mathbb{V}_1, \dots, \mathbb{V}_n, \mathbb{E}_1, \dots, \mathbb{E}_c$ and we will write this set
as $\TNS(G; \mathbb{E}_1, \dots, \mathbb{E}_c, \mathbb{V}_1, \dots, \mathbb{V}_n)$, i.e
$$
\begin{align}
    \TNS(G; \mathbb{E}_1, \dots, \mathbb{E}_c, \mathbb{V}_1, \dots, \mathbb{V}_n) := & \Bigg\{ \kappa_G(\mathcal{G}_1 \otimes \cdots \otimes \mathcal{G}_n) \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n : \\
                                                                                     & \mathcal{G}_i \in \left( \bigotimes\nolimits_{j \in \IN (i)} \mathbb{E}_j \right) \otimes \mathbb{V}_i \otimes 
\left( \bigotimes\nolimits_{j \in \OUT (i)} \mathbb{E}_j^*  \right) \Bigg\}
\end{align}
$$
\end{definition}
Now, since all vector spaces are determined up to isomorphism by its dimension, when the vector spaces $\mathbb{E}_1, \dots, \mathbb{E}_c, \mathbb{V}_1, \dots, \mathbb{V}_n$ are unimportant,
we will write the tensor network as $\TNS(G; R_1, \dots, R_c, N_1, \dots, N_n)$. We will also make this substitution on the representation of the
tensor network in the Penrose notation.

Now we will give some examples of common tensor network structures:


\section{Common Tensor network structures}

\begin{example}[Tensor Train decomposition]
    \normalfont{\cite{oseledetsTensorTrainDecomposition2011}} Let $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$. A tensor
train decomposition or matrix product state of $\mathcal{T}$ are a set of $3$th-order tensors $\mathcal{G}_1,\mathcal{G}_2,\dots,\mathcal{G}_n$ with
    $\mathcal{G}_i \in \mathbb{R}^{R_{i-1} \times N_i \times R_i}$ and $R_0 = R_n = 1$ such that every element of $\mathcal{T}$ is written in the
    form
    \begin{equation} \label{eq:tt-contraction}
    \mathcal{T}(i_1,i_2,\dots,i_n) = \sum_{r_0, \dots, r_n}^{R_0, \dots, R_n} \mathcal{G}(r_0, i_1, r_1) \mathcal{G} (r_1, i_2, r_2) \cdots \mathcal{G}(r_{n-1}, i_n, r_n)
\end{equation}
We denote $R_0, R_1, \dots, R_n$ as the ranks of the tensor train decomposition, or $TT$-ranks.
\end{example}

We can easily see that the tensor train decomposition (or TT) is obtained by our definition of a tensor network when $G$ is a path, also
the contraction of the whole network yields (\ref{eq:tt-contraction})


\begin{figure}[h]
    \centering
   \begin{minipage}{0.3\textwidth}
\begin{tikzpicture}[scale=0.85, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-4,0) {$1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (-1,0) {$2$};
    \node(3)[shape=circle, minimum size=0.8cm] at (2,0) {$\dots$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (5,0) {$n$};
    \node(i1) at (-4, 1) {$N_1$};
    \node(i2) at (-1, 1) {$N_2$};
    \node(i4) at (5, 1) {$N_n$};


    \draw (1) -> (2)
    node[above, pos=0.5] {$R_1$}
    \draw (2) -> (3)
    node[above, pos=0.5] {$R_2$}
    \draw (3) -> (4)
    node[above, pos=0.5] {$R_n$}
    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (4) -- (i4);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.15\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1, -0.5);
    \node(2) at(1, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\kappa_G$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.25\textwidth}
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at(0,0) {$T$};
   \node(i1) at (-1, 0) {$N_1$};
    \node(i2) at (-0.4, 1) {$N_2$};
    \node(i3) at (0.4, 1) {$\dots$};
    \node(i4) at (1, 0) {$N_n$};
    \draw (T) -- (i1);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i4);
    \end{tikzpicture}
\end{minipage}

    \caption{Tensor Train decomposition}
    \label{tt:scheme}
\end{figure}

\iffalse
\begin{theorem}
    \normalfont{\cite{oseledetsTensorTrainDecomposition2011}}
    Given the unfoldings $A_k = \mathcal{T}_{[1:k, k:n]}$, if we choose $\rank{A_k} = r_k$ then there exists a $TT$-decomposition
    with its ranks not higher than $r_k$
\end{theorem}
\fi


\begin{example}[Tensor Ring decomposition]
    \normalfont{\cite{zhaoTensorRingDecomposition2016}} Tensor ring decomposition (or TR) or also known a matrix product state with periodic boundary conditions, is obtained when $G$ is a cycle.
\end{example}
Tensor Ring decomposition is considered generalization of Tensor Train decomposition, it's contraction is the same as
(\ref{eq:tt-contraction}) but removing the condition $R_0 = R_1 = 1$.

\begin{figure}[h]

    \centering
   \begin{minipage}{0\textwidth}
\begin{tikzpicture}[scale=0.85, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-1,2) {$1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (1,2) {$2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (2,0) {$3$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (1,-2) {$4$};
    \node(5)[shape=circle, minimum size=0.8cm] at (-1,-2) {$\dots$};
    \node(n)[draw, shape=circle, minimum size=0.8cm] at (-2,0) {$n$};

    \node(i1) at (-1.5, 3) {$N_1$};
    \node(i2) at (1.5, 3) {$N_2$};
    \node(i3) at (3.25, 0) {$N_3$};
    \node(i4) at (1.5, -3) {$N_4$};
    \node(in) at (-3.25, 0) {$N_n$};

    \draw (1) -> (2)
    node[above, pos=0.5] {$R_1$}
    \draw (2) -> (3)
    node[above right, pos=0.5] {$R_2$}
    \draw (3) -> (4)
    node[below right, pos=0.5] {$R_3$}
    \draw (4) -> (5)
    node[below, pos=0.5] {$R_4$}
    \draw (5) -> (n)
        node[right, pos=0.5] {$R_{n-1}$}
    \draw (n) -> (1)
    node[below right, pos=0.5] {$R_n$}

    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
    \draw[--] (n) -- (in);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.235\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(1.5, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\kappa_G$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.3\textwidth}
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at(0,0) {$T$};
   \node(i1) at (-1, 0) {$N_1$};
    \node(i2) at (-0.4, 1) {$N_2$};
    \node(i3) at (0.4, 1) {$\dots$};
    \node(i4) at (1, 0) {$N_n$};
    \draw (T) -- (i1);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i4);
    \end{tikzpicture}
\end{minipage}

    \caption{Tensor Ring (TR) decomposition}
    \label{tr:schema}
\end{figure}

\begin{theorem} [Circular dimensional permutation invariance] Let $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ be
    a $n$th-order tensor with its corresponding tensor ring decomposition $\mathcal{T} = \mathcal{R}(\mathcal{U}^{(1)}, \mathcal{U}^{(2)}, \dots, \mathcal{U}^{(n)})$

\end{theorem}

\begin{example}[Fully connected tensor network]
The fully connected tensor network decomposition is obtenied when $G$ is a complete graph.
\end{example}

\begin{figure}[h]
    \begin{tikzpicture}

    \end{tikzpicture}
    \caption{Fully connected tensor network decomposition (FCTN)}
    \label{fctn:schema}
\end{figure}


































\section{Approximating tensors to tensor network states}

Now, one natural question that might occur is, given some tensor $T \in \mathbb{V}_1 \otimes \cdots \times \mathbb{V}_n$ and a 
tensor network $X = \TNS(G; R_1, \dots, R_c, N_1, \dots, N_n)$, can we ensure that $T \in X$? (i.e $T$ is an state of $X$) And if not, given all $\mathcal{X} \in X$,
can we find an $\mathcal{X}_0 \in X$ such that $\| T - \mathcal{X}_0 \|_F$ is minimal?

The following theorem gives us that by some $R_1, \dots, R_c$, every tensor $T$ can be a state of $X$:

\begin{theorem} \normalfont{\cite{yeTensorNetworkRanks2019}} Let $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ and let $G$ be a connected graph with $n$ vertices and $c$ edges.
    There exists $R_1, \dots, R_c \in \mathbb{N}$ such that
    $$T \in \TNS(G; R_1, \dots, R_c, N_1, \dots, N_d)$$
    in fact, we can choose $R_1 = \dots = R_c = \rank{T}$
\end{theorem}

\begin{proof} Let $r = \rank{T}$. Then there exist $v_1^{(i)}, \dots, v_r^{(i)} \in \mathbb{V}_i, i = 1, \dots, n$ such that
    $$T = \sum_{p=1}^r v_1^{(p)} \otimes \cdots \otimes v_n^{(p)}$$
    We take $R_1 = \dots = R_c = r$ we take for each $i = 1, \dots, n$
    $$\mathcal{G}_i = \sum_{p=1}^r \left( \bigotimes\nolimits_{j \in \IN(i)} e_p^{(j)} \right) \otimes v_p^{(i)} \otimes 
    \left( \bigotimes\nolimits_{j \in \OUT(i)} e_p^{(j)*} \right)$$

    Now observe that for each $i = 1, \dots, n$ there exists an unique $h$ such that whenever $j \in \IN(i)\cap \OUT(i)$,
    $e_p^{(j)}$ and $e_p^{(j)*}$ contract and give $\delta_{pq}$, therefore the summand vanishes except when $p = q$.
    This together with the assumption that $G$ is a connected graph implies that $\kappa_G(\mathcal{G}_1 \otimes \cdots \otimes \mathcal{G}_n)$ reduces
    to a sum of terms of the form $v_p^{(1)} \otimes \cdots \otimes v_p^{(d)}$ for $p = 1, \dots, r$, which is of course $T$
\end{proof}

This theorem serves as an upper bound for our problem.
Suppose that $T \in \mathbb{K}^{N_1 \times \cdots \times N_n}$ and $\rank T = \prod_{i=1}^n N_i$. This theorem says that given any connected
directed graph $G$ we can express of 
We will see that in fact, we can find some graphs $G$ where $T \in \TNS(G; R_1, \dots, R_c, N_1, \dots, N_d)$
with $R_1, \dots, R_c < \rank{T}$

\section{The tensor tensor network structure space}

\section{Finding the best structure}


\cite{liPermutationSearchTensor2022}


TODO:
\begin{itemize}
\item Descriure $G$-ranks
\item Algorismes per aproximar TNS per $G$-ranks propers i mínims si es pot fer
\item Algun algorisme per trobar heuristicament els $G$-ranks adequats? (suposo q depen de compressió ratio i l'error relatiu)
\item Com podem trobar un $G$ adequat?
\item Estratègies per contraure tensors més ràpidament? (DRMG)
\item Algorismes, part pràctica en C/C++
\item Fer moltes gràfiques
\item Fer aplicacions per machine learning, etc.
\item Fixar la mathematical subject classification
\end{itemize}












\chapter{Conclusions}

Fent servir un s\'{\i}mil geom\`etrico-cartogr\`afic, aquesta mem\`oria constitueix un mapa a escala planet\`aria de la demostraci\'o de la conjectura feble de Goldbach presentada per Helfgott i un mapa a escala continental de la verificaci\'o num\`erica d'aquesta. Estudis posteriors i m\'es profunds haurien de permetre elaborar mapes de menor escala.

La naturalesa dels nombres primers ens ha portat per molts racons diferents de les Matem\`atiques; en no imposar-nos restriccions en la forma de pensar, hem pogut gaudir del viatge i assolir els objectius que ens vam plantejar a l'inici del projecte i anar m\'es enll\`a, sobretot en el camp de la computaci\'o i la manipulaci\'o de grans volums de dades num\`eriques.

Una gran part dels coneixements b\`asics que hem hagut de fer servir han estat treballats en les assignatures de M\`etodes anal\'{\i}tics en teoria de nombres i d'An\`alisi harm\`onica i teoria del senyal, que s\'on optatives de quart curs del Grau de Ma\-te\-m\`a\-ti\-ques. Altres els hem hagut d'aprendre durant el desenvolupant del projecte. S'ha realitzat una tasca de recerca bibliogr\`afica important, consultant recursos antics i moderns, tant en format digital com en format paper.

\normalfont


\newpage

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

\appendix
\chapter{Proofs}
\section{Tensor contractions}
%\begin{definition}[Tensor contraction]
%    (https://math.stackexchange.com/questions/1792230/coordinate-free-notation-for-tensor-contraction)
%\end{definition}

Let $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_k \otimes \cdots \otimes \mathbb{V}_p \otimes \mathbb{W}_1^* \otimes \cdots
\otimes \mathbb{W}_l \otimes \cdots \otimes \mathbb{W}_q$

 
\chapter{Experimental results}
\end{document} 

