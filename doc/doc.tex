\documentclass[11pt,a4paper,openright,oneside]{book}
\usepackage{amsfonts, amsmath, amssymb,latexsym,amsthm, mathrsfs, enumerate}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{epsfig}
\usepackage{csquotes}
\usepackage{biblatex}
\addbibresource{refs.bib}

\parskip=5pt
\parindent=15pt
\usepackage[margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{tikz}

\setcounter{page}{0}



\numberwithin{equation}{section}
\newtheorem{defn0}{Definition}[chapter]
\newtheorem{prop0}[defn0]{Proposition}
\newtheorem{thm0}[defn0]{Theorem}
\newtheorem{lemma0}[defn0]{Lemma}
\newtheorem{corollary0}[defn0]{Corollary}
\newtheorem{example0}[defn0]{Example}
\newtheorem{remark0}[defn0]{Remark}
\newtheorem{conjecture0}[defn0]{Conjecture}

\newenvironment{definition}{ \begin{defn0}}{\end{defn0}}
\newenvironment{proposition}{\bigskip \begin{prop0}}{\end{prop0}}
\newenvironment{theorem}{\bigskip \begin{thm0}}{\end{thm0}}
\newenvironment{lemma}{\bigskip \begin{lemma0}}{\end{lemma0}}
\newenvironment{corollary}{\bigskip \begin{corollary0}}{\end{corollary0}}
\newenvironment{example}{ \begin{example0}\rm}{\end{example0}}
\newenvironment{remark}{ \begin{remark0}\rm}{\end{remark0}}
\newenvironment{conjecture}{\begin{conjecture0}}{\end{conjecture0}}

\newcommand{\defref}[1]{Definition~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\exref}[1]{Example~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}
\newcommand{\conjref}[1]{Conjecture~\ref{#1}}


\DeclareMathOperator{\vectorize}{vec}

% --------------------------------------------------
\usepackage{fancyhdr}

\lhead{}
\lfoot{}
\rhead{}
\cfoot{}
\rfoot{\thepage}

\begin{document}

\bibstyle{plain}

\thispagestyle{empty}

\begin{titlepage}
\begin{center}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=6cm]{matematiquesinformatica-pos-rgb.png}
\end{center}
\end{figure}

\vspace*{1cm}
\textbf{\LARGE GRAU DE MATEM\`{A}TIQUES } \\
\vspace*{.5cm}
\textbf{\LARGE Treball final de grau} \\

\vspace*{1.5cm}
\rule{16cm}{0.1mm}\\
\begin{Huge}
\textbf{COMPRESSING NEURAL NETWORKS USING TENSOR RING DECOMPOSITION} \\
\end{Huge}
\rule{16cm}{0.1mm}\\

\vspace{1cm}

\begin{flushright}
\textbf{\LARGE Autor: Aran Roig}

\vspace*{2cm}

\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ll}
\textbf{\Large Director:} & \textbf{\Large Dr. Nahuel Statuto} \\
\textbf{\Large Realitzat a:} & \textbf{\Large  Departament de Matemàtiques   } \\
 & \textbf{\Large i Informàtica} \\
\\
\textbf{\Large Barcelona,} & \textbf{\Large \today }
\end{tabular}

\end{flushright}

\end{center}



\end{titlepage}


\newpage
\pagenumbering{roman} 

\section*{Abstract}

Goldbach's weak conjecture asserts that every odd integer greater than 5 is the sum of three primes. We study that problem and the proof of it presented by H. A. Helfgott and D. Platt. We focus on the circle method. Finally, we describe a computation that confirms Goldbach's weak conjecture up to $10^{28}$.

\section*{Resum}
La conjectura feble de Goldbach afirma que tot nombre enter imparell major que 5 \'es la suma de tres nombres primers. En aquest treball estudiem aquest problema i la seva prova presentada per HA Helfgott i D. Platt. Ens centrem en el m\`etode del cercle. Finalment, describim un c\`alcul que confirma la conjectura feble de Goldbach fins a $10^{28}$.



% TODO: Omplir això
% 15A69 - Multilinear Algebra, Tensor Calculus
{\let\thefootnote\relax\footnote{2020 Mathematics Subject Classification. 11G05, 11G10, 14G10}}



\newpage 


\section*{Agra\"{\i}ments}

Vull agrair a ... 
\newpage

\tableofcontents

\newpage

\pagenumbering{arabic} 
\setcounter{page}{1}
\chapter{Introduction}


\newpage

\chapter{Tensor networks}

\section{Tensors}

\nocite{yokotaVeryBasicsTensors2024}
We treat a tensor of $n$-order as a $n$-dimensional array $\mathcal{A} \in \mathbb{R}^{N_1\times N_2 \times \cdots \times N_n}$
Tensors are therefore generalizations of vectors and matrices, if we see them as arrays of numbers. For example, a first order
tensor is a vector, a second order tensor is a matrix, a third order tensor can be seen as a cube of numbers represented as an
array of matrices (Fig \ref{fig:tint}), a fourth order is as an array of arrays of matrices, and so on.

We denote $\mathcal{A}_{i_1 i_2 \dots i_N} = \mathcal{A}(i_1, i_2, \dots, i_N)$ as the $(i_1, i_2, \dots, i_N)$nth entry of the
tensor. The arrangement of directions (or axes) of the tensors are called \textbf{modes}. The number of arranged
entries are called the size or length of the mode. We denote the $n$-mode as arranging from the $n$th index. For example,
the vertical direction of a matrix $\mathcal{M} \in \mathbb{R}^{N_1 \times N_2}$ is the first mode. Notice that we arrange the 
using the $i_1$ index. The horizontal direction corresponds to the second mode, and it is arranged using $i_2$. An $N$-order
tensor has $N$ modes


\begin{figure}
\begin{center}
    $$A = \begin{bmatrix}
        a_1 \\
        a_2 \\
        \vdots \\
        a_n
    \end{bmatrix} \qquad
    B = \begin{bmatrix}
        b_{11} & \dots & b_{1m} \\
        \vdots & & \vdots \\
        b_{n1} & \dots & b_{nm}
    \end{bmatrix}
    \qquad
    C=\begin{bmatrix}
        c_{111} & \dots & c_{1m1} \\
        \vdots & & \vdots \\
        c_{n11} & \dots & c_{nm1} 
    \end{bmatrix}
    \dots
\begin{bmatrix}
        c_{11k} & \dots & c_{1mk}\\
        \vdots & & \vdots \\
        c_{n1k} & \dots & c_{nmk} 
    \end{bmatrix}
    \
    $$

\caption{
    Explicit representation of tensors of different orders. $A$, $B$ and $C$ are tensors of order $1$,$2$ and $3$ respectively
}
\end{center}
\label{fig:tint}

\end{figure}


\begin{figure}
\centering
\tikz{
    \node at (-1,-0.5) {$A_i \Leftrightarrow$};
    \node[draw, shape=circle] (v0) at (0,0) {$A$};
    \node(i) at (0,-1) {$i$};
    \draw (v0) -- (i);
}\qquad \tikz{
    \node at (-1,-0.5) {$B_{ij} \Leftrightarrow$};
    \node[draw, shape=circle] (v0) at (0,0) {$B$};
    \node(i) at (-0.5,-1) {$i$};
    \node(j) at (0.5,-1) {$j$};
    \draw (v0) -- (i);
    \draw (v0) -- (j);
}\qquad \tikz{
    \node at (-1,-0.5) {$C_{ijk} \Leftrightarrow$};
    \node[draw, shape=circle] (v0) at (0,0) {$C$};
    \node(i) at (-0.5,-1) {$i$};
    \node(j) at (0,-1) {$j$};
    \node(k) at (0.5,-1) {$k$};
    \draw (v0) -- (i);
    \draw (v0) -- (j);
    \draw (v0) -- (k);
}
\caption{
    Representation of the same tensors of Figure \ref{fig:tint} in tensor network notation.
}
\label{fig:tnot}
\end{figure}

The way we represent tensors in Figure \ref{fig:tint} is useful for
relating tensors as data arrays but it is very difficult to visualize for high order tensors. Also, it is time consuming
to write. The graphical notation or tensor network diagram notation (Fig \ref{fig:tnot}) uses nodes and edges to represent tensors. Each node is
a tensor and each edge represents a mode

\iffalse
\begin{figure}
    \centering
        \begin{tikzpicture}
            \node(T)[draw, shape=circle] at (0, 0) {T};
            \node(P)[draw, shape=circle] at (2, 0) {P};
            \draw (T) -- node[midway, above]{n} (P);
            \draw (T) -- node[right]{$I_2$} (0, -1);
            \draw (P) -- node[right]{$I_3$} (2, -1);
            \draw (T) -- node[above]{$I_1$} (-1, 0);
            \draw (3,0) -- node[above]{$I_4$} (P);
        \end{tikzpicture}
        \caption{
            Representació diagramàtica d'una xarxa tensorial. Cada aresta connectada entre dos tensors representa una contracció o un
            sumatori respecte d'un índex. Per conveni, els indexos s'ordenen en sentit horari començant des de baix. En aquest cas, el tensor resultant de la contracció s'indexaria de la forma 
            $(Q_{i_1i_2i_3i_4}) = \sum_{j=1}^{n} T_{i_2i_1j} P_{i_3ji_4}$.
        }
    \label{fig:tcont}
\end{figure}
\fi

\section{Tensor operations}

\subsection*{Tensor slices}
Consider a matrix $A = [\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_J] = [\mathbf{\tilde a}_1, \mathbf{\tilde a}_2, \dots, \mathbf{\tilde a}_I]^T \in \mathbb{R}^{I \times J}$. We define the slices of a matrix
as $$A(:, j) = \mathbf{a}_j \in \mathbb{R}^I \qquad A(i, :) = \mathbf{\tilde{a}}_j \in \mathbb{R}^J$$
In other words, the slice of a matrix is obtained by fixing a certain index and then looping through the other
free indexes. For example, for the vertical slice $A(:, j)$, is obtained by fixing $j$ and then writing on a vector
the column $j$ from top to bottom. The same goes for $A(i, :)$, where we obtain the slice by fixing the second
index $i$ and then writing the vector by the free second index from left to right.

We can generalize matrix slices by considering a tensor $\mathcal{A} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$, and then
considering the $p < n$ free indexes $i_{k_1}, i_{k_2}, \dots, i_{k_p}$. By iterating from the first to the last free index
we obtain a $p$th-order tensor $\mathcal{T} \in \mathbb{R}^{N_{k_1} \times \cdots \times N_{k_p}}$

\begin{example}
    Consider a $4$th-order tensor $\mathcal{A} \in \mathbb{R}^{N_1 \times N_2 \times N_3 \times N_4}$. If we choose $k_1 = 1$ and
    $k_2 = 4$ the tensor slice is defined by the tensor of $2$th-order $\mathcal{T} = \mathcal{A}(:, i_2, i_3, :) \in \mathbb{R}^{N_1 \times N_4}$ with its entries defined by:
    $$\mathcal{T}(i_1, i_4) = \mathcal{A}(i_1, i_2, i_3, i_4)$$
\end{example}

\begin{example}
    Consider a $n$th-order tensor $\mathcal{A} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$. Let $k \leqslant n$ We call the \textbf{$k$-mode} of a tensor
    the tensor slice $\mathcal{T} = \mathcal{A}(i_1, i_2, \dots, i_{k-1}, :, i_{k+1}, \dots, i_n) \in \mathbb{R}^{N_k}$
\end{example}

\subsection*{Vectorization}

Vectorization is the operation of transforming an $n$-order tensor $\mathcal{A} \in \mathbb{R}^{N_1 \times \dots \times N_n}$
into a vector $a$ of length $\prod_{i=1}^{n} N_i$. The entries of the vector $a$ are defined by $$a(\overline{i_1 i_2 \dots i_n}) = 
\mathcal{A}(i_1, i_2, \dots, i_n)$$
Where $\overline {i_1, i_2, \dots, i_n} = \sum_{j=2}^{n} \left( (i_j - 1) \prod_{k=1}^j N_k \right) + i_1$

\subsection*{Matricization}

\subsection*{Foldings}

\subsection*{Outer product}
The outer product of two vectors $a \in \mathbb{R}^I$ and $b \in \mathbb{R}^J$ is denoted by $a \circ b$ and it results
as a matrix $M = a \circ b \in \mathbb{R}^{I \times J}$ with its entries defined as $M_{ij} = a_i b_j$. We can extend the outer
product of two vectors to an arbitrary number of vectors and get as a result a $n$-order tensor

\begin{definition}[Outer product]
    We define the \textbf{outer product} of $n$ vectors 
    $a_1 \in \mathbb{R}^{N_1}, a_2 \in \mathbb{R}^{N_2}, \dots, a_n \in \mathbb{R}^{N_n}$
    as the $n$th-order tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times N_2 \times \dots \times N_n}$ with its entries being
    $$\mathcal{T}(i_1, \dots, i_n) = a_1(i_1) a_2(i_2) \cdots a_n(i_n)$$
\end{definition}
\subsection*{Inner product}
The inner product of two tensors $\mathcal{X}, \mathcal{Y} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ is defined by
$$\langle \mathcal{X},\mathcal{Y} \rangle = \sum_{i_1, \dots, i_N}^{N_1, \dots, N_n} \mathcal{X}_{i_1, \dots, i_n} \mathcal{Y}_{i_1, \dots, i_n} = 
\vectorize(\mathcal{X})^T \vectorize(\mathcal{Y}) = \langle \vectorize(\mathcal{X}), \vectorize(\mathcal{Y}) \rangle$$

\subsection*{Frobenius norm}
\begin{definition}[Frobenius norm]
The \textbf{Frobenius norm} of a tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ is given by
$$\|\mathcal{T}\|_F = \sqrt{\langle \mathcal{T}, \mathcal{T} \rangle} = \sqrt{\sum_{i_1, \dots, i_n}^{N_1, \dots, N_n}
\mathcal{T}_{i_1 \dots i_n}^2}$$
\end{definition}


\newpage

TODO: Que es un espai latent? Latent dimensions?
\begin{itemize}
    \item Explicar normes tensorials
 \item Explicar tot el tema de descomposició SVD de tensors i algoritmes
\end{itemize}




\subsection{Subapartat 2.1}

Idea de guió del TFG:

\begin{itemize}
    \item Preliminaries
    \begin{itemize}
        \item Tensors
        \item Xarxes Tensorials
        \item Xarxes Neuronals
        \item Xarxes convulucionals
    \end{itemize}
    \item Tensor Ring Nets
    \begin{itemize}
        \item Propietats de Tensor Ring Nets
        \item Com trobar la descomposició en anells de xarxes tensorials
        \begin{itemize}
            \item TR-SVD (Singular Value Decomposition)
            \item TR-ALS (Alternating Least Squares)
            \item Scalable Bayesian Tensor Ring Factorization
            \item TR-STF (Scaled Tri-Factorization) 
        \end{itemize}
    \end{itemize}
    \item Comprimir Xarxes Tensorials amb Tensor Ring Nets
        \begin{itemize}
            \item Petita Intro (Related work) \cite{obukhovTBasisCompactRepresentation2021}
            \item Com fer-ho per xarxes neuronals (Fully connected neurons)
            \item Per a xarxes convulocionals
        \end{itemize}
    \item Experiments pràctics
    \begin{itemize}
        \item Comparar datasets i coses amb diferents algoritmes de compressió
    \end{itemize}
    \item Conclusions
\end{itemize}

\chapter{Tensor ring networks}
In this chapter we will study the Tensor Ring (TR) decomposition. We will see some properties, and some advantages
and disadvantages between some tensor decompositions.

Tensor rings are chains of tensors with both ends joined, forming a ring. Tensor rings
have gained interest in tensor decomposition because they are a generalization of Tensor Train (TT) networks with
the difference that both ends of the tensor decomposition have more freedom and can store some information [CN]

A matrix $A \in \mathbb{R}^{m\times n}$ of rank $r$ can be decomposed as $A = WH$ where $W \in \mathbb{R}^{m \times r}$ and
$H \in \mathbb{R}^{r \times n}$. For a lower rank $k < r$, $A$ can be decomposed as the product of two matrices 
$W \in \mathbb{R}^{m \times k}$ and $H \in \mathbb{R}^{k \times n}$ at the expense of some error after optimizing the function
$$\min_{W, H} \frac{1}{2} \|A - WH\|^2_F$$

A common technique of achieving this factorization is the Singular Value Decomposition
(SVD). For orthogonal matrices $U \in \mathbb{R}^{n \times r}$, $V \in \mathbb{R}^{r \times m}$ and a
diagonal matrix $\Sigma \in \mathbb{R}^{r \times r}$ of singular values we can write
$$A = U\Sigma V^T$$
If $k < r$, then this factorization is called truncated SVD
\chapter{Neural network compression}

\chapter{Conclusions}

Fent servir un s\'{\i}mil geom\`etrico-cartogr\`afic, aquesta mem\`oria constitueix un mapa a escala planet\`aria de la demostraci\'o de la conjectura feble de Goldbach presentada per Helfgott i un mapa a escala continental de la verificaci\'o num\`erica d'aquesta. Estudis posteriors i m\'es profunds haurien de permetre elaborar mapes de menor escala.

La naturalesa dels nombres primers ens ha portat per molts racons diferents de les Matem\`atiques; en no imposar-nos restriccions en la forma de pensar, hem pogut gaudir del viatge i assolir els objectius que ens vam plantejar a l'inici del projecte i anar m\'es enll\`a, sobretot en el camp de la computaci\'o i la manipulaci\'o de grans volums de dades num\`eriques.

Una gran part dels coneixements b\`asics que hem hagut de fer servir han estat treballats en les assignatures de M\`etodes anal\'{\i}tics en teoria de nombres i d'An\`alisi harm\`onica i teoria del senyal, que s\'on optatives de quart curs del Grau de Ma\-te\-m\`a\-ti\-ques. Altres els hem hagut d'aprendre durant el desenvolupant del projecte. S'ha realitzat una tasca de recerca bibliogr\`afica important, consultant recursos antics i moderns, tant en format digital com en format paper.

\normalfont


\newpage

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

\appendix
\chapter{Noseque}
\section{Nosequantos}
\end{document} 

