\documentclass[11pt,a4paper,openright,oneside]{book}
\usepackage{amsfonts, amsmath, amssymb,latexsym,amsthm, mathrsfs, enumerate}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{epsfig}
\usepackage{csquotes}
\usepackage{biblatex}

\addbibresource{refs.bib}
\addbibresource{man-refs.bib}

\parskip=5pt
\parindent=15pt
\usepackage[margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{tikz}
\usepackage{tikz-cd}

\usetikzlibrary{decorations.pathreplacing}

\usepackage{parskip}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{color}
\hypersetup{
    linktoc=all,
    colorlinks=true,
    linkcolor=blue,  %choose some color if you want links to stand out
}

\setcounter{page}{0}



\numberwithin{equation}{section}
\newtheorem{defn0}{Definition}[chapter]
\newtheorem{prop0}[defn0]{Proposition}
\newtheorem{thm0}[defn0]{Theorem}
\newtheorem{lemma0}[defn0]{Lemma}
\newtheorem{corollary0}[defn0]{Corollary}
\newtheorem{example0}[defn0]{Example}
\newtheorem{remark0}[defn0]{Remark}
\newtheorem{conjecture0}[defn0]{Conjecture}

\newenvironment{definition}{ \begin{defn0}}{\end{defn0}}
\newenvironment{proposition}{\bigskip \begin{prop0}}{\end{prop0}}
\newenvironment{theorem}{\bigskip \begin{thm0}}{\end{thm0}}
\newenvironment{lemma}{\bigskip \begin{lemma0}}{\end{lemma0}}
\newenvironment{corollary}{\bigskip \begin{corollary0}}{\end{corollary0}}
\newenvironment{example}{ \begin{example0}\rm}{\end{example0}}
\newenvironment{remark}{ \begin{remark0}\rm}{\end{remark0}}
\newenvironment{conjecture}{\begin{conjecture0}}{\end{conjecture0}}

\newcommand{\defref}[1]{Definition~\ref{#1}}
\newcommand{\propref}[1]{Proposition~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\exref}[1]{Example~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\remref}[1]{Remark~\ref{#1}}
\newcommand{\conjref}[1]{Conjecture~\ref{#1}}
\newcommand{\figref}[1]{\cref{#1}}
\newcommand{\refeq}[1]{\cref{#1}}



\DeclareMathOperator{\vectorize}{vec}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\unfolding}{unfold}
\DeclareMathOperator{\IN}{IN}
\DeclareMathOperator{\OUT}{OUT}
\DeclareMathOperator{\TNS}{TNS}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\size}{Size}
\DeclareMathOperator{\Det}{Det}

% --------------------------------------------------
\usepackage{fancyhdr}

\lhead{}
\lfoot{}
\rhead{}
\cfoot{}
\rfoot{\thepage}

\begin{document}

\bibstyle{plain}

\thispagestyle{empty}

\begin{titlepage}
\begin{center}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=6cm]{matematiquesinformatica-pos-rgb.png}
\end{center}
\end{figure}

\vspace*{1cm}
\textbf{\LARGE GRAU DE MATEM\`{A}TIQUES } \\
\vspace*{.5cm}
\textbf{\LARGE Treball final de grau} \\

\vspace*{1.5cm}
\rule{16cm}{0.1mm}\\
\begin{Huge}
\textbf{SEARCHING OPTIMAL LOW-RANK APPROXIMATIONS USING TENSOR NETWORKS} \\
\end{Huge}
\rule{16cm}{0.1mm}\\

\vspace{1cm}

\begin{flushright}
\textbf{\LARGE Autor: Aran Roig}

\vspace*{2cm}

\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ll}
\textbf{\Large Director:} & \textbf{\Large Dr. Nahuel Statuto} \\
\textbf{\Large Realitzat a:} & \textbf{\Large  Departament de Matemàtiques   } \\
 & \textbf{\Large i Informàtica} \\
\\
\textbf{\Large Barcelona,} & \textbf{\Large \today }
\end{tabular}

\end{flushright}

\end{center}



\end{titlepage}


\newpage
\pagenumbering{roman} 

\section*{Abstract}

Tensor network structure search has been interesting research topic since the raise on
complexity of deep learning models and quantum mechanics. This is an Undegraduate Thesis whose main goal is to give an 
automated search of an optimal tensor network structure for representing a given
tensor with some fixed error.

For these purpose we first give an introduction to tensors, tensor networks and then we present some examples
of well studied tensor networks, including Tucker decomposition, Tensor Train decomposition, Tensor Ring decomposition
and Fully Connected Tensor Network decomposition. 

Then with some practical experiments we demonstrate that it is possible to find more optimized structures
without significant losses on performance and accuracy, and finally we will present
an algorithm for finding these optimized structures.

\section*{Resum}

La recerca de l'estructura òptima de xarxes de tensors ha estat un tema d'interès des de l'augment en la complexitat dels models d'aprenentatge profund i de la mecànica quàntica. 
Aquest és un treball de final de grau que té com a objectiu principal oferir una cerca automatitzada 
d’una estructura òptima de xarxa de tensors per representar un tensor donat amb un error fixat.

Per aconseguir aquest propòsit, primer oferim una introducció als tensors, a les xarxes de tensors, i tot seguit presentem alguns exemples de xarxes de tensors ben estudiades, 
incloent-hi la descomposició de Tucker, la descomposició en tren de tensors (Tensor Train), la descomposició en anell
de tensors (Tensor Ring) i la descomposició de xarxa de tensors totalment connectada.

A continuació, amb alguns experiments pràctics, demostrem que és possible trobar estructures més optimitzades sense pèrdues significatives 
en el rendiment i la precissió de la representació, i finalment presentem un algoritme per trobar aquestes estructures optimitzades.

% TODO: Omplir això
% 15A69 - Multilinear Algebra, Tensor Calculus, Graph Theory, no se
{\let\thefootnote\relax\footnote{2020 Mathematics Subject Classification. 11G05, 11G10, 14G10}}



\newpage 


\section*{Agra\"{\i}ments}

Vull agrair a ... 
\newpage

{\hypersetup{linkcolor=black}
\tableofcontents
}

\newpage

\pagenumbering{arabic} 
\setcounter{page}{1}
\chapter{Introduction}

% TODO: Fer mes intro a les nn, com afecten al món, i tal.

On the last decades, neural networks have emerged as one of the most influential topics inside the fields of
artificial inteligence and machine learning. Neural networks are inspired on how the human brain works, they are 
made as a simplification about how our networks interact and in some way they try to emulate the way we think.

In the last years, neural networks have gained a lot of importance, positioning themselves at the center of
impactful technological advances. Some models that use neural networks at its core are
for example large language models (LLMs) which serve as advanced virtual agents, diffusers which
generate images and other media. Other models based on neural networks have been used to make important
advances in a lot of different fields such as in medicine, transportation, education, entertainment, and others.

\begin{figure}[h]
    \centering
    \begin{tikzpicture} 

    \node(i1)[draw, shape=circle, minimum size=0.5cm] at (0,-2) {};
    \node(i2)[draw, shape=circle, minimum size=0.5cm] at (0,-1) {};
    \node(i3)[draw, shape=circle, minimum size=0.5cm] at (0,0) {};
    \node(i4)[draw, shape=circle, minimum size=0.5cm] at (0,1) {};
    \node(i5)[draw, shape=circle, minimum size=0.5cm] at (0,2) {};
    \node(i6)[draw, shape=circle, minimum size=0.5cm] at (0,3) {};
    \node(i7)[draw, shape=circle, minimum size=0.5cm] at (0,4) {};
    \node(i8)[draw, shape=circle, minimum size=0.5cm] at (0,5) {};

    \node(l1)[draw, shape=circle, minimum size=0.5cm] at (2,4) {};
    \node(l2)[draw, shape=circle, minimum size=0.5cm] at (2,3) {};
    \node(l3)[draw, shape=circle, minimum size=0.5cm] at (2,2) {};
    \node(l4)[draw, shape=circle, minimum size=0.5cm] at (2,1) {};
    \node(l5)[draw, shape=circle, minimum size=0.5cm] at (2,0) {};
    \node(l6)[draw, shape=circle, minimum size=0.5cm] at (2,-1) {};

    \node(p1)[draw, shape=circle, minimum size=0.5cm] at (4,4) {};
    \node(p2)[draw, shape=circle, minimum size=0.5cm] at (4,3) {};
    \node(p3)[draw, shape=circle, minimum size=0.5cm] at (4,2) {};
    \node(p4)[draw, shape=circle, minimum size=0.5cm] at (4,1) {};
    \node(p5)[draw, shape=circle, minimum size=0.5cm] at (4,0) {};
    \node(p6)[draw, shape=circle, minimum size=0.5cm] at (4,-1) {};

    \node(o1)[draw, shape=circle, minimum size=0.5cm] at (6,-2) {};
    \node(o2)[draw, shape=circle, minimum size=0.5cm] at (6,-1) {};
    \node(o3)[draw, shape=circle, minimum size=0.5cm] at (6,0) {};
    \node(o4)[draw, shape=circle, minimum size=0.5cm] at (6,1) {};
    \node(o5)[draw, shape=circle, minimum size=0.5cm] at (6,2) {};
    \node(o6)[draw, shape=circle, minimum size=0.5cm] at (6,3) {};
    \node(o7)[draw, shape=circle, minimum size=0.5cm] at (6,4) {};
    \node(o8)[draw, shape=circle, minimum size=0.5cm] at (6,5) {};


    \foreach \x in {i1, i2, i3, i4, i5, i6, i7, i8}
        \foreach \y in {l1, l2, l3, l4, l5, l6}
        {
            \draw (\x) -- (\y);
        }

    \foreach \x in {p1, p2, p3, p4, p5, p6}
        \foreach \y in {l1, l2, l3, l4, l5, l6}
        {
            \draw (\x) -- (\y);
        }

    \foreach \x in {p1, p2, p3, p4, p5, p6}
        \foreach \y in {o1, o2, o3, o4, o5, o6, o7, o8}
        {
            \draw (\x) -- (\y);
        }
    \end{tikzpicture}
    \caption{A representation of a fully connected neural network. Each dot is a neuron and each edge represents a connection}
\end{figure}

Neural networks are modeled after how real neurons work, but in a simplified manner: each neural network contains a set of neurons which are split in
different layers. In a fully connected neural network, each neuron has an output that is connected to each neuron of the next layer, and all neurons of a layer
have as inputs the outputs of all the neurons of the previous layer. The first layer is called the input layer and the last layer is called the output layer.

As real neurons work, each connection will have a distinct role on when one neuron should fire or not. So, the outputs of each neuron will depend on its inputs. The output of each neuron in
a neural network is modeled using usually a non-linear function $f : \mathbb{K}^N \rightarrow \mathbb{K}$,
with $N$ being the number of neurons on the previous layer.

Each neuron of the neural network also has assigned some weights $w_{ji}$ to each input connection, with the idea that
some connections will influence the firing of the network more than others. The key to making a neural network "learn" is
to properly adjust these weights $w_{ji}$ that are also called the \textit{parameters} of the neural network.

So, more formally, the output of a neuron $j$ in some layer will be given by the formula:

$$ y_j(x) = f\left(\sum_{i=1}^N w_{ji} x_i + b_{j}\right) $$ 

Where $x_{i}$ is the output of the neuron $i$ of the previous layer, and $b_j$ is some bias that is also a parameter of the neural network.

We can represent the weights of a fully connected neural network as a matrix, that we call the \textbf{weight matrix}.
Since neural networks have been incresing in complexity during the recent years, these weight matrices can be very large
and can contain a lot of parameters. So naturally there has been a rising interest on compressing weight matrices without sacrificing accuracy or performance.

The main goal of this thesis will be to compress the parameters of neural networks using tensor networks, a concept 
that originates from the study of many-body quantum systems \cite{orusTensorNetworksComplex2019}
and recently has recently attracted significant interest on machine learning.

Tensor networks are a structure that is aimed to represent and efficiently manipulate large tensors by breaking them into
smaller ones, called core tensors, which are connected into a specific pattern. Through the thesis we will explain how
these smaller tensors are connected between them, but the general idea is that after contracting these connections,
we get a representation of the original tensor.

So, what we aim to do is that given some tensor $T \in \mathbb{K}^{N_1 \times N_2 \times \cdots \times N_n}$ that
will be our weight matrix reshaped onto a tensor, we will find a tensor network structure (TNS) that
represents a good aproximation of $T$. This structure will minimize the relative error of the representation of $T$,
the size of the TNS, since we are
interested in compressing $T$ to fewer parameters, and we will also care about the computational complexity of recovering
$T$.

We will see that finding the best structure is an integer programming problem, that is $NP$-hard. We will deduce
an algorithm that finds a locally best structure by using program synthesis, and finally, we will compare this 
structure with some other more studied cases of tensor networks, such as Tensor Train networks, Tensor Ring networks and
fully connected tensor networks.

%\section{Objectives}

\section{Thesis structure}

% TODO: Cambiar això una mica

First we will present some preliminaries about tensor algebra. Then, we will introduce the
diagrammatic notation made by Roger Penrose in the earlies 1970. We will
use it for representing both matrix product states and tensor networs.

Then, we will give a formal introduction in tensor networks, based in most part from \cite{yeTensorNetworkRanks2019}.
We will introduce also well studied tensor networks such as the Tucker decomposition, the tensor train decomposition, the tensor ring decomposition,
the fully connected tensor network decomposition.

We will make special emphasis on tree tensor networks, since they have some nice properties that we can later use
for splitting tensor networks. We will also describe the alternating least squares algorithm applied to tensor networks
for finding approximated states for any tensor network.

Finally, we will explain some algorithms for finding optimal tensor network structures. We will give an algorithm for
finding a general tensor network structure based following \cite{liPermutationSearchTensor2022} and we will also describe an algorithm for finding an 
optimal tensor network tree structure using program synthesis following mainly from the paper \cite{guoTensorNetworkStructure2025}



\chapter{Tensors}


\iffalse
\begin{definition}[Graph isomorphisms]
    We say that two graphs $G, H$ are \textbf{isomorphic} if 
\end{definition}


\begin{definition}[Outer product]
    We define the \textbf{outer product} of $n$ vectors 
    $a_1 \in \mathbb{R}^{N_1}, a_2 \in \mathbb{R}^{N_2}, \dots, a_n \in \mathbb{R}^{N_n}$
    as the $n$th-order tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times N_2 \times \dots \times N_n}$ with its entries being
    $$\mathcal{T}(i_1, \dots, i_n) = a_1(i_1) a_2(i_2) \cdots a_n(i_n)$$
\end{definition}

\begin{example}
The outer product of two vectors $a \in \mathbb{R}^I$ and $b \in \mathbb{R}^J$ is denoted by $a \circ b$ and it results
as a matrix $M = a \circ b \in \mathbb{R}^{I \times J}$ with its entries defined as $M_{ij} = a_i b_j$.
\end{example}


\begin{definition}[Inner product]
The inner product of two tensors $\mathcal{X}, \mathcal{Y} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ is defined by
$$\langle \mathcal{X},\mathcal{Y} \rangle = \sum_{i_1, \dots, i_N}^{N_1, \dots, N_n} \mathcal{X}_{i_1, \dots, i_n} \mathcal{Y}_{i_1, \dots, i_n} = 
\vectorize(\mathcal{X})^T \vectorize(\mathcal{Y}) = \langle \vectorize(\mathcal{X}), \vectorize(\mathcal{Y}) \rangle$$
\end{definition}


\begin{definition}[Kronecker product]
    \normalfont{\cite{panagakisTensorMethodsComputer2021}} Given two matrices $A \in \mathbb{R}^{N_1 \times N_2}$ and $B \in \mathbb{R}^{M_1 \times M_2}$,
    their kronecker product is defined as the matrix $A \otimes B \in \mathbb{R}^{N_1 \cdot M_1 \times N_2 \cdot M_2}$ with
    $$A \otimes B = \begin{bmatrix}
        a_{11}B & \cdots & a_{1N_2}B \\ 
        \vdots & \ddots & \vdots \\
        a_{N_1 1}B & \cdots & a_{N_1 N_2}B \\
    \end{bmatrix}$$
\end{definition}

\begin{definition}[Khatri-Rao product]
    \normalfont{\cite{panagakisTensorMethodsComputer2021}} Given two matrices $A \in \mathbb{R}^{N \times R}$ and $B \in \mathbb{R}^{M \times R}$ their
Khatri-Rao, also known as column-wise Kronecker product is defined as $A \odot B \in \mathbb{R}^{N \cdot M \times R}$
    $$ A \odot B = \begin{bmatrix} A_{:,1} \otimes B_{:,1} & A_{:,2} \otimes B_{:,2} & \cdots & A_{:,R} \otimes B_{:,R}  \end{bmatrix}$$
        

\end{definition}

\begin{definition}[Frobenius norm]
The \textbf{Frobenius norm} of a tensor $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ is given by
$$\|\mathcal{T}\|_F = \sqrt{\langle \mathcal{T}, \mathcal{T} \rangle} = \sqrt{\sum_{i_1, \dots, i_n}^{N_1, \dots, N_n}
\mathcal{T}_{i_1 \dots i_n}^2}$$
\end{definition}

\begin{definition}[Tensor transposition]
    \normalfont{\cite{zhengFullyConnectedTensorNetwork2021}} Let $\mathcal{T} \in \mathbb{R}^{N_1 \times \cdots \times N_n}$
    be an $n$th-order tensor and $p$ a permutation of the vector $(1, 2, \dots, n)$. We define the \textbf{vector $p$ based tensor
    transposition of $\mathcal{T}$} as the tensor $\overrightarrow{\mathcal{T}_p} \in \mathbb{R}^{N_{p_1} \times \cdots \times N_{p_n}}$ with its entries defined as follows:
    $$\overrightarrow{\mathcal{T}_p}(i_1, i_2, \cdots, i_n) = (i_{p_1}, i_{p_2}, \cdots, i_{p_n})$$
\end{definition}

\begin{definition}[Tensor contraction]
    \normalfont{\cite{zhengFullyConnectedTensorNetwork2021}} Suppose that $p$ and $q$ are reorderings of the vectors
    $(1,2,\dots,n)$ and $(1,2,\dots,m)$ respectively, and let ${\mathcal{X} \in \mathbb{R}^{N_1 \times \cdots \times N_n}}$ 
    and $\mathcal{Y} \in \mathbb{R}^{M_1 \times \cdots \times M_m}$ two tensors with $N_{p_i} = M_{q_i}$ for all $i = 1,2,\dots,d$
    with $d \leqslant \min{(n, m)}$. We define the tensor contraction along the $p_{1:d}$-modes of $\mathcal{X}$ and the $q_{1:d}$-modes
    of $\mathcal{Y}$ as the tensor $\mathcal{Z}$ of order $n + m - 2d$
$$\mathcal{Z} = \mathcal{X} \times_{p_{1:d}}^{q_{1:d}} \mathcal{Y} \in \mathbb{R}^{N_{p_{d+1}} \times \cdots \times N_{p_{n}} \times N_{q_{d+1}} \times \cdots \times N_{q_m}}$$
whose elements are defined by:
$$\mathcal{Z}(i_{p_{d+1}}, \cdots, i_{p_n}, j_{q_{d+1}}, \cdots, j_{q_m}) = $$$$ \sum_{i_{p_1} = 1}^{N_1} \sum_{i_{p_2} = 1}^{N_2} \cdots \sum_{i_{p_d} = 1}^{N_d}
\overrightarrow{\mathcal{X}_p}(i_{p_1}, \cdots, i_{p_d}, i_{p_{d+1}}, \cdots, i_{p_n}) \overrightarrow{\mathcal{Y}_q}(i_{p_1}, \cdots, i_{p_d}, j_{q_{d+1}}, \cdots, j_{q_m})$$
\end{definition}



\fi


In this chapter we will construct tensors in a formal way and we will lay down the basics of tensor algebra. We will
define the notion of rank of a tensor and then,
we will describe some basic tensor reshaping operations.
All of this will be crucial for then presenting tensor contractions and tensor networks on the following chapter.

\section{The tensor product space}

We will denote $\mathbb{V}_1, \dots, \mathbb{V}_n$ as finite vector spaces over a field $\mathbb{K}$ ($\mathbb{R}$ if unspecified) of dimension $\dim{\mathbb{V}_i} = N_i \; \forall i = 1, \dots, n$.

We will present the notion of a tensor. From a mathematical standpoint, it can be defined as a multilinear map:

\begin{definition}
    A \textbf{multilinear map} or a tensor is an application ${T: \mathbb{V}_1 \times \dots \times \mathbb{V}_n \rightarrow \mathbb{K}}$ which satisfies:
    \begin{enumerate}
        \item $T(v_1, \dots, \lambda v_i, \dots, v_n) = \lambda \cdot T(v_1, \dots, v_i, \dots, v_n)$
        \item $T(v_1, \dots, v_i + u, \dots, v_n) = T(v_1, \dots, v_i, \dots, v_n) + T(v_1, \dots, u, \dots, v_n) \\ {\forall i = 1, \dots, n, \; u \in \mathbb{V}_i, \lambda \in \mathbb{K}}$
    \end{enumerate}
\end{definition}

Linear maps and bilinear maps are specific cases of multilinear maps with $n=1$ and $n=2$ respectively.

All tensors will be multilinear
maps as tensors. $T: \mathbb{V}_1 \times \cdots \times \mathbb{V}_p \times \mathbb{W}_1^* \times \cdots \times \mathbb{W}_q^*$
with $n=p+q$. We will write
$T: \mathbb{V}_1 \times \cdots \times \mathbb{V}_p \times \mathbb{W}_1^* \times \cdots \times \mathbb{W}_q^*$
when we need to explicitly distinguish what vector spaces are duals from another vector space.


% TODO: Necessito això més endavant?
\begin{definition}
    Let $T: \mathbb{V}_1 \times \cdots \times \mathbb{V}_p \times \mathbb{W}_1^* \times \cdots \times \mathbb{W}_q^* \rightarrow \mathbb{K}$
    a multilinear map.
    We say that the tensor $T$ is $p$-times covariant and $q$-times contravariant.
\end{definition}

Now we will present formally the tensor space and its elements by defining a relation between all the vectors in the free vector space over the cartesian product
$\mathbb{V}_1 \times \cdots \times \mathbb{V}_n$. First of all, we will need to define what the free vector space of a set is:

\begin{definition} Let $S$ be a set and $\mathbb{K}$ a field. We denote the free vector space over $S$ as $\mathbb{K}[S]$, which
    is the vector space containing all linear combinations of elements from $S$ with coefficients of $\mathbb{K}$, i.e:
    $$\mathbb{K}[S] = \left\{ \sum_{i=1}^n a_i s_i \mid s_i \in S, n \in \mathbb{N}, a_i \in \mathbb{K}  \right\}$$
\end{definition}

\begin{example} If we take $\mathbb{K} = \mathbb{R}$ and $S = (x,y)$, then the elements of $\mathbb{R}[S]$ have the form $ax + by$ with $a, b \in \mathbb{R}$.
% TODO: Allargar
We can see that $\mathbb{R}[S]$ is the vector space $\mathbb{R}^2$ with some fixed basis $(x, y)$.
\end{example}

Let $\mathbb{L} = \mathbb{K}[\mathbb{V}_1 \times \dots \times \mathbb{V}_n]$.
    We define relation $R$ as the smallest equivalence relation that satisfies: 
    $$(v_1, \dots, \alpha v_i, \dots, v_n) \sim \alpha(v_1, \dots, v_n) \; \forall i = 1, \dots, n, \forall \alpha \in \mathbb{K}$$
    $$(v_1, \dots, v_i + u_i, \dots, v_n) \sim (v_1, \dots, v_i \dots, v_n) + (v_1, \dots, u_i, \dots, v_n) \; \forall i = 1, \dots, n$$

\begin{proposition}
    $[0]_R$ is a subspace of $\mathbb{K} \left[ \mathbb{V}_1 \times \cdots \mathbb{V}_n \right]$

    \begin{proof}
        $0 \in [0]_R$. It is sufficient to see that for all $\lambda \in \mathbb{K}$ and $u, v \in [0]_R$ then $u + \lambda v \in [0]_R$.
        We can write $u + \lambda v \sim_R (u_1 + v_1, \dots, u_i + \lamba v_i, \dots, u_n + v_n) \sim_R 0 + (v_1, \dots, \lambda v_i, \dots, v_n) \sim_R 0 + \lambda 0 \sim_R 0$ and therefore
        $u + \lambda v \sim_R 0$ since $R$ is an equivalence relation.
    \end{proof}
\end{proposition}

\begin{definition} 
    The \textbf{tensor product space} $\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n$ is defined as the quotient $\mathbb{L} / R_0$. 
    We denote the equivalence class of $(v_1, \dots, v_n)$ as $v_1 \otimes \dots \otimes v_n$. 
\end{definition}
    We will see with the universal property of the tensor product that since the 
    elements of $\mathbb{L}/R$ satisfy the properties that define a multilineal map, each vector of $\mathbb{L} / R$ is a \textbf{tensor}.

Before, we present now an example of a tensor product space:

\begin{example}
    Given $\mathbb{R}^2$ and $\mathbb{R}^3$ with its canoncial vector space
    structures, $\mathbb{R}^2 \otimes \mathbb{R}^3$ is defined by the equivalence classes that follow the relations
    $$(\alpha u, v) \sim \alpha (u, v) \sim (u, \alpha v) \quad \forall \alpha \in \mathbb{K}, \; u \in \mathbb{R}^2, v \in \mathbb{R}^3$$
    $$(u + u', v) \sim (u, v) + (u' + v) \quad \forall u, u' \in \mathbb{R}^2, v \in \mathbb{R}^3$$
    $$(u, v + v') \sim (u, v) + (u + v') \quad \forall u \in \mathbb{R}^2, v, v' \in \mathbb{R}^3$$
    And an element of $\mathbb{R}^2 \otimes \mathbb{R}^3$ would be the representant of $[((1, 2, 3), (0, 4))]$ in which
    for example other representants of the same equivalence class would be $2 \cdot ((1,2,3), (0, 2))$ or ${((1, 0, 3), (0, 2)) + ((0,2,0), (0, 2))}$
\end{example}
% \begin{definition}[Order of a tensor] Given a tensor $T \in \mathbb{V}_1 \times \cdots \times \mathbb{V}_n \rightarrow \mathbb{K}$ We define the \textbf{order} of the tensor $T$ as $n$.
% \end{definition}
We will now state and prove the universal property of the tensor product:
\begin{theorem}[Universal property of the tensor product]
    Let $\varphi$ be the quotient mapping from $\mathbb{V}_1 \times \cdots \times \mathbb{V}_n$ to $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$. 
    For every multilinear map $h: \mathbb{V}_1 \times \cdots \times \mathbb{V}_n \rightarrow X$ where $X$ is any vector space there exists an unique linear map 
    $\tilde{h}: \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \rightarrow X$ such that the following diagram commutes:

    \centering
% https://q.uiver.app/#q=WzAsMyxbMCwwLCJWXFx0aW1lcyBXIl0sWzEsMCwiViBcXG90aW1lcyBXIl0sWzEsMSwiWCJdLFswLDEsIlxcdmFycGhpIl0sWzAsMiwiaCIsMl0sWzEsMiwiXFx0aWxkZSBoIl1d
\begin{tikzcd}
	{\mathbb{V}_1 \times \cdots \times \mathbb{V}_1} & {\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n} \\
	& X
	\arrow["\varphi", from=1-1, to=1-2]
	\arrow["h"', from=1-1, to=2-2]
	\arrow["{\tilde h}", from=1-2, to=2-2]
\end{tikzcd}

\end{theorem}

\begin{proof}
    Let $\varphi(v_1, \dots, v_n) := [(v_1, \dots, v_n)] \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$. Let $h : \mathbb{V}_1 \times \cdots \mathbb{V}_n \rightarrow X$ be a
    multilinear map. We define $\tilde H : \mathbb{K}[\mathbb{V}_1 \times \cdots \times \mathbb{V}_n] \rightarrow X$ by:
    $$\tilde H \left( \sum_{i=1}^p a_i (v_i^1, \dots, v_i^n) \right) := \sum_{i=1}^p a_i h(v_i^1, \dots, v_i^n) $$
    Consider now the vector subspace $[0]_R$ of $\mathbb{K}[\mathbb{V}_1 \times \cdots \times \mathbb{V}_n]$.
    Since $h$ is multilinear, we can see that $\tilde H$ sends every element of $[0]_R$ to
    $0 \in X$, therefore $W \subseteq \ker{ \tilde H}$ and hence $\tilde H$ induces a well-defined linear map $\tilde h: \mathbb{K}[\mathbb{V}_1 \times \cdots \times \mathbb{V}_n]/R = \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \rightarrow X$
    that satisfies $\tilde h (v_1 \otimes \cdots \otimes v_n) = h(v_1, \dots, v_n)$

    Suppose that exists another mapping $f : \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \rightarrow X$ such that $f(v_1 \otimes \cdots \otimes v_n) = h(v_1, \dots, v_n)$, then we would have
    $$f \left( \sum_{i=1}^p a_i v_i^1 \otimes \cdots \otimes v_i^n \right) = \sum_{i=1}^p a_i h(v_i^1, \dots, v_i^n) = \tilde h \left( \sum_{i=1}^n a_i v_i^1 \otimes \cdots \otimes v_i^n \right)$$
    therefore, the linear mapping $\tilde h$ is unique
\end{proof}

% TODO: Posar on es demostra això o demostrar-ho
\iffalse
The universal property of the tensor product can be extended to the tensor product of more than two spaces by changing the condition
of bilinearity to multilinearity: If we consider the tensor product $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$, for every
multilinear map $h : \mathbb{V}_1 \times \cdots \times \mathbb{V}_n \rightarrow X$ then there exists an unique linear map
$\tilde h : \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \rightarrow X$ with $h = \varphi \circ \tilde h$
\fi

We can now construct explicitly the corresponding vector space (and a basis) of ${\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n}$.

\begin{proposition} Let $\{e_1^i, e_2^i, \dots, e_{N_i}^i\}$ be basis for each $\mathbb{V}_i$ and $N_i = \dim \mathbb{V}_i$. Then the set
$$\mathcal{B}_{\otimes} = \{e_{i_1}^1 \otimes \cdots \otimes e_{i_n}^n = [(e_{i_1}^1, \dots, e_{i_n}^n)]_R : 1 \leqslant i_j \leqslant N_j, 1 \leqslant j \leqslant n\}$$
is a basis of $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$.
\begin{proof}
If we have $v_1 \otimes \cdots \otimes v_n \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$, if we write
$v_i = \displaystyle\sum_{j=1}^{N_i} \lambda_j^i e_j^i$ then, because of the multilinearity of $R$ we get that
$$v_1 \otimes \cdots \otimes v_n = \left(\sum_{i=1}^{N_1} \lambda_1^i e_1^i \right) \otimes \cdots \otimes 
\left( \sum_{i=1}^{N_n} \lambda_n^i e_n^i \right) = \sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} \lambda_{1}^{s_1} \cdots \lambda_n^{s_n} (e_1^{s_1} \otimes \cdots \otimes e_n^{s_n})$$
And since all elements of $\mathbb{V}_1 \otimes \mathbb{V}_n$ can be written in this form, $\mathcal{B}_\otimes$ spans the entire space. For proving
the independance of each element of $\mathcal{B}_\otimes$, suppose that we have a linear combination such that
\begin{equation}
\sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} \lambda_{s_1, \dots, s_n} e_{s_1}^1 \otimes \cdots \otimes e_{s_n}^n = 0
\label{eq:base_rep}
\end{equation}
Let $\{\phantom{}^* e_1^i, \dots, \phantom{}^* e_n^i \}$ be the dual basis for $\mathbb{V}^i$ such that $\phantom{}^* e_j^i (e_k^i) = \delta_{jk}$. We define now the multilinear map
$$\begin{align}
    f_{(k_1, \dots, k_n)}: \mathbb{V}_1 \times \cdots \times \mathbb{V}_n & \longrightarrow \mathbb{K} \\
    (v_1, \dots, v_n) & \longmapsto \phantom{}^* e_{k_1}^1 (v_1) \cdot \phantom{}^* e_{k_2}^2 (v_2) \cdots \phantom{}^* e_{k_n}^n (v_n)
\end{align}$$
With $1 \leqslant k_i \leqslant N_i$. The image of $f_{(k_1, \dots, k_n)}$ extracts the coefficient $\lambda_{k_1, \dots, k_n}$ of any tensor $v_1 \otimes \cdots \otimes v_n$.

Applying the universal property of the tensor product, there exists an unique linear map $\tilde f_{(s_1, \dots, s_n)} : \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \rightarrow \mathbb{K}$
such that
$$\tilde f_{(k_1, \dots, k_n)}(e_{j_1}^1 \otimes \cdots \otimes e_{j_n}^n) = f_{(k_1, \dots, k_n)}(e_{j_1}^1, \dots, e_{j_n}^n) = \delta_{j_1 k_1} \cdot \delta_{j_2 k_2} \cdots \delta_{j_n k_n}$$
If we now apply the linerar combination to $\tilde f_{(s_1, \dots, s_n)}$ we get
$$\tilde f_{(k_1, \dots, k_n)} \left( 
\sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} \lambda_{s_1, \dots, s_n} e_{s_1}^1 \otimes \cdots \otimes e_{s_n}^n 
\right) =  \sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} \lambda_{s_1, \dots, s_n} \cdot \delta_{k_1 s_1} \cdot \delta_{k_2 s_2} \cdots \delta_{k_n s_n} = \lambda_{k_1, \dots, k_n}$$
But from \eqref{eq:base_rep} $\lambda_{k_1, \dots, k_n} = 0$ for all
$1 \leqslant k_i \leqslant N_i$. Therefore, the elements of $\mathcal{B}_\otimes$ are independent and form a basis.
\end{proof}
\end{proposition}


So now, we can directly work with the tensor space since it is a vector space
and we have a well defined basis. The dimension
    of ${\mathbb{V}_1 \otimes \dots \otimes \mathbb{V}_n}$ is ${N_1 \cdot N_2 \cdots N_n}$ and its elements can be expressed as
    \begin{equation} \label{eq:base-representation}
T = \sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} T_{s_1, \dots, s_n} \cdot  e_{s_1}^1 \otimes \cdots \otimes e_{s_n}^n
\end{equation}

If we wanted to store a tensor in a computer program, it would be enough to save all the $T_{s_1, \dots, s_n}$ entries.

\begin{definition}
We will define the \textbf{size} of the tensor $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ as $\size(T) = N_1 N_2 \cdots N_n$. We will 
also say that the \textbf{order} of $T$ is $n$.
\end{definition}

Now, we want to be able to do the product between two tensors of different tensor product spaces since we will need this for contracting
tensor networks. In other words, suppose that
$T \in \mathbb{V}_1 \otimes \cdots \mathbb{V}_n$ and $U \in \mathbb{W}_1 \otimes \cdots \otimes \mathbb{W}_m$. We want 
that a tensor product
operation results in a tensor $T \otimes U \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \otimes \mathbb{W}_1 \otimes \cdots \otimes \mathbb{W}_m$.
We will define this operation in the following way:

% (POSAR COSES DEL KROENKER PRODUCT, PRESENTARLO)
\begin{definition}[Tensor product] With the above notation, let $\dim \mathbb{V}_i = N_i$, $\dim \mathbb{W}_j = M_j$
    and some basis $\{e_1^i, \dots, e_{N_i}^i\}$ of each $\mathbb{V}_i$ and $\{p_1^j, \dots, p_{M_j}^j\}$ of each $\mathbb{W}_j$,
    we define the tensor product
    $T \otimes U$ as
    \begin{equation}
        T \otimes U = \sum_{i_1, \dots, i_n}^{N_1, \dots, N_n} \sum_{j_1, \dots, j_m}^{M_1, \dots, M_m} T_{i_1, \dots, i_n} U_{j_1, \dots, j_m} \cdot
    e_{i_1}^1 \otimes \cdots \otimes e_{i_n}^n \otimes p_{j_1}^1 \otimes \cdots \otimes p_{j_m}^m
    \label{eq:tensor_product}
\end{equation}
\end{definition}

% TODO: Aixo justificar? Maybe
Which naturally is an element of $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n \otimes \mathbb{W}_1 \otimes \cdots \otimes \mathbb{W}_m$.
We call the tensor product for two tensors of order $2$ as the \textbf{outer product}, so we can say that 
the tensor product is a generalization of it.

% TODO Fill example
\begin{example}
    Let $\{a_1, a_2\} \subset \mathbb{V}_1$, $\{b_1, b_2\} \subset \mathbb{V}_2$ and $\{c_1, c_2\} \subset \mathbb{W}_1$, $\{d_1, d_2\} \subset \mathbb{W}_2$ be basis
    of their corresponding vector spaces. Let $T \in \mathbb{V}_1 \otimes \mathbb{V}_2$ and $U \in \mathbb{W}_1 \otimes \mathbb{W}_2$ defined as:
    $$T = 2 (a_1 \otimes b_1) + 3 (a_2 \otimes b_1) \qquad U = c_1 \otimes d_1 + c_2 \otimes d_2$$
    Then, the tensor product $T \otimes U$ would be:
    $$T \otimes U = 2 (a_1 \otimes b_1 \otimes c_1 \otimes d_1) + 2 (a_1 \otimes b_1 \otimes c_2 \otimes d_2) + $$$$
    3 (a_2 \otimes b_1 \otimes c_1 \otimes d_1) + 3 (a_2 \otimes b_1 \otimes c_2 \otimes d_2)$$
\end{example}

% TODO AAAAA LLARGAR
Now, we already know how to add and subtract two tensors of the same tensor product space since its sum is already
defined by its vector space. We will introduce a tensor norm since in the following chapters we will want to know if
a tensor is "small" or "big", and also but not less important, we want to know if two tensors are near each other
to test convergance for algorithms involving tensor networks. In our case we will stick to the Frobenius norm:


\begin{definition}
    We define the frobenius norm as:
    $$\begin{align}
        \| \cdot \|_F : \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n & \longrightarrow \mathbb{R}_+ \\
        \left( \sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} T_{s_1, \dots, s_n} \cdot e_{s_1}^1 \otimes \cdots \otimes e_{s_n}^n \right) & \longmapsto 
        \sqrt{\sum_{s_1, \dots, s_n}^{N_1, \dots, N_n} T_{s_1, \dots, s_n}^2}
    \end{align}$$
\end{definition}

\section{Tensor ranks}

In this section we will present the rank of a tensor. It will serve as the extension of the
matrix rank, which is defined as the dimension of the vector space spanned by the vectors
on its columns. For the matrix rank, we say that if it is spanned by a single vector, the rank is $1$, so
we could somewhat say that a vector has rank $1$. We will do something similar, defining
that a tensor $t$ is a rank-$1$ tensor if it can be written as $$t = v^1 \otimes \cdots \otimes v^n$$
with $v^i \in \mathbb{V}_i$. Therefore, the rank of a tensor will be $r$ if it can be spanned from $r$ rank-$1$ tensors:

\begin{definition}
    We say that a tensor $T$ has rank $r$ as $\rank{T} = r$ with 
    $r \in \mathbb{N}$ if $r$ is the minimum value such that we can write $T$ as the following form
    \begin{equation}
        T= \sum_{p=1}^r \lambda_p v_p^1 \otimes \cdots \otimes v_p^n
        \label{eq:rank}
    \end{equation}
    where $v_1^i, \dots, v_r^i \in \mathbb{V}_i, i = 1, \dots, n$ and $\lambda_p \in \mathbb{K}$
\end{definition}

The rank of a tensor is bounded by $\prod_{i=1}^n N_i$ since we can decompose every tensor as the sum
of the elements of a basis of the tensor product space, and so the rank of the tensor does not exceed the number
of elements of the basis.

Unlike matrices, determining the rank of a tensor is an NP-hard problem (See Section 8 of \cite{hillarMostTensorProblems2013}).
Finding the maximum rank, i.e determining $\displaystyle\max_{T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n} \rank{T}$ still remains an unresolved problem.
\cite{christandlTensorRankNot2018}

By manipulating the vectors $v_p^1 \otimes \cdots \otimes v_p^n$ to represent each value of the tensor
$T_{s_1, \dots, s_n}$, we can find an slightly better upper bound for the tensor rank:
\begin{proposition} Let $\dim \mathbb{V}_i = N_i$, then
    \begin{equation} \label{eq:rank-dimensional-bound}
        \rank{T} \leqslant \left\lfloor \frac{\prod_{i=1}^n N_i}{\sum_{i=1}^n N_i} \right\rfloor
    \end{equation}

\begin{proof}
    Let $r = \rank{T}$. We can write $T = \sum_{p=1}^r v_p^{1} \otimes \cdots \otimes v_p^{n}$. Now, each term of this
    sum has $\sum_{i=1}^n N_i$ adjustable parameters, since each $v_p^{(i)}$ is a vector of $\mathbb{V}_i$ with its dimension being $N_i$.
    So, in total we will have $r \sum_{i=1}^n N_i$ adjustable parameters in our decomposition. Since our tensor $T$ is completly
    determined by $\prod_{i=1}^n N_i$ parameters, we can impose $r \sum_{i=1}^n N_i \leqslant \prod_{i=1}^n N_i$ 
\end{proof}
\end{proposition}

Decomposing a tensor $T$ in rank-$1$ tensors as in \refeq{eq:rank} is known as \textbf{tensor rank decomposition}.
One could ask if given a tensor $T$ and fixed $r$, can we construct a tensor $T'$ of rank $t$ such that
$\| T - T' \|_F$ is minimum. The decomposition of $T'$ is called \textbf{canonical polyadic decomposition} and it is
an special case of a tensor network that we will see on the following chapter.

% TODO: Espera osigui podem intentar dir que si volem representar un tensor de rank noseque necessitem com a minim
% tals ranks al fer productes? Aixo pot ser molt util

% SI QUE HI HA MIRA EL PAPER DE G-RANKS ET DIU QUE LITERALMENT NECESSITES RANKS MES GRANS QUE EL G RANK DEL TENSOR
% I ALLA ET SURTEN COSES PER INTENTAR APROXIMAR EL G-RANK D UN TENSOR LOS TENSORES SON CLAVES

\section{Reshaping operations}

% TODO: Falta parlar de que es un mode i que es permutar indexos!! També transposicions <---- ES clave

In this section we will introduce reshaping operations for tensors with the object to try to extend some numeric algorithms for
matrices and to tensors by reshaping the tensors into matrices, doing operations with the matrix-shaped tensors and then recover
back the original tensor shape. This will be later use for example when we introduce the alternating least squares algorithms for tensor networks.

Any tensor $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ can be identified as an $n$-dimensional array. In other words, for each
tensor $T$ we can define a discrete function $\mathcal{T}$ that encodes the representation in a basis of the tensors $T$ as:
$$\begin{align}
    \mathcal{T}: \prod_{i=1}^n \{1, \dots, N_i\} & \longrightarrow \mathbb{K} \\
    (i_1, \dots, i_n) & \longmapsto T_{i_1, \dots, i_n}
\end{align}$$

From now on we will identify the set of all images of $\mathcal{T}$ as
an element of ${\mathbb{K}^{N_1 \times \cdots \times N_n}}$.
% \cite{yokotaVeryBasicsTensors2024}.
We will often write a tensor $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ as an element $T \in \mathbb{K}^{N_1 \times \cdots \times N_n}$
with $\dim \mathbb{V}_i = N_i$. Sometimes we will write $T(i_1, \dots, i_n)$ as the image of $\mathcal{T}$ of $(i_1, \dots, i_n)$.
Since now we can see a tensor as an $n$-dimensional array thanks to the mapping $\mathcal{T}$, we can 
start reshaping tensors. But before that, we will define what is a mode of a tensor:

\begin{definition}
We define the $j$-th of a tensor as its $j$-th dimension. A tensor of order $n$ has $n$ different modes.
\end{definition}

For example, having a $3$-order tensor $T \in \mathbb{R}^{N_1 \times N_2 \times N_3}$. We can write each entry of the tensor
as $T(i_1, i_2, i_3)$. The first mode of $T$ is $i_1$, the second one, $i_2$ and the third $i_3$. In other words, when we say
the $j$-mode of a tensor we are referring at one argument of the discrete function $\mathcal{T}$


Now we will introduce the linearization operation which simplify the notation a lot when we define
reshaping operations.

\begin{definition}[Linearization]
    Fixed $N_1, \dots, N_n \in \mathbb{N}$, given $i_1, \dots, i_n \in \mathbb{N}$ such that $1 \leqslant i_1, \leqslant N_1, \dots, 1 \leqslant i_n \leqslant N_n$,
    we define the \textbf{linearization} of the indices $i_1, \dots, i_n$ as the mapping ${\prod_{i=1}^n \{1, \dots, N_i\} \rightarrow \{1, \dots, \prod_{i=1}^n N_i\}}$ and
    with its images defined as:
    $$\overline {i_1, i_2, \dots, i_n} = \sum_{j=2}^{n} \left( (i_j - 1) \prod_{k=1}^j N_k \right) + i_1$$
\end{definition}

The purpose of the linearization mapping is to give a bijection within each element of the form $(i_1, \dots, i_n) \in \prod_{i=1}^n \{1, \dots, N_i\}$
to a positive natural number. For example, consider $n = 3$ and $N_1 = N_2 = N_3 = 3$. The encoding of the tuple $(1, 1, 1)$ corresponds to $1$,
the tuple $(2, 1, 1)$ to $2$, the tuple $(1, 2, 3)$ to $1 + (2 - 1) \cdot 3 + (3 - 1) \cdot 3 = 10$. One should keep in mind
that when defining an array in computer science, usually the first element starts at the index $0$, and our linearization operation starts with
the indices at $1$. Throughout the thesis we will stick to array indices starting at $1$ for consistency, but by doing a change of variables $i_j' = i_j - 1$
before applying the linearization and then subtracting $1$ also on the image, we will get the same operation but for arrays starting at $0$.

We will now define the vectorization operation, which reshapes a tensor $T \in \mathbb{K}^{N_1 \times N_2 \times \cdots \times N_n}$ to a 
vector of $\mathbb{K}^{N_1 \cdot N_2 \cdots N_n}$


\begin{definition}
    We define the \textbf{vectorization} of $T$
    as the first order tensor (or vector) $\mathcal{V} \in \mathbb{K}^{N_1 N_2 \cdots N_n}$ defined entrywise as
    $$\mathcal{V}(\overline{i_1 i_2 \dots i_n}) = T(i_1, i_2, \dots, i_n)$$
    We will write the vectorization of $T$ as $\vectorize{T}$
\end{definition}


\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \definecolor{filler}{rgb}{0.9, 0.9, 0.9}

        \pgfmathsetmacro{\cubex}{0.6}
        \pgfmathsetmacro{\cubey}{0.6}
        \pgfmathsetmacro{\cubez}{0.6}
        

        \def\coordlist{0, 0.75, 1.5, 2.25, 3}
        
        \foreach \coordz in \coordlist
            \foreach \coordx in \coordlist
                \foreach \coordy in \coordlist {
                     \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
                     \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
                     \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
                }

                \node(T) at (0, -2.25) {$T \in \mathbb{K}^{5 \times 5 \times 5}$};
        
        \node(a) at(4,1);
        \node(b) at(7,1);
        \draw[->] (a) -- (b);

        \pgfmathsetmacro{\coordy}{1.5}
        \pgfmathsetmacro{\coordz}{1}
        \foreach \coordx in {8.75, 9.5, 10.25, 11, 13} {
            \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
            \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
            \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
        }

        \node(p) at (11.5, 1) {$\dots$};

        \foreach \c in {1,2,3,4,5}
            \node at(-1.45, 1.75 - \c / 1.35 + 0.5) {$T_\c$};
       \node at(-1.45 + 0.75, 1.55) {$T_6$};
        \node at(-1.45 + 0.75, 1.55 - 0.75) {$\dots$};
        \node at(1.55, -1.45) {$T_{25}$};
        

\draw [decorate,decoration={brace,amplitude=5pt,mirror,raise=4ex}]
    (7.75,1) -- (13,1) node[midway,yshift=-3em]{$\vectorize{T}$};

    \node at(8.1, 0.8) {$T_1$};
    \node at(8.1 + 0.75, 0.8) {$T_2$};
    \node at(8.1 + 1.5, 0.8) {$T_3$};
    \node at(8.1 + 2.25, 0.8) {$T_4$};
    \node at(8.5 + 4.5, 1.55) {$T_{125}$};

    \end{tikzpicture}
    \label{fig:vectorization}
    \caption{Tensor vectorization}
\end{figure}

We will also define the permutation of two modes of a tensor. That means swapping arguments of the function $\mathcal{T}$
following some permutation $\sigma \in S_n$.

\begin{definition}[Tensor permutation]
    Given a permutation $\sigma \in S_n$, and a tensor $T \in \mathbb{K}^{N_1 \times N_2 \times \cdots \times N_n}$ we define
    $T_\sigma \in \mathbb{K}^{N_{\sigma(1)} \times N_{\sigma(2)} \times \cdots \times N_{\sigma(n)}}$ entrywise as
    $$T_\sigma (i_1, \dots, i_n) = T(i_{\sigma^{-1}(1)}, i_{\sigma^{-1}(2)}, \dots, i_{\sigma^{-1}(n)})$$
\end{definition}

\begin{example}
    Let $M \in \mathbb{K}^{N_1 \times N_2}$. The matrix transposition $M^T$ can be written as $M_{(2, 1)}$ 
\end{example}

\begin{example}
    Suppose that we have $X \in \mathbb{R}^{3 \times 2 \times 2}$ defined as
    $$X = \left[\begin{pmatrix}
            1 & 0 \\
            0 & 1
        \end{pmatrix}, 
        \begin{pmatrix}
            1 & 0 \\
            1 & 1
        \end{pmatrix},
        \begin{pmatrix}
            1 & 0 \\
            0 & -1
        \end{pmatrix}
        \right]$$

        (3, 1, 2)
        Then the tensor $X_{(3, 1, 2)} \in \mathbb{R}^{2 \times 2 \times 3}$ would be written entrywise as
        $$X_{(3, 1, 2)}(i_1, i_2, i_3) = X(i_3, i_1, i_2)$$
        And therefore,

        $$X_{(3,1,2)} = \left[\begin{pmatrix} 1 & 1 & 1 \\ 0 & 0 & 0  \end{pmatrix}, \begin{pmatrix}  0 & 1 & 0 \\ 1 & 1 & -1 \end{pmatrix} \right]$$ 


    \label{ex:perm}
\end{example}

Reshaping tensors onto matrices will also be very useful since it will let us treat high order tensors as matrices and then apply numerical algorithms
there.

\begin{definition}[Tensor unfolding]
Let $T$ be a tensor of order $n$ with $n \geqslant 2$. Let $\sigma \in S_n$ be a permutation of $(1,2,\dots, n)$. We define the
\textbf{unfolding} of the tensor $T$ as the $2$nd-order tensor or matrix 
$\mathcal{U} \in \mathbb{R}^{\prod_{i=1}^d N_{\sigma(i)} \times \prod_{i=d+1}^n N_{\sigma(i)}}$ entrywise as
$$ \mathcal{U} (\overline{i_{\sigma(1)}, \dots, i_{\sigma(d)}}, \overline{i_{\sigma(d+1)}, \dots, i_{\sigma(n)}}) = T(i_1, \dots, i_n)$$

We will write $\mathcal{U} = \unfolding{(\mathcal{T}, (p_1, \dots, p_d), (p_{d+1}, \dots, p_n))}$ 

\begin{example}
    Suppose that we have $X$ defined as in \exref{ex:perm}.
        Then $\unfolding{(X, (1, 2), (3))}$ would result in a matrix $\mathbb{R}^{3 \times 4}$ with its elements defined as 
        $\mathcal{U}(\overline{i_1  i_2}, \overline{i_3}) = X(i_1, i_2, i_3)$. Computing each entry gives
        $$\unfolding{(X, (1, 2), (3))} = \begin{pmatrix}
            1 & 0 & 0 & 1 \\
            1 & 0 & 1 & 1 \\
            1 & 0 & 0 & -1
        \end{pmatrix}
        $$
    \label{exa:unfold}
\end{example}

%TODO: Seguir per aqui

\end{definition}

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \definecolor{filler}{rgb}{0.9, 0.9, 0.9}

        \pgfmathsetmacro{\cubex}{0.6}
        \pgfmathsetmacro{\cubey}{0.6}
        \pgfmathsetmacro{\cubez}{0.6}
        

        \def\coordlist{0, 0.75, 1.5, 2.25, 3}
        
        \foreach \coordz in \coordlist
            \foreach \coordx in \coordlist
                \foreach \coordy in \coordlist {
                     \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(-\cubex,0,0) -- ++(0,-\cubey,0) -- ++(\cubex,0,0) -- cycle;
                     \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(0,0,-\cubez) -- ++(0,-\cubey,0) -- ++(0,0,\cubez) -- cycle;
                     \draw[fill=filler] (\coordx,\coordy,\coordz) -- ++(-\cubex,0,0) -- ++(0,0,-\cubez) -- ++(\cubex,0,0) -- cycle;
                }

        \node(T) at (0, -2.25) {$T \in \mathbb{K}^{5 \times 5 \times 5}$};

        \foreach \c in {1,2,3,4,5}
            \node at(-1.45, 1.75 - \c / 1.35 + 0.5) {$T_\c$};
        \node at(-1.45 + 0.75, 1.55) {$T_6$};
        \node at(-1.45 + 0.75, 1.55 - 0.75) {$\dots$};
        \node at(1.55, -1.45) {$T_{25}$};
        
        \node(a) at(4,1);
        \node (b) at (9,1) {$
            \begin{pmatrix} 
                T_1 & T_2 & \dots & T_{25} \\
                T_{26} & T_{27} & \dots & T_{50} \\
                \vdots & \vdots & \ddots & \vdots \\
                T_{101} & T_{102} & \dots & T_{125}
            \end{pmatrix}$};
        \draw[->] (a) -- (b);

\draw [decorate,decoration={brace,amplitude=5pt,mirror,raise=4ex}]
    (7,0.5) -- (11,0.5) node[midway,yshift=-3em]{$\unfolding{(T, (1,2), (3))} \in \mathbb{K}^{25 \times 5}$};

    \end{tikzpicture}
    \label{fig:unfolding}
    \caption{Tensor unfolding}
\end{figure}


\iffalse
\begin{definition}[Tensor slices]
%Consider a matrix $A = [\mathbf{a}_1, \mathbf{a}_2, \dots, \mathbf{a}_J] = [\mathbf{\tilde a}_1, \mathbf{\tilde a}_2, \dots, \mathbf{\tilde a}_I]^T \in \mathbb{R}^{I \times J}$. We define the slices of a matrix
%as $$A(:, j) = \mathbf{a}_j \in \mathbb{R}^I \qquad A(i, :) = \mathbf{\tilde{a}}_j \in \mathbb{R}^J$$

    Let $\mathcal{T} \in \mathbb{K}^{N_1\times \cdots \times N_n}$. Consider $S \subset \{1, \dots, n\}$ a subset of modes (dimensions)
    of $\mathcal{T}$. Let $\mathbf{i}_S = (i_k)_{k \in S}$. We define the \textbf{slice of $\mathcal{T}$} as the tensor $\mathcal{T}_{\mathbf{i}_S}$
    of order $n - \#S$
    $$\begin{align}
        \mathcal{T}_{\mathbf{i}_S} : \prod_{k \not\in S} \{1, \dots, N_k\} & \longrightarrow \mathbb{K} \\
        \mathcal{T}_{\mathbf{i}_S}(i_{j_1}, \dots, i_{j_m}) & \longmapsto \mathcal{T}(i_1, \dots, i_n)
    \end{align}$$
    Where $\{j_1, \dots, j_m\} = \{1, \dots, n\} \setminus S$. 

    Sometimes we will also implicitly specify $\mathbf{i}_S$ by writing $\mathcal{T}(a_1, \dots, a_n)$ and replacing $a_j$ with $i_j$ if $j \in S$ and "$:$" otherwise

    We will denote $\mathcal{T}^{(m)} := \mathcal{T}(:, \dots, :, i_m, :, \dots, :)$ and $\mathcal{T}^{(\neq m)} := \mathcal{T}(i_1, \dots, i_{m-1}, :, i_{m+1}, \dots, i_n)$
\end{definition}

\begin{example}
    Consider $\mathcal{M} \in \mathbb{K}^{N_1 \times N_2}$ a second order tensor. We can see this tensor as a
    bidimensional array (matrix). The slice $\mathcal{M}(i, :)$ results in the $i$-th row of $\mathcal{M}$.
    The slice $\mathcal{M}(:, j)$ results in the $j$-th column of $\mathcal{M}$
\end{example}

\begin{example}
    Consider a $4$th-order tensor $\mathcal{T} \in \mathbb{K}^{N_1 \times N_2 \times N_3 \times N_4}$. 
    Fixed $i_2, i_3$, the tensor slice of $2$th-order $\mathcal{T} = \mathcal{A}(:, i_2, i_3, :) \in \mathbb{K}^{N_1 \times N_4}$ 
    with its entries defined by ${\mathcal{T}(i_1, i_4) = \mathcal{A}(i_1, i_2, i_3, i_4)}$
\end{example}
\fi


\chapter{Tensor networks}

In this chapter we will start by presenting tensor networks from a mathematically formal standpoint. Then, we will present the notion
of the tensor $G$-rank that will be useful for determining if a tensor network can represent a tensor. We will present some common
tensor network structures. After that, we will discuss over the ordering in which its the most optimal to contract a tensor network
and we will give an algorithm that finds an optimal order of contraction, and finally, we will present the alternating least
squares algorithm applied to generic tensor networks for explicitly finding the tensor cores of a tensor network.

The concept of tensor networks originated from a physics background. Roger Penrose described how its
diagrammatic language could be used in various applications of physics \cite{rogerPenroseApplications}. Later, 
in 1992, Steven R. White developed de Density Matrix Renormalization Group (DRMG) algorithm for
quantum lattice systems. It was considered the first successfull tensor network application \cite{whiteDensityMatrixFormulation1992}.



\section{Tensor contraction and the Penrose Notation}

In this section we will define the tensor contraction operation, which is one of the most important operations
in tensor algebra and is the core concept behind tensor networks.
The idea behind tensor contraction is that given two tensors from different spaces, say for example
$T \in \mathbb{R}^{N_1 \times \cdots \times N_n}$ and $U \in \mathbb{R}^{M_1 \times \cdots \times M_m}$, we pick one dimension of
each tensor $N_i$ and $M_j$ with the same size $N_i = M_j$ and then we obtain a new tensor of order $n + m - 2$ that contains all dimensions except for $N_i$ and $M_j$, and
each element of the resulting tensor is obtained by generalizing the matrix product opertion but along $N_i$ and $M_j$ dimensions.

So, the elements of the tensor contraction $T \times^i_j U$ would be, element-wise:

$$\sum_{k = 1}^{N_i} T(s_1, \dots, s_{i-1}, k, s_{i+1}, \dots s_n) \cdot U(t_1, \dots, t_{j-1}, k, t_{j+1}, \dots, t_m) $$

Where $T \times^i_j U \in \mathbb{K}^{N_1 \times \cdots \times N_{i-1} \times N_{i+1} \times \cdots \times N_n \times M_1 \times \cdots \times 
M_{j-1} \times M_{j+1} \times \cdots \times M_m}$

Since the notation of tensor contractions is often very tedius, we will introduce the Penrose Notation for representing tensor contractions in a
more compact and elegant way. The Penrose notation dates back from at least the early 1970s and was
firstly used by Robert Penrose, to which the name is owed. \cite{rogerPenroseApplications}

Given an $n$th-order tensor $T \in \mathbb{K}^{N_1 \times \dots \times N_n}$ we represent it using the
Penrose notation is as a circle with as many edges as the order of the tensor, as seen in \figref{fig:tens}

\begin{figure}[h]
\centering
\tikz {
    \node(T)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$T$};
    \node(i) at (-1, 0) {$i_1$};
    \node(i2) at (-0.7, 0.8) {$i_2$};
    \node(id) at (0.7, 0.8) {$\dots$};
    \node(in) at (1, 0) {$i_n$};
    \draw (T) -- (i);
    \draw (T) -- (i2);
    \draw (T) -- (id);
    \draw (T) -- (in);
}
\caption{
    Representation of a tensor $T \in \mathbb{K}^{N_1 \times \cdots \times N_n}$ using the Penrose notation
}
\label{fig:tens}
\end{figure}

The dimensions of the tensor are not explicitly written in the Penrose notation. Instead, only the order
of the indices is preserved. If they are not explicitly set on the labels of the edges,
the order of the indexes of the tensor will be determined by their orientation respect to the circle: 
the order starts from the left and then follows a clockwise rotation. The order in which we encounter the edges will be the order of the indexes. 
For example, in \figref{fig:tens}, the order would be $i_1, i_2, \dots, i_n$

Then, we represent a contraction of two tensors on the Penrose notation by joining to edges of different tensors. The edges that are
joined will be the edges that the contraction is performed. As seen in \figref{fig:tencon}.

We represent a free edge that is unconnected to anything as the identity matrix.
\begin{figure}
    \centering
    \begin{tikzpicture}
        \node(a) at (-1, 0);
        \node(b) at (1, 0);
        \draw(a) -- (b);
        \node(i1) at (-1, 0.3) {$i_1$};
        \node(i2) at (1, 0.3) {$i_2$};
    \end{tikzpicture}

    \caption{
        The identity matrix represented using the Penrose Notation
    }
\end{figure}

Now, we will give a more formal definition of the tensor contraction since there is a natural way in which tensor contraction definition
appears from applying a vector space with its dual with different tensors. As before, we take two tensors $T$ and $U$ with
$T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ and $U \in \mathbb{W}_1 \otimes \cdots \otimes \mathbb{W}_m$. Suppose that there exists
some vector space of the dimensions of $U$ that is the dual of some space of the tensor $T$, in other words, suppose that exists
some $i$ and $j$ in which $\mathbb{W}_j = \mathbb{V}_i^*$. Then, the tensor contraction is doing the tensor product of $T$ and $U$ and then applying
$\mathbb{V}_i$ with its dual afterwards. Doing this gives the following definition:

\begin{definition}
%    \normalfont{\cite{dodsonTensorsMultilinearForms1991}}
    Let $P \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$
    and suppose that there exists some $i$ and $j$ such
    that $\mathbb{V}_j = \mathbb{V}_i^*$ The following map
    $$
    \begin{align}
    \mathcal{C}_j^i: 
    \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n &\longrightarrow 
    \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_{i-1} \otimes \mathbb{V}_{i+1} \otimes \cdots \otimes \mathbb{V}_{j-1} \otimes \mathbb{V}_{j+1} \otimes \cdots \otimes \mathbb{V}_n \\
 v_1 \otimes \cdots \otimes v_n
                                                     &\longmapsto \left( v_1 \otimes \cdots \otimes v_{i-1} \otimes v_{i+1} \otimes \cdots \otimes v_{j-1} \otimes v_{j+1} \otimes \cdots \otimes v_n \right) v_j(v_i)
\end{align}
$$
where $v_j \in \mathbb{V}_i^*$. We define 
$\mathcal{C}_j^i$ as the tensor contraction mapping of $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ over the indices $i$ and $j$.
We call $\mathcal{C}_i^j(T)$ the contraction of $T$ by indices $(i,j)$.
\label{def:contraction}
\end{definition}

Now, we define the \textbf{contraction of two tensors} $T$ and $U$ defined as before with some space $\mathbb{W}_j$ being the dual of
some $\mathbb{V}_i$ as $\mathcal{C}_j^i (T \otimes U)$. We will sometimes write $T \times_j^i U$

We will represent the contraction between two tensors 
as their representation in the Penrose notation with the edges that represent the indexes that are contracting by joining them, as seen in \figref{fig:tencon}.


\begin{figure}[h]
    \begin{center}
    \hfill
\begin{minipage}{0.2\textwidth}
    \begin{tikzpicture}
    \node(X)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{X}$};
    \node(Y)[draw, shape=circle, minimum size=0.8cm] at (2,0) {$\mathcal{Y}$};
    \node(i1) at (-1, 0) {$i_1$};
    \node(i2) at (0, 1) {$i_2$};
    \node(j2) at (2, 1) {$j_2$};
    \node(j3) at (3, 0) {$j_3$};
    \draw (X) -- (i1);
    \draw (X) -- (i2);
    \draw (Y) -- (j2);
    \draw (Y) -- (j3);
    \draw (X) -- (Y) 
        node[above, pos=0.8] {$j_1$}
        node[above, pos=0.2] {$i_3$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.225\textwidth}
\begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(3, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\mathcal{C}^{i_3}_{j_1}$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.225\textwidth}
\begin{tikzpicture}
    \node(Z)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{Z}$};
    \node(i1) at (-1, 0) {$i_1$};
    \node(i2) at (-0.4, 1) {$i_2$};
    \node(j2) at (0.4, 1) {$j_2$};
    \node(j3) at (1, 0) {$j_3$};
    \draw (Z) -- (i1);
    \draw (Z) -- (i2);
    \draw (Z) -- (j2);
    \draw (Z) -- (j3);
\end{tikzpicture}
\end{minipage}
\end{center}

\caption{
    Representation in the Penrose notation of the contraction between two tensors $\mathcal{X} \in \mathbb{K}^{N_1 \times N_2 \times N_3}, \mathcal{Y} \in \mathbb{K}^{M_1 \times M_2 \times M_3}$
    by their indices $i_3$ and $j_1$ with $N_1, N_2, N_3, M_1, M_2, M_3 \in \mathbb{N}$ and $N_3 = M_1$
}
\label{fig:tencon}
\end{figure}

If we fix basis for $\mathbb{V}_1, \dots, \mathbb{V}_p, \mathbb{W}_1, \dots, \mathbb{W}_q$ and we represent $X, Y$
as discrete functions by its representations in those basis, we get a way for computing $\mathcal{C}_k^l(X \otimes Y)$ as:
    \begin{equation}
    \begin{align}\mathcal{C}_k^l & (i_1, \dots, i_{k-1}, i_{k+1}, \dots, i_p, j_1, \dots, j_{l-1}, j_{l+1}, \dots, j_q) \\  =& \sum_{s=1}^{N_k}
        \mathcal{X}(i_1, \cdots, i_{k-1}, s, i_{k+1}, \cdots, i_p) \mathcal{Y}(j_1, \cdots, j_{l-1}, s, j_{l+1}, \cdots, j_q)
\end{align}
\label{eq:contraction}
    \end{equation}

\begin{example}
    The tensor contraction for two tensors $M_1 \in \mathbb{K}^{N_1 \times N_2}$ and $M_2 \in \mathbb{K}^{N_2 \times N_3}$ over one edge of each tensor yields the matrix multiplication.
    Applying \refeq{eq:contraction} we get that $Z = \mathcl{C}_2^2 (X \otimes Y)$ is defined entry-wise as:
    $$Z (i_1, j_2) = \sum_{s=1}^{N_2} M_1(i_1, s) M_2(s, j_2)$$
    And that is identical to the conventional matrix product. Visually, we can represent it using the Penrsone notation:
    \begin{center}
    \hfill
\begin{minipage}{0.2\textwidth}
    \begin{tikzpicture}
    \node(X)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$X$};
    \node(Y)[draw, shape=circle, minimum size=0.8cm] at (2,0) {$Y$};
    \node(i1) at (-1, 0) {$i_1$};
    \node(j2) at (3, 0) {$j_2$};
    \draw (X) -- (i1);
    \draw (Y) -- (j2);
    \draw (X) -- (Y) 
        node[above, pos=0.8] {$j_1$}
        node[above, pos=0.2] {$i_2$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.225\textwidth}
\begin{tikzpicture}
    \node(1) at(-1.5, 1);
    \node(2) at(3, 1);
    \node(i) at(3, 0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\mathcal{C}_{i_2}^{j_1}$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.225\textwidth}
\begin{tikzpicture}
    \node(Z)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$Z$};
    \node(i1) at (-1, 0) {$i_1$};
    \node(j2) at (1, 0) {$j_2$};
    \draw (Z) -- (i1);
    \draw (Z) -- (j2);
\end{tikzpicture}
\end{minipage}

    \end{center}
\end{example}

\begin{example}
    Suppose that we take $X \in \mathbb{R}^{3 \times 2 \times 2}$ from \exref{exa:unfold} and
    $Y \in \mathbb{R}^{2 \times 2 \times 3}$ defined as:
  $$Y = \left[ 
    \begin{pmatrix}
        1 & 0 & -1 \\
        0 & 2 & -2 \\
    \end{pmatrix},
    \begin{pmatrix}
        1 & 1 & 0 \\
        2 & 0 & 2 \\
    \end{pmatrix}
    \right] $$

    We want to compute the contraction of $X$ and $Y$ from the second index of each tensor $Z = \mathcl{C}_2^2 (X \otimes Y)$.
    Note that $Z \in \mathbb{R}^{3 \times 2 \times 2 \times 3}$. Using the Penrose
    Notation, the contraction would look like

    \begin{center}
    \hfill
\begin{minipage}{0.2\textwidth}
    \begin{tikzpicture}
    \node(X)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$X$};
    \node(Y)[draw, shape=circle, minimum size=0.8cm] at (3,0) {$Y$};

    \node(i1) at (0, 1) {$i_1$};
    \node(i3) at (0, -1) {$i_3$};
    \node(j1) at (3, 1) {$j_1$};
    \node(j3) at (3, -1) {$j_3$};
    
    \draw (X) -- (i1);
    \draw (X) -- (i3);
    \draw (Y) -- (j1);
    \draw (Y) -- (j3);
    \draw (X) -- (Y) 
        node[above, pos=0.8] {$j_2$}
        node[above, pos=0.2] {$i_2$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.225\textwidth}
\begin{tikzpicture}
    \node(1) at(-1.5, 1);
    \node(2) at(3, 1);
    \node(i) at(3, 0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\mathcal{C}^{2}_{2}$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.225\textwidth}
\begin{tikzpicture}
    \node(Z)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$Z$};
    \node(i1) at (-1, 0) {$i_1$};
    \node(i3) at (0, 1) {$i_3$};
    \node(j1) at (1, 0) {$j_1$};
    \node(j3) at (0, -1) {$j_3$};
    \draw (Z) -- (i1);
    \draw (Z) -- (i3);
    \draw (Z) -- (j1);
    \draw (Z) -- (j3);
\end{tikzpicture}
\end{minipage}

    \end{center}

And each element of the contracted tensor $Z$ would be defined as:

$$Z(i_1, i_3, j_1, j_3) = \sum_{s=1}^2 X(i_1, s, i_3) Y(j_1, s, j_3)$$

Therefore,

$$Z = \begin{bmatrix}
\begin{pmatrix}
    1 & 0 & -1 \\
    1 & 1 & 0 \\
\end{pmatrix} &
\begin{pmatrix}
    0 & 2 & -2 \\
    2 & 0 & 2 \\
\end{pmatrix} \\
\begin{pmatrix}
    1 & 2 & -3 \\
    3 & 1 & 2 \\
\end{pmatrix} &
\begin{pmatrix}
    0 & 2 & -2 \\
    2 & 0 & 2 \\
\end{pmatrix} \\
\begin{pmatrix}
    1 & 0 & -1 \\
    1 & 1 & 0 \\
\end{pmatrix} &
\begin{pmatrix}
    0 & -2 & 2 \\
    -2 & 0 & -2 \\
\end{pmatrix} \\
\end{bmatrix}$$
\label{ex:bigmatrix}
\end{example}

One can clearly see that the contraction that we defined on
the Penrose notation (joining two edges) is the same as the formal definition of the contraction of two tensors.

Now we will be extending the Penrose notation for representing some special cases:

\subsection* {Identity tensors}

Identity tensors on the Penrose notation are represented as free edges that are not connected to
any tensor. We denote by $I_n$ as the tensor identity of order $n$. $I_2$ are identity matrices.

\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \node(a) at (-1, 0);
        \node(b) at (1, 0);
        \draw(a) -- (b);

        \node(l1) at (0, -0.5) {$I_2$};

        \node (c) at (1.5, 1);
        \node (d) at (3.5, 1);
        \node (e) at (2.5, -1);

        \node(l2) at (2.5, 0) {};

        \draw (c) -- (l2.center);
        \draw (d) -- (l2.center);
        \draw (e) -- (l2.center);

        \node(n2) at (2.5, -1.5) {$I_3$};

        \node (c1) at (4.5, 1);
        \node (d1) at (6.5, 1);
        \node (e1) at (4.5, -1);
        \node (f1) at (6.5, -1);

        \node(l3) at (5.5, 0) {};

        \draw (c1) -- (l3.center);
        \draw (d1) -- (l3.center);
        \draw (e1) -- (l3.center);
        \draw (f1) -- (l3.center);

        \node(n2) at (5.5, -1.5) {$I_4$};

    \end{tikzpicture}

    \vspace{0.5cm}

\begin{minipage}{0.3\textwidth}
\centering
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$T$};
        \node(o) at (3,0);

        \node(i1) at (1, -0.5) {$i_1$};
        \node(i1) at (2, -0.5) {$I_2$};

        \draw (T) -- (o);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}
    \centering
\begin{tikzpicture}
    \node(1) at(-1.5, 1);
    \node(2) at(3, 1);
    \node(i) at(3, 0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\mathcal{C}^{1}_{1}$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.3\textwidth}
    \centering
\begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$T$};
        \node(o) at (1,0) {$i_1$};
        \draw (T) -- (o);
\end{tikzpicture}
\end{minipage}

    \caption{
        The identity matrix represented using the Penrose Notation. Contracting a mode over the identity yields the same
        tensor.
    }
    \label{fig:pen_identity}
\end{figure}

This makes complete sense since contracting some tensor along some mode over the identity matrix yields the same tensor.
This is ilustrated in \figref{fig:pen_identity}


\subsection*{Trace}

There may be the case that when contracting a series of tensors, we might end up as what
we see as a loop in the Penrose Notation. Contracting over these two indexes we get the trace
of the tensor $\mathcal{T}$ respect the indices $i_k$ and $i_p$ and we denote it as $\Tr^k_p(T)$ (See \figref{fig:trace}).
This operation is well defined since contracting over two modes of the same tensor is doing a tensor contraction
using \defref{def:contraction}.

\begin{figure}[h]
\centering

    \begin{minipage}{0.2\textwidth}
        \begin{tikzpicture}
    \node(T)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$T$};
    \node(i) at (-1, 0) {$i_1$};
    \node(i2) at (-0.7, 0.8) {$i_2$};
    \node(i6) at (0, -1) {$i_6$};
    \node(i3) at (0, 1) {$i_3$};
    \node(i7) at (-0.6, -0.8) {$i_7$};
    \node(i4) at (0.6, 0.5) {$i_4$};
    \node(i5) at (1, -0.7) {$i_5$}
    \draw (T) -- (i);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i6);
    \draw (T) -- (i7);
    \draw[--] (T) edge[in=15,out=-40,loop] ();
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.225\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(1.5, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\Tr^4_5(T)$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.2\textwidth}
    \begin{tikzpicture}
    \node(T)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$T$};
    \node(i) at (-1, 0) {$i_1$};
    \node(i2) at (-0.7, 0.8) {$i_2$};
    \node(i6) at (0, -1) {$i_6$};
    \node(i3) at (0, 1) {$i_3$};
    \node(i7) at (-0.6, -0.8) {$i_7$};
    \draw (T) -- (i);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i6);
    \draw (T) -- (i7);

    \end{tikzpicture}
\end{minipage}


\caption{
    Representation of the trace of a tensor using the Penrose notation
}
\label{fig:trace}
\end{figure}

\subsection*{Diagonal tensors}
A diagonal tensor $\Lambda \in \mathbb{K}^{N \times \cdots \times N$ of order $n$ is a tensor which only has entries in his diagonals, i.e,
    there are $\lambda_1, \dots, \lambda_n \in \mathbb{K}$ such that
    $$\Lambda(i_1, \dots, i_n) = \begin{cases}
        \lambda_j & \text{if} \; i_1 = \dots i_n = j \\
        0 & \text{otherwise}
    \end{cases}$$
    We will draw diagonal tensors as identity tensors but with an small circle on the middle.
    \begin{figure}[h]
    \begin{center}
    \begin{tikzpicture}
        \node (1) at (0, 1.5);
        \node (2) at (1.5, 0.75);
        \node (3) at (-1.5, 0.75);
        \node (4) at (1.5, -0.75);
        \node (5) at (-1.5, -0.75);
        \node (6) at (0, -1.5);

        \node(l)[fill, color=lightgray, shape=circle] at (0, 0) {};
        \node(l1) at (0.3, 0.45) {$\Lambda$};

        \foreach \x in {1,2,3,4,5,6} {
            \draw (\x) -- (l);
        }
    \end{tikzpicture}
    \end{center}

    \caption{Diagonal tensor represented using the Penrose Notation}
\end{figure}
\subsection*{Outer product}

And if two tensors are unconnected, we can represent it as one tensor but we do not perform any contraction between them
since they are not connected. In this case, the outer product of the two tensors is performed.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.2\textwidth}
        \begin{tikzpicture}
    \node(T_1)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$T_1$};
    \node(T_2)[draw, shape=circle, minimum size=0.8cm] at (2,0) {$T_2$};
    \node(i_1) at (0, 1) {$i_1$};
    \node(j_1) at (2, 1) {$j_1$};
    \draw (T_1) -- (i_1);
    \draw (T_2) -- (j_1);
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.225\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(1.5, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45];
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.2\textwidth}
    \begin{tikzpicture}
    \node(T)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$T$};
    \node(i) at (-1, 0) {$i_1$};
    \node(j) at (1, 0) {$j_1$};

    \draw(i) -- (T);
    \draw(j) -- (T);

    \end{tikzpicture}
\end{minipage}
    \caption{Representation of the outer product using the Penrose notation}
    \label{fig:penrose_outer}
\end{figure}




% \subsection {Computing the tensor contraction using the matrix product}

As we previously said, \refeq{eq:contraction} is equivalent to a matrix product, therefore it is possible
to compute a tensor contraction by multiplying two tensors unfolded in a concrete way.
the tensors $\mathcal{X}$ and $\mathcal{Y}$, we can compute $\mathcal{C}_l^k(X \otimes Y)$ as a matrix product.
\begin{corollary}
    Let $X \in \mathbb{K}^{N_1 \times \cdots \times N_k \times \cdots \times N_n}, Y \in \mathbb{K}^{M_1 \times \cdots \times M_l \times \cdots \times M_m}$
    with $1 \leqslant k \leqslant n$, $1 \leqslant l \leqslant m$, $N_k = M_l$. The matrix product
    $$ \mathcal{X}(\overline{i_1, \dots, i_{k-1}, i_{k+1}, \dots, i_n}, \overline{i_k}) \cdot \mathcal{Y}(\overline{j_l}, \overline{j_1, \dots, j_{l-1}, j_{l+1}, \dots, j_m}) $$
    Results in a $\left(\prod_{i=1}^n N_i \right) / N_k \times \left( \prod_{i=1}^m M_i \right) / M_l$ matrix, which can be reshaped back
    onto $\mathcl{C}^k_l (X \otimes Y)$.
\end{corollary}

\begin{example}
    Following from \exref{ex:bigmatrix}, we will reshape the tensors $X$ and $Y$ for computing $Z$ by only performing a matrix product.
    We need to first compute the matrices $\mathcal{X} = \unfolding{(X, (1, 3), (2))} \in \mathbb{R}^{6 \times 2}$
    and $\mathcal{Y} = \unfolding{(Y, (2), (1, 3))} \in \mathbb{R}^{2 \times 6}$. These
    unfoldings result in:

    $$X = \begin{pmatrix}
        1 & 0 \\ 0 & 1 \\ 1 & 1 \\
        0 & 1 \\ 1 & 0 \\ 0 & -1
        \end{pmatrix} \qquad
        Y = \begin{pmatrix}
            1 & 0 & -1 & 1 & 1 & 0 \\
            0 & 2 & -2 & 2 & 0 & 2
        \end{pmatrix}$$
        Therefore:
    $$Z = XY = \begin{pmatrix}
        1 & 0 & -1 & 1 & 1 & 0 \\
        0 & 2 & -2 & 2 & 0 & 2 \\
        1 & 2 & -3 & 3 & 1 & 2 \\
        0 & 2 & -2 & 2 & 0 & 2 \\
        1 & 0 & -1 & 1 & 1 & 0 \\
        0 & -2 & 2 & -2 & 0 & -2
    \end{pmatrix}$$

    And we can now reshape $Z \in \mathbb{R}^{9 \times 9}$ as the tensor in $\mathbb{R}^{3\times 3\times 3\times 3}$
    as $$Z(i_1, i_3, j_1, j_3) = Z(\overline{i_1, i_3}, \overline{j_1, j_3})$$
    which in fact, is identical to $Z$ in \exref{ex:bigmatrix}
\end{example}








\section{Tensor Network States}

In this section we will present the formal definition of tensor network states and the definition of a tensor network. 
We will use directed graphs to represent tensor networks. Before that we will need to also define what are
input and output edges:

\iffalse
\begin{definition}[Graph]
    \normalfont{\cite{wilsonIntroductionGraphTheory2009}}
    A \textbf{graph} $G$ is defined as a tuple $G = (V,E)$ where $V = V(G)$ is a set of elements called \textbf{vertices} and 
    $E = E(G) \subset \{\{u, v\} : u, v \in V\}$ is a set of elements called \textbf{edges}.
\end{definition}

\begin{definition}[Directed Graph]
    A \textbf{directed graph} $D$ is a tuple $D = (V, \bar{E})$ where $V = V(D)$ are its vertices and
    $\bar{E} = \bar{E}(D) \subset \{(u,v) : u,v \in V\}$
\end{definition}
\fi

\begin{definition} Given a directed graph $G=(V,\bar{E})$ and a vertex $i \in V$ we define
    $$\IN(i) = \{j \in V : (j, i) \in \bar{E}\} \qquad \OUT(i) = \{j \in V : (i, j) \in \bar{E} \}$$
\end{definition}



The way tensor networks are constructed consists of picking a connected directed graph $G = (V, \bar{E})$, and for each vertex $i \in V$ we assign
a vector space $\mathbb{V}_i$ and for each edge $(i,j) \in \bar{E}$ we assign a vector space $\mathbb{E}_i$ to the tail of the edge and its 
dual covector space $\mathbb{E}_i^*$ to the head of the edge. In other words, tensor networks can be seen as a series of contractions between
a set of tensors $\mathcal{G}_1, \dots, \mathcal{G}_n$ which once all of them are contracted results in a tensor of $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$

More formally, let $\mathbb{V}_1, \dots, \mathbb{V}_d$ be vector spaces with $\dim{\mathbb{V}_i} = N_i, i = 1, \dots, d$. Let
$\mathbb{E}_1, \dots, \mathbb{E}_c$ be finite vector spaces with $\dim{\mathbb{E}_i} = R_i, i = 1, \dots, c$. For each vertex $i \in V$ 
we will associate the tensor product space
$$\xi_i := \left( \bigotimes\nolimits_{j \in \IN (i)} \mathbb{E}_j \right) \otimes \mathbb{V}_i \otimes 
\left( \bigotimes\nolimits_{j \in \OUT (i)} \mathbb{E}_j^*  \right)$$

A tensor network is completly defined by the graph $G$ and all the tensor product spaces $\xi_1, \dots, \xi_n$. We will also associate
to the tensor network a contraction mapping $\mathcal{C}_G$ defined by contracting factors
in $\mathbb{E}_j$ with factors of $\mathbb{E}^*_j$:
$$\mathcal{C}_G : \bigotimes\nolimits_{i=1}^n \xi_i \longrightarrow \bigotimes\nolimits_{i=1}^d \mathbb{V}_i$$

Note that we have given this shapes to the tensors that we fix onto each vertex
$i \in V$ because when we contract the whole graph, we will get a tensor of ${\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n}$.
Since every directed edge $(i,j)$ must point out of a vertex $i$ and point into a vetex $j$, each copy of $\mathbb{E}_j$ is paired with one
copy of $\mathbb{E}^*_$, so the contraction $\mathcal{C}_G$ is well defined and it results in a tensor of $\mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$
(See \figref{fig:tn_graph})

\begin{figure}[h]
    \centering

    \begin{minipage}{0.3\textwidth}
\begin{tikzpicture}[scale=0.85, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-3,1.5) {$\mathcal{G}_1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (-3,-1.5) {$\mathcal{G}_2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{G}_3$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (3,0) {$\mathcal{G}_4$};
    \node(i1) at (-3, 2.5) {$\mathbb{V}_1$};
    \node(i2) at (-4, -1.5) {$\mathbb{V}_2$};
    \node(i3) at (0, 1) {$\mathbb{V}_3$};
    \node(i4) at (3, 1) {$\mathbb{V}_4$};


    \draw (1) -> (2)
    node[left, pos=0.8] {$\mathbb{E}_1$}
    node[left, pos=0.15] {$\mathbb{E}_1^*$};
    \draw (2) -> (3)
    node[above, pos=0.75] {$\mathbb{E}_2$}
    node[above, pos=0.1] {$\mathbb{E}_2^*$};
    \draw (3) -> (1)
    node[above, pos=0.75] {$\mathbb{E}_3$}
    node[above, pos=0.15] {$\mathbb{E}_3^*$};
    \draw (3) -> (4)
    node[above, pos=0.8] {$\mathbb{E}_4$}
    node[above, pos=0.15] {$\mathbb{E}_4^*$};
    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.28\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(2.5, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
    node[above, pos=0.45] {$\mathcal{C}_G(\mathcal{G}_1, \mathcal{G}_2, \mathcal{G}_3, \mathcal{G}_4)$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.25\textwidth}
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at(0,0) {$T$};
   \node(i1) at (-1, 0) {$\mathbb{V}_1$};
    \node(i2) at (-0.4, 1) {$\mathbb{V}_2$};
    \node(i3) at (0.4, 1) {$\mathbb{V}_3$};
    \node(i4) at (1, 0) {$\mathbb{V}_4$};
    \draw (T) -- (i1);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i4);
    \end{tikzpicture}
\end{minipage}
\caption{
    Example of a tensor network and a tensor network state evaluation using the Penrose notation.
}
\label{fig:tn_graph}
\end{figure}

By picking some tensors $\mathcal{G}_i \in \xi_i$ and evaluating them through $\mathcal{C}_G$
we get a tensor $T$ that we will call \textbf{tensor network state}. We will call the tensors $\mathcal{G}_i$
\textbf{core tensors}. 

\begin{definition}
    A tensor $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ is a tensor state of a tensor network if it
    can be written as $T = \mathcal{C}_G(\mathcal{G}_1 \otimes \cdots \otimes \mathcal{G}_n)$ with $\mathcal{G}_i \in \xi_i$
\end{definition}

\begin{definition}
    We will define the set of all possible tensor states 
    of a tensor network as the set $\TNS(G; \mathbb{E}_1, \dots, \mathbb{E}_c, \mathbb{V}_1, \dots, \mathbb{V}_n)$, i.e
$$
    \TNS(G; \mathbb{E}_1, \dots, \mathbb{E}_c, \mathbb{V}_1, \dots, \mathbb{V}_n) := \Bigg\{ \kappa_G(\mathcal{G}_1 \otimes \cdots \otimes \mathcal{G}_n) \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n : \mathcal{G}_i \in \xi_i \Bigg\}
$$
\end{definition}
The vector spaces $\mathbb{E}_1, \dots, \mathbb{E}_c, \mathbb{V}_1, \dots, \mathbb{V}_n$ are not really important 
on the Penrose representation of the tensor network, and we will usually not write them. Also, since all vector spaces are determined up to isomorphism by its dimension, 
we will write the tensor network as $\TNS(G; R_1, \dots, R_c, N_1, \dots, N_n)$. 

And since $n$ is equal to the number of vertices of $G$ and $c$ is equal to the number
of edges of $G$, we will write $\TNS(G; R)$ for a more compact notation.

\begin{definition}
    Given a tensor network state $\TNS(G, R)$, from the graph given by its Penrose Notation,
    we will call the edges with a dangling end \textit{free edges}, and the edges that connect two vertex \textit{contracted edges}
\end{definition}
For example, in \figref{fig:tn_graph}, the edges labeled as $\mathbb{V}_1, \mathbb{V}_2, \mathbb{V}_3, \mathbb{V}_4$ are 
free edges and the rest are contracted edges.


\section{Common Tensor network structures}

\begin{example}[Canonical polyadic decomposition]
    As we have seen, canonocal polyadic decomposition consists on decomposing $T$ as a sum of $r$ $1$-rank tensors. We can write
    $$\sum_{p=1}^r \lambda_p v_i^1 \otimes v_i^2 \otimes \cdots \otimes v_i^n$$
    As the contraction of a diagonal tensor $\Lambda$ of order $r$ with its enties being $\lambda_p$ and the other $1$-rank tensors. 
    If we denote $t_i = v_i^1 \otimes v_i^2 \otimes \cdots \otimes v_i^n$ so that the decomposition can be written as
    $\sum_{p=1}^r  \lambda_p t_p$, the resulting tensor network has the following star shape:

    \begin{center}
    \begin{tikzpicture}
        \node (1) [draw, shape=circle, minimum size=0.5cm] at (0, 1.5) {$t_2$};
        \node (2) [draw, shape=circle, minimum size=0.5cm] at (1.5, 0.75) {$t_3$};
        \node (3) [draw, shape=circle, minimum size=0.5cm] at (-1.5, 0.75) {$t_1$};
        \node (4) [draw, shape=circle, minimum size=0.5cm] at (1.5, -0.75) {$t_4$};
        \node (5) [draw, shape=circle, minimum size=0.5cm] at (-1.5, -0.75) {$t_r$};
        \node (6) [shape=circle, minimum size=0.5cm] at (0, -1.5) {$\dots$};

        \node (o1) at (0, 2.5);
        \node (o2) at (1.5 * 1.5, 0.75 * 1.5); 
        \node (o3) at (-1.5 * 1.5, 0.75 * 1.5);
        \node (o4) at (1.5 * 1.5, -0.75 * 1.5);
        \node (o5) at (-1.5 * 1.5, -0.75 * 1.5);
        \node (o6) at (0, -2.5); 

        \node(l)[fill, color=lightgray, shape=circle] at (0, 0) {};
        \node(l1) at (0.2, 0.4) {$\Lambda$};

        \foreach \x in {1,2,3,4,5,6} {
            \draw (\x) -- (l);
            \draw (o\x.center) -- (\x);
        }

    \end{tikzpicture}
    \end{center}
\end{example}

\begin{example}
    \nocite{oseledetsTensorTrainDecomposition2011} A tensor
train decomposition or matrix product state of $T$ are a set of $3$th-order tensors $\mathcal{G}_1,\mathcal{G}_2,\dots,\mathcal{G}_n$ with
    $\mathcal{G}_i \in \mathbb{K}^{R_{i-1} \times N_i \times R_i}$ and $R_0 = R_n = 1$ such that every element of $T$ is written in the
    form
    \begin{equation} \label{eq:tt-contraction}
    T(i_1,i_2,\dots,i_n) = \sum_{r_0, \dots, r_n}^{R_0, \dots, R_n} \mathcal{G}_1(r_0, i_1, r_1) \mathcal{G}_2 (r_1, i_2, r_2) \cdots \mathcal{G}_n(r_{n-1}, i_n, r_n)
\end{equation}
We denote $R_0, R_1, \dots, R_n$ as the ranks of the tensor train decomposition, or TT-ranks.
\end{example}

We can easily see that the tensor train decomposition (or TT) is obtained by our definition of a tensor network when $G$ is a path, also
the contraction of the whole network yields \refeq{eq:tt-contraction}

\begin{figure}[h]
    \centering
   \begin{minipage}{0.3\textwidth}
\begin{tikzpicture}[scale=0.85, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-4,0) {$\mathcal{G}_1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (-1,0) {$\mathcal{G}_2$};
    \node(3)[shape=circle, minimum size=0.8cm] at (2,0) {$\dots$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (5,0) {$\mathcal{G}_n$};
    \node(i1) at (-4, 1) {$N_1$};
    \node(i2) at (-1, 1) {$N_2$};
    \node(i4) at (5, 1) {$N_n$};


    \draw (1) -> (2)
    node[above, pos=0.5] {$R_1$}
    \draw (2) -> (3)
    node[above, pos=0.5] {$R_2$}
    \draw (3) -> (4)
    node[above, pos=0.5] {$R_n$}
    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (4) -- (i4);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.15\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1, -0.5);
    \node(2) at(1, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\mathcal{C}_G$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.25\textwidth}
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at(0,0) {$T$};
   \node(i1) at (-1, 0) {$N_1$};
    \node(i2) at (-0.4, 1) {$N_2$};
    \node(i3) at (0.4, 1) {$\dots$};
    \node(i4) at (1, 0) {$N_n$};
    \draw (T) -- (i1);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i4);
    \end{tikzpicture}
\end{minipage}

    \caption{Tensor Train decomposition}
    \label{tt:scheme}
\end{figure}

Tensor train decompositions are very well studied because since they are a chain of products of matrices, there are a lot
of mathematical theory that can be applied in this structure, for example, for finding the cores of the TT decomposition
one could fix a core, reshape and unfold onto a matrix the objective tensor $T$, apply then SVD and update the cores according to
the decomposition. A more detailed algorithm can be found in \cite{rohrig-zollnerPerformanceLowrankTensortrain2022}. Since in this thesis
is more general and not focused on tensor train decompositions we will skip this algorithm.

\iffalse
\begin{theorem}
    \normalfont{\cite{oseledetsTensorTrainDecomposition2011}}
    Given the unfoldings $A_k = \mathcal{T}_{[1:k, k:n]}$, if we choose $\rank{A_k} = r_k$ then there exists a $TT$-decomposition
    with its ranks not higher than $r_k$
\end{theorem}
\fi


\begin{example}
    \nocite{zhaoTensorRingDecomposition2016} Tensor ring decomposition (or TR) or also known a matrix product state with periodic boundary conditions, is obtained when $G$ is a cycle.
\end{example}
Tensor Ring decomposition is considered generalization of Tensor Train decomposition, it's contraction is the same as
\refeq{eq:tt-contraction} but removing the condition $R_0 = R_1 = 1$. Tensor Rings have also been widely studied
because of their circular invariance: shifting the cores on the tensor network results in an equivalent tensor network
that once its evaluated results in the objective tensor with its dimensions fixed. Thanks to this fact, one can still
apply SVD decomposition on tensor rings \cite{zhaoTensorRingDecomposition2016}.

\begin{figure}[h]
    \centering
   \begin{minipage}{0\textwidth}
\begin{tikzpicture}[scale=0.85, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-1,2) {$\mathcal{G}_1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (1,2) {$\mathcal{G}_2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (2,0) {$\mathcal{G}_3$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (1,-2) {$\mathcal{G}_4$};
    \node(5)[shape=circle, minimum size=0.8cm] at (-1,-2) {$\dots$};
    \node(n)[draw, shape=circle, minimum size=0.8cm] at (-2,0) {$\mathcal{G}_n$};

    \node(i1) at (-1.5, 3) {$N_1$};
    \node(i2) at (1.5, 3) {$N_2$};
    \node(i3) at (3.25, 0) {$N_3$};
    \node(i4) at (1.5, -3) {$N_4$};
    \node(in) at (-3.25, 0) {$N_n$};

    \draw (1) -> (2)
    node[above, pos=0.5] {$R_1$}
    \draw (2) -> (3)
    node[above right, pos=0.5] {$R_2$}
    \draw (3) -> (4)
    node[below right, pos=0.5] {$R_3$}
    \draw (4) -> (5)
    node[below, pos=0.5] {$R_4$}
    \draw (5) -> (n)
        node[right, pos=0.5] {$R_{n-1}$}
    \draw (n) -> (1)
    node[below right, pos=0.5] {$R_n$}

    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
    \draw[--] (n) -- (in);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.235\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(1.5, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\mathcal{C}_G$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.3\textwidth}
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at(0,0) {$T$};
   \node(i1) at (-1, 0) {$N_1$};
    \node(i2) at (-0.4, 1) {$N_2$};
    \node(i3) at (0.4, 1) {$\dots$};
    \node(i4) at (1, 0) {$N_n$};
    \draw (T) -- (i1);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i4);
    \end{tikzpicture}
\end{minipage}

    \caption{Tensor Ring (TR) decomposition}
    \label{tr:schema}
\end{figure}


\begin{example}
The fully connected tensor network decomposition is obtenied when $G$ is a complete graph.
\end{example}

\begin{figure}[h]

    \centering
   \begin{minipage}{0\textwidth}
\begin{tikzpicture}[scale=0.75, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-1.5,3) {$\mathcal{G}_1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (1.5,3) {$\mathcal{G}_2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (3,0) {$\mathcal{G}_3$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (1.5,-3) {$\mathcal{G}_4$};
    \node(5)[shape=circle, minimum size=0.8cm] at (-1.5,-3) {$\dots$};
    \node(n)[draw, shape=circle, minimum size=0.8cm] at (-3,0) {$\mathcal{G}_n$};

    \node(i1) at (-2, 4) {$N_1$};
    \node(i2) at (2, 4) {$N_2$};
    \node(i3) at (4.5, 0) {$N_3$};
    \node(i4) at (2, -4.5) {$N_4$};
    \node(in) at (-4.5, 0) {$N_n$};

    \draw (1) -> (2)
    \draw (2) -> (3)
    \draw (3) -> (4)
    \draw (4) -> (5)
    \draw (5) -> (n)
    \draw (n) -> (1)

    \draw (1) -> (3)
    \draw (1) -> (4)

    \draw (2) -> (n)
    \draw (2) -> (4)

    \draw (3) -> (n)

    \draw (1) -> (5)
    \draw (2) -> (5)
    \draw (3) -> (5)


    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
    \draw[--] (n) -- (in);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.235\textwidth}
    \begin{tikzpicture}
    \node(1) at(-1.5, -0.5);
    \node(2) at(1.5, -0.5);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2)
        node[above, pos=0.45] {$\mathcal{C}_G$};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.3\textwidth}
    \begin{tikzpicture}
        \node(T)[draw, shape=circle, minimum size=0.8cm] at(0,0) {$T$};
   \node(i1) at (-1, 0) {$N_1$};
    \node(i2) at (-0.4, 1) {$N_2$};
    \node(i3) at (0.4, 1) {$\dots$};
    \node(i4) at (1, 0) {$N_n$};
    \draw (T) -- (i1);
    \draw (T) -- (i2);
    \draw (T) -- (i3);
    \draw (T) -- (i4);
    \end{tikzpicture}
\end{minipage}

    \caption{Fully connected tensor network decomposition (FCTN)}
    \label{tctn:schema}
\end{figure}

\begin{example}
    A tree tensor network decomposition is obtained when $G$ is a tree graph. We will see that tensor tree networks have the 
    the nice property that if we can represent a tensor $T$ in some state of the network, the $G$-rank of $T$ is unique. We
    will see this in more detail on the next section.
\end{example}

\begin{figure}[H]
    \centering
\begin{tikzpicture}[scale=0.75, arrows = {-Latex[length=8pt, inset=2pt]}]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (0,4) {$\mathcal{G}_1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (-2,1) {$\mathcal{G}_2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (0,1) {$\mathcal{G}_3$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (2,1) {$\mathcal{G}_4$};
    \node(5)[draw, shape=circle, minimum size=0.8cm] at (-5,-2) {$\mathcal{G}_5$};
    \node(6)[draw, shape=circle, minimum size=0.8cm] at (-3,-2) {$\mathcal{G}_6$};
    \node(7)[draw, shape=circle, minimum size=0.8cm] at (-1,-2) {$\mathcal{G}_7$};
    \node(8)[draw, shape=circle, minimum size=0.8cm] at (2,-2) {$\mathcal{G}_8$};

    \node(i1) at (-1.5, 4) {$N_1$};
    \node(i2) at (-3.5, 1) {$N_2$};
    \node(i3) at (0, -0.5) {$N_3$};
    \node(i4) at (3.5, 1) {$N_4$};
    \node(i5) at (-5, -3.5) {$N_5$};
    \node(i6) at (-3, -3.5) {$N_6$};
    \node(i7) at (-1, -3.5) {$N_7$};
    \node(i8) at (2, -3.5) {$N_8$};

    \draw (1) -> (2);
    \draw (1) -> (3);
    \draw (1) -> (4);
    
    \draw (2) -> (5);
    \draw (2) -> (6);
    \draw (2) -> (7);

    \draw (4) -> (8);

    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
    \draw[--] (5) -- (i5);
    \draw[--] (6) -- (i6);
    \draw[--] (7) -- (i7);
    \draw[--] (8) -- (i8);
\end{tikzpicture}

    \caption{A tree tensor network}
    \label{fig:tree_net}
\end{figure}



\section{Tensor Network Ranks}

In this section we will present the notion of $G$-rank of a tensor. The $G$-rank of a tensor is defined as the
minimum ranks that a tensor $T$ can be represented by a tensor state. Knowing the $G$-rank of a tensor will
be useful because we could find cores $\mathcal{G}_1, \dots, \mathcal{G}_n$ such that we could represent $T$, and surely
enough, with a enough sotisficated algorithm, it could converge to these core tensors. Our main goal will be
to try to estimate the $G$-rank of a tensor, since finding the $G$-rank explicitly is a very
hard task. Approximating the $G$-rank will be enough for us because in the next chapter
we will slightly vary the approximated $G$-rank for finding a good representation.

As we defined the rank for a tensor $T$ as the minimum number
of $1$-rank tensors that compose the tensor $T$, we will define the $G$-rank as the minimum ranks that a tensor network state
needs for representing $T$. More formally,

\begin{definition} [Tensor G-rank]
    Given a graph $G$, we define the tensor rank respect to a $G$ or $G$-rank as
        $$\rank_G (T) = \min {\{(R_1, \dots, R_c) \in \mathbb{N}^c : T \in \TNS(G; R_1, \dots, R_c, N_1, \dots, N_d)\}}$$
 
    Where $\min(S)$ with $S \subset \mathbb{N}^c$ denotes the minimal elements of $S$. We treat $\mathbb{N}^c$ with its usual
    partial order:
    $$ (a_1, \dots, a_c) \leqslant (b_1, \dots, b_c) \Longleftrightarrow a_1 \leqslant b_1, a_2 \leqslant b_2, \dots, a_c \leqslant b_c$$
    So for example if $S = \{(3,4,5), (2,1,3), (1,3,2)\}$, then $\min(S) = \{(2,1,3), (1,3,2)\}$
\end{definition}

Now, we will see that $\rank_G(T)$ is a finite set. The following theorem gives us that if we make $R_1, \dots, R_c$ big enough, 
every tensor $T$ can be a state of $\TNS(G; R_1, \dots, R_c, N_1, \dots, N_n)$.
In fact, these values that guarantee that $T$ is an state are $R_1 = \dots R_c = \rank T$
\begin{theorem} Let $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$ and let $G$ be a connected graph with $n$ vertices and $c$ edges.
    There exists $R_1, \dots, R_c \in \mathbb{N}$ such that
    $$T \in \TNS(G; R_1, \dots, R_c, N_1, \dots, N_d)$$
    in fact, we can choose $R_1 = \dots = R_c = \rank{T}$
    \label{thm:finiterank}
\end{theorem}

\begin{proof} Let $r = \rank{T}$. Then there exist $v_1^{(i)}, \dots, v_r^{(i)} \in \mathbb{V}_i, i = 1, \dots, n$ such that
    $$T = \sum_{p=1}^r v_1^{(p)} \otimes \cdots \otimes v_n^{(p)}$$
    We take $R_1 = \dots = R_c = r$ we take for each $i = 1, \dots, n$
    $$\mathcal{G}_i = \sum_{p=1}^r \left( \bigotimes\nolimits_{j \in \IN(i)} e_p^{(j)} \right) \otimes v_p^{(i)} \otimes 
    \left( \bigotimes\nolimits_{j \in \OUT(i)} e_p^{(j)*} \right)$$

    Now observe that for each $i = 1, \dots, n$ there exists an unique $h$ such that whenever $j \in \IN(i)\cap \OUT(i)$,
    $e_p^{(j)}$ and $e_p^{(j)*}$ contract and give $\delta_{pq}$, therefore the summand vanishes except when $p = q$.
    This together with the assumption that $G$ is a connected graph implies that $\kappa_G(\mathcal{G}_1 \otimes \cdots \otimes \mathcal{G}_n)$ reduces
    to a sum of terms of the form $v_p^{(1)} \otimes \cdots \otimes v_p^{(d)}$ for $p = 1, \dots, r$, which is of course $T$
\end{proof}

Now, by picking the ranks as $R_1 = \dots = R_c = \rank{T}$ is usually not optimal, since we will end up that our tensor network
will use more memory to represent $T$ than the memory we need to write $T$ itself.

We will proceed to show that the $G$-rank can be a lot more smaller than the tensor rank, and in fact, we will show some
examples of tensor networks that can represent tensors in a more efficient way than the canonical polyadic decomposition that we have
seen in chapter $2$.

The following theorem says that there exists some tensor networks and some tensors that make this claim true:
\begin{theorem}
    For $n \geqslant 3$ there exists a connected simple graph $G$ with $n$ vertices and $c$ edges
    such there exists a tensor $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_n$
    with tensor rank $\rank (T) = r$ is much larger than the $G$-rank $\rank_G(T) = (r_1, \dots, r_n)$. More specifically,
    $$r \gg r_1 + \dots + r_n$$
\end{theorem}
% TODO

The proof of this theorem can be found in \cite{yeTensorNetworkRanks2019}. In the paper the authors find that for 
representing a tensor network that contracts to a tensor $T \in (\mathbb{C}^{n\times n})^* \otimes (\mathbb{C}^{n \times n})^* \otimes \mathbb{C}^{n \times n}
\cong \mathbb{C}^{n\times n\times n}$ picking $G$ as $C_3$ and $P_3$ the inequality holds for $n = 3$. Then, the proof
constructs from $T$ for $n > 3$. We won't enter in more details onto the proof
of the theorem.

% TODO: Aixo pel capitol 4
%And finally, we will present an algorithm that tries to compute an approximate of the real $G$-rank of a tensor,
%if $G$ is a tree.

 % TODO





\section{Contracting tensor networks}

We will finish this chapter by giving an algorithm for contracting (or evaluating) a tensor network.

Evaluating a tensor network is as easy as contracting each edge of $G$. The order in which we contract
does not affect the resulting tensor, as we can see from Equation \eqref{eq:tensor_product}. But instead
the order affects the computational cost, as we can see in the following example:

\begin{example} Consider the following tensor train network:

\begin{center}
    \begin{tikzpicture}

    \node(1)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$X$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (3,0) {$Y$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (6,0) {$Z$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (9,0) {$T$};

    \node(i1) at (0, 1) {$N_1$};
    \node(i2) at (3, 1) {$N_2$};
    \node(i3) at (6, 1) {$N_3$};
    \node(i4) at (9, 1) {$N_4$};

    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);

    \draw (1) -> (2)
    node[above, pos=0.5] {$R_1$};
    \draw (2) -> (3)
    node[above, pos=0.5] {$R_2$};
    \draw (3) -> (4)
    node[above, pos=0.5] {$R_3$};
    \end{tikzpicture}
\end{center}

The contraction of the whole network is given by

$$T_{ijkl} = \sum_{p,q,r}^{R_1, R_2, R_3} X_{ip} Y_{pjq} Z_{qkr} T_{rl}$$

We could contract first $R_1$, then $R_2$ and finally $R_3$. That means computing:
$$A_{ijq} = \sum_{p}^{R_1} X_{ip} Y_{pjq} \qquad B_{ijkr} = \sum_{q}^{R_2} A_{ijq} Z_{qkr} \qquad T_{ijkl} = \sum_{r}^{R_3} B_{ijkr} T_{rl}$$

For this computation, we would need $R_1 R_2 N_1 N_2 + R_2 R_3 N_1 N_2 N_3 + R_3 N_1 N_2 N_3 N_4$ products, and we would need to store
first $A$, then $B$ and then $T$

If we consider a different ordering for the contraction, for example first $R_1$, then $R_3$ and finally $R_2$ we get a different computation cost.
We would need to compute the matrices

$$A'_{ijq} = \sum_{p}^{R_1} X_{ip} Y_{pjq} \qquad B'_{qkl} = \sum_{r}^{R_3} Z_{qkr} T_{rl} \qquad T_{ijkl} = \sum_{q}^{R_2} A'_{ijq} B'_{qkl}$$

The result of the contraction is indeed the same, but now instead the number of computations is
$R_1 R_2 N_1 N_2 + R_3 N_1 N_3 N_4 + R_2 N_1 N_2 N_3 N_4$. 

\end{example}

In \cite{schindlerAlgorithmsTensorNetwork2020}, the authors discuss different algorithms for finding the optimal contraction order. 
First, the exhaustive search of the order is considered. It consists in searching all the possible contraction orders and finding the
one that has less computational cost. The authors show that the exhaustive search algorithm scales like $\mathcal{O}(e^E)$ where $E$ is the number
of edges of the network.

In our case, we will use a greedy search algorithm which consists on contracting over one edge at a time and for each
contraction step we take the one with the lowest cost. We can see $k$ steps further
and pick the one that has the less incremental cost. The computational complexity of the greedy algorithm scales lake
$\mathcal{O}(E^k)$ where $k$ is the number of next steps that we consider when picking the next contraction.

We will finish this section by providing a pseudocode of the greedy algorithm for contraction order selection:

\begin{algorithm}[H]
    \caption{Greedy contraction order selection}

    \hspace*{\algorithmicindent} \textbf{Input}: A tensor $T \in \mathbb{K}^{N_1 \times \cdots \times N_n}$ and some fixed error $\epsilon$ \\
    \hspace*{\algorithmicindent} \textbf{Output}: Core tensors $\mathcal{G}_1, \dots, \mathcal{G}_n$ 

    \begin{algorithmic}[1]
        \State Initialize tensors $\mathcal{G}_1, \dots, \mathcal{G}_n$
        \While{$\|T - \kappa_G (\mathcal{G}_1, \dots, \mathcal{G}_n)\|_F > \epsilon$ }
            \For {$k = 1, \dots, n$}
            \State $\mathcal{G}_m \leftarrow \argmin_{G_m} \|G^{\neq m} G_m - T^{(m)}\|_2$
            \EndFor
        \EndWhile
        \State \Return $\mathcal{G}_1, \dots, \mathcal{G}_n$

    \end{algorithmic}

\end{algorithm}

% TODO: Posar algunes gràfiques per aqui?


\chapter{Tensor network state search}

In this chapter we will study methods for finding optimal $TNS$ such that $T \in \TNS(G; R)$ or at least
$T + E \in \TNS(G; R)$ where $E$ is a tensor such that $\|E\|_F \simeq 0$ that describes the absolute error of the representation
of the state.

Suppose that we have already found some optimal $G$ and $R$ for representing $T$. We will 
start the chapter by describing the \textbf{alternated least squares} or ALS
algorithm. The goal of the ALS algorithm is finding cores $\mathcal{G}_1, \dots, \mathcal{G}_c$ of $\TNS(G; R)$ given $T, G$ and $R$.


\section{The Alternating Least Squares algorithm}

\nocite{malikSamplingBasedDecompositionAlgorithms2022a}

We will denote $T \in \mathbb{K}^{N_1 \times \cdots \times N_n}$ as our objective tensor. Let $G = (V, E)$, $c = \#E$ and $R = (r_1, \dots, r_c)$.
We would want to find some cores
$\mathcal{G}_1, \dots, \mathcal{G}_c$ of $\TNS(G; R)$ such that minimize
$$\|T - \mathcal{C}_G (\mathcal{G}_1, \dots, \mathcal{G}_c)\|_F$$
If we impose that all the tensor cores remain fixed except $\mathcal{G}_m$, our problem would become
$$\argmin_{\mathcal{G}_m} \|T - \mathcal{C}_G(\mathcal{G}_1, \dots, \mathcal{G}_n)\|_F$$
Now, we apply our contraction mapping $\mathcal{C}_G$ for all cores excluding $\mathcal{G}_m$ (\figref{fig:als_tr,fig:als_con}).
We will call this contracted tensor $\mathcal{G}^{\neq m}$. 

\begin{figure}[H]

    \centering
   \begin{minipage}{0\textwidth}
       \begin{tikzpicture}[scale=0.85]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (-1,2) {$\mathcal{G}_2$};
    \node(2)[draw, shape=circle, fill=lightgray, minimum size=0.8cm] at (1,2) {$\mathcal{G}_3$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (2,0) {$\mathcal{G}_4$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (1,-2) {$\mathcal{G}_5$};
    \node(5)[draw, shape=circle, minimum size=0.8cm] at (-1,-2) {$\mathcal{G}_6$};
    \node(n)[draw, shape=circle, minimum size=0.8cm] at (-2,0) {$\mathcal{G}_1$};

\node(I) at(2.5, 4) {$\kappa_G(\mathcal{G}_1, \dots, \mathcal{G}_n)$}

    \node(i1) at (-1.5, 3) {$N_2$};
    \node(i2) at (1.5, 3) {$N_3$};
    \node(i3) at (3.25, 0) {$N_4$};
    \node(i4) at (1.5, -3) {$N_5$};
    \node(i5) at (-1.5, -3) {$N_6$};
    \node(in) at (-3.25, 0) {$N_1$};

    \draw (1) -> (2)
    node[above, pos=0.5] {$R_2$}
    \draw (2) -> (3)
    node[above right, pos=0.5] {$R_3$}
    \draw (3) -> (4)
    node[below right, pos=0.5] {$R_4$}
    \draw (4) -> (5)
    node[below, pos=0.5] {$R_5$}
    \draw (5) -> (n)
        node[right, pos=0.5] {$R_6$}
    \draw (n) -> (1)
    node[below right, pos=0.5] {$R_1$}

    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
    \draw[--] (5) -- (i5);
    \draw[--] (n) -- (in);
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.235\textwidth}
    \begin{tikzpicture}
        \node(a) at (-2, 0);
    \node(1) at(-1.5, 0);
    \node(2) at(1.25, 0);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2);
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.35\textwidth}
    \begin{tikzpicture}[scale=0.85]
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (0,4) {$\mathcal{G}_4$};
    \node(4)[draw, shape=circle, minimum size=0.8cm] at (0,2) {$\mathcal{G}_5$};
    \node(5)[draw, shape=circle, minimum size=0.8cm] at (0, 0) {$\mathcal{G}_6$};
    \node(0)[draw, shape=circle, minimum size=0.8cm] at (0, -2) {$\mathcal{G}_1$};
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (0, -4) {$\mathcal{G}_2$};

    \node(ii1) at (0, 5) {};
    \node(i3) at (-1.5, 4) {$N_4$};
    \node(i4) at (-1.5, 2) {$N_5$};
    \node(i5) at (-1.5, 0) {$N_6$};
    \node(i0) at (-1.5, -2) {$N_1$};
    \node(i1) at (-1.5, -4) {$N_2$};
    \node(ii2) at (0, -5) {};

    \node(I) at (2, 4) {$\mathcal{G}^{\neq 2}$};

    \draw (3) -> (4)
    node[right, pos=0.5] {$R_4$}
    \draw (4) -> (5)
    node[right, pos=0.5] {$R_5$}
    \draw (5) -> (0)
    node[right, pos=0.5] {$R_6$}
    \draw (0) -> (1)
    node[right, pos=0.5] {$R_1$}
    \draw[--] (1) -- (ii2)
    node[right] {$R_3$};
    \draw[--] (ii1) -- (3)
    node[above right, pos=1] {$R_2$};



    \draw[--] (1) -- (i1);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
    \draw[--] (5) -- (i5);
    \draw[--] (0) -- (i0);
    \end{tikzpicture}
\end{minipage}

\caption{The representation of $\mathcal{G}^{\neq m}$ on the TR decomposition, with $m = 2$ }
    \label{fig:als_tr}
\end{figure}
%
%
%
%
%
%
%
%
\begin{figure}[H]
    \centering
   \begin{minipage}{0\textwidth}
       \begin{tikzpicture}[scale=0.8]
    \node(1)[draw, shape=circle, minimum size=0.8cm] at (0,4) {$\mathcal{G}_1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (3.5,4) {$\mathcal{G}_2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (-2,2) {$\mathcal{G}_3$};
    \node(4)[draw, fill=lightgray, shape=circle, minimum size=0.8cm] at (1.75,2) {$\mathcal{G}_4$};
    \node(5)[draw, shape=circle, minimum size=0.8cm] at (5,2) {$\mathcal{G}_5$};
    \node(6)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{G}_6$};
    \node(7)[draw, shape=circle, minimum size=0.8cm] at (3.5,0) {$\mathcal{G}_7$};


    \node(I) at(1.75, 6.5) {$\kappa_G(\mathcal{G}_1, \dots, \mathcal{G}_n)$}

    \node(i1) at (0, 5.25) {$N_1$};
    \node(i2) at (3.5, 5.25) {$N_2$};
    \node(i3) at (-2, 3.25) {$N_3$};
    \node(i4) at (0.75, 2.75) {$N_4$};
    \node(i5) at (5, 3.25) {$N_5$};
    \node(i6) at (0, -1.25) {$N_6$};
    \node(i7) at (3.5, -1.25) {$N_7$};

    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (4) -- (i4);
    \draw[--] (5) -- (i5);
    \draw[--] (6) -- (i6);
    \draw[--] (7) -- (i7);

    \draw (1) -> (2)
    node[above, pos=0.5] {$R_1$};
    \draw (2) -> (5)
    node[below left, pos=0.5] {$R_2$};
    \draw (4) -> (2)
    node[above left, pos=0.5] {$R_3$};
    \draw (3) -> (6)
    node[below left, pos=0.5] {$R_4$};
    \draw (6) -> (4)
    node[above left, pos=0.5] {$R_5$};
    \draw (4) -> (7)
    node[left, pos=0.5] {$R_6$};
    \draw (7) -> (5)
    node[above left, pos=0.5] {$R_7$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.235\textwidth}
    \begin{tikzpicture}
        \node(a) at (-2, 1);
    \node(1) at(-1.5, 0);
    \node(2) at(1.25, 0);
    \draw[-{Stealth[length=12pt, inset=2pt]}] (1) -- (2);
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.35\textwidth}
    \begin{tikzpicture}[scale=0.8]
    
        \node(I) at(1.75, 6.5) {$\mathcal{G}^{\neq 4}$}

    \node(1)[draw, shape=circle, minimum size=0.8cm] at (0,4) {$\mathcal{G}_1$};
    \node(2)[draw, shape=circle, minimum size=0.8cm] at (3.5,4) {$\mathcal{G}_2$};
    \node(3)[draw, shape=circle, minimum size=0.8cm] at (-2,2) {$\mathcal{G}_3$};
    \node(4)[shape=circle, minimum size=0.8cm] at (1.75,2);
    \node(5)[draw, shape=circle, minimum size=0.8cm] at (5,2) {$\mathcal{G}_5$};
    \node(6)[draw, shape=circle, minimum size=0.8cm] at (0,0) {$\mathcal{G}_6$};
    \node(7)[draw, shape=circle, minimum size=0.8cm] at (3.5,0) {$\mathcal{G}_7$};

    \node(i1) at (0, 5.25) {$N_1$};
    \node(i2) at (3.5, 5.25) {$N_2$};
    \node(i3) at (-2, 3.25) {$N_3$};
    \node(i5) at (5, 3.25) {$N_5$};
    \node(i6) at (0, -1.25) {$N_6$};
    \node(i7) at (3.5, -1.25) {$N_7$};

    \draw[--] (1) -- (i1);
    \draw[--] (2) -- (i2);
    \draw[--] (3) -- (i3);
    \draw[--] (5) -- (i5);
    \draw[--] (6) -- (i6);
    \draw[--] (7) -- (i7);

    \draw (1) -> (2)
    node[above, pos=0.5] {$R_1$};
    \draw (2) -> (5)
    node[below left, pos=0.5] {$R_2$};
    \draw (4) -> (2)
    node[above left, pos=0.5] {$R_3$};
    \draw (3) -> (6)
    node[below left, pos=0.5] {$R_4$};
    \draw (6) -> (4)
    node[above left, pos=0.5] {$R_5$};
    \draw (4) -> (7)
    node[left, pos=0.5] {$R_6$};
    \draw (7) -> (5)
    node[above left, pos=0.5] {$R_7$};

    \end{tikzpicture}
\end{minipage}

\caption{The representation of $\mathcal{G}^{\neq 4}$ over some arbitrary TN }
    \label{fig:als_con}
\end{figure}


The last contractions that remain between $\mathcal{G}^{\neq m}$ and $\mathcal{G}_m$ are equivalent to evaluating the whole network.
Now, if we consider appropiate matricizations $T^{(m)}$, $G^{\neq m}$ and $G_m$, these contractions can be computed calculating the matrix
product $G^{\neq m} G_m$, so our minimization problem is now equivalent to solving the 
following least squares problem:
\begin{equation}
    \argmin_{G_m} \| G^{\neq m} G_m - T^{(m)}\|_F
\label{eq:als}
\end{equation}
Let $x^{(i)}$ be the $i$-th column of $G_m$ and $y^{(i)}$ the $i$-th
column of $T^{(m)}$. Solving \ref{eq:als} means solving for each $i$ 
\begin{equation}
    \argmin_{x^{(i)}} \| G^{\neq m} x^{(i)} - y^{(i)} \|_2
    \label{eq:als_lsq}
\end{equation}

Since we can't assure that there exists an exact solution to $G^{\neq m} x^{(i)} = y^{(i)}$,
we can use the solution to the normal equation $(G^{\neq m})^T G^{\neq m} x^{(i)} = (G^{\neq m})^T y^{(i)}$.

Once we have solved \eqref{eq:als_lsq}, we can reshape back $G_m$ to $\mathcal{G}_m$.
We can iteratively change the varying core tensor $\mathcal{G}_i$ until the contraction of the whole tensor network
is $T$ with some fixed error $\epsilon$. The Tensor Network ALS algorithm does this entire process:

\begin{algorithm}[H]
    \caption{Tensor Network ALS}

    \hspace*{\algorithmicindent} \textbf{Input}: A tensor $T \in \mathbb{K}^{N_1 \times \cdots \times N_n}$ and some fixed error $\epsilon$ \\
    \hspace*{\algorithmicindent} \textbf{Output}: Core tensors $\mathcal{G}_1, \dots, \mathcal{G}_n$ 

    \begin{algorithmic}[1]
        \State Initialize tensors $\mathcal{G}_1, \dots, \mathcal{G}_n$
        \While{$\|T - \kappa_G (\mathcal{G}_1, \dots, \mathcal{G}_n)\|_F > \epsilon$ }
            \For {$k = 1, \dots, n$}
            \State $\mathcal{G}_m \leftarrow \argmin_{G_m} \|G^{\neq m} G_m - T^{(m)}\|_2$
            \EndFor
        \EndWhile
        \State \Return $\mathcal{G}_1, \dots, \mathcal{G}_n$

    \end{algorithmic}

\end{algorithm}

% TODO: Potser caldria jugar una mica amb l'algoritme i motivar la següent secció!

\section{Structure search}

% TODO: Connectar això com deu mana. 

Tensor network structure search prioblem aims to generally find the most compressed tensor network
models for computational purposes while maintaining the expressivity of the network. That means, that our
tensor network is capable of giving good representations of the set of tensors that we want to represent with it.

By now, we know that for any tensor $T$, if we choose a graph $G = (V, E)$ we can choose 
$R_1, \dots, R_c \leqslant \rank{T} \leqslant \left\lfloor \frac{\prod_{i=1}^n N_i}{\sum_{i=1}^n N_i} \right\rfloor$
such that $T \in \TNS(G, R)$. We can also find thanks to the ALS algorithm the cores $\mathcal{G}_1, \dots, \mathcal{G}_n$ of 
$\TNS(G, R)$. The only thing that remains is how we pick an optimal $G$ for compressing $T$ with a fixed error $\epsilon$, since we know that
the optimal graph for a tensor network state of a given tensor $T$ depends on the underlying data of $T$ itself.

% We will aim to search the tensor network that gives the best compression ratio for $T$, for that, we will define the size 
% of a tensor network:

Suppose that we have defined a loss function $\pi_D : \mathbb{R}^{N_1 \times N_2 \times \cdots \times N_n} \rightarrow \mathbb{R}_+$
involving $D$, with $D$ being a dataset.
For example, if our objective is compressing an objective tensor $T_o \in \mathbb{R}^{N_1 \times N_2 \times \cdots \times N_n}$,
we could define our loss function as the relative error of our evaluated tensor respect to the objective tensor:
$$\pi_D(T) = \frac{\| T - T_o \|_F}{\| T_o \|_F} $$
The tensor network structure search problem is defined as solving the following discrete optimization problem:
\begin{equation}
\min_{(G, R) \in \mathbb{G} \times \mathbb{F}_G} \left(  \phi(G, R) + \lambda \cdot \min_{\mathcal{Z} \in TNS(G,R)} \pi_D(\mathcal{Z}) \right)
\label{eq:tnss}
\end{equation}
where $\mathbb{G}$ is the space of all graphs, $G \in \mathbb{G}$ is a graph of $N$ vertices and $K$ edges, ${R = (r_1, \dots, r_K) \in \mathbb{F}_G \subseteq
\mathbb{Z}_+^K}$ are the ranks of the tensor network, $\phi(G, R)$ represents a function that determines the complexity of the
tensor network, and $\lambda > 0$ is a tuning parameter.

The intuition behind presenting this optimization problem is that the inner minimization that depends on the tuning parameter $\lambda$
serves to evaluate the expressivity of the tensor network.

% TODO: Link works

We call the problem \ref{eq:tnss} as Tensor Network Structure Selection (TN-SS). TN-SS is a very generalized problem.
There has been a lot of research on solving it under certain conditions:
\begin{itemize}
    \item Tensor Network Rank Selection (TN-RS), it restricts $G$ to be a fixed graph, and its objective is to find the
tensor network ranks $R \in \mathbb{F}_G$.
    \item Tensor Network Permutation Selection (TN-PS) fixes the ranks $R$ and search over the set
of all the simple graphs that are isomorphic to $G$.
    \item Lastly, Tensor Network Topology Selection (TN-TS) searches over the set of all simple graphs of $N$ vertices and
        the tensor ranks $R$ are fixed
\end{itemize}

In this chapter we will present the tensor network structure search algorithm using alternating local enumeration (TnALE), which is based
on \cite{liAlternatingLocalEnumeration2023a}

The purpose of the TnALE algorithm is to solve \ref{eq:tnss}, but for each evaluation that it does, it needs another algorithm for evaluating
the inner minimum inside the equation. It requires that we already know how to compute $\min_{\mathcal{Z} \in TNS(G,R)} \pi_D (\mathcal{Z})$






% TODO:
% Suposo que començar per dir quines parts del graf caldria tallar maybe???
% Fer més representacions gràfiques de segons quina demostració com més clar quedi tot millor
% \begin{itemize}
% \item Descriure $G$-ranks
% \item Algorismes per aproximar TNS per $G$-ranks propers i mínims si es pot fer
% \item Algun algorisme per trobar heuristicament els $G$-ranks adequats? (suposo q depen de compressió ratio i l'error relatiu)
% \item Com podem trobar un $G$ adequat?
% \item Estratègies per contraure tensors més ràpidament? (DRMG?)
% \item Algorismes, part pràctica en C/C++
% \item Fer moltes gràfiques
% \item Fer aplicacions per machine learning, etc.
% \item Fixar la mathematical subject classification
% \end{itemize}



\chapter{Conclusions}

TODO

Fent servir un s\'{\i}mil geom\`etrico-cartogr\`afic, aquesta mem\`oria constitueix un mapa a escala planet\`aria de la demostraci\'o de la conjectura feble de Goldbach presentada per Helfgott i un mapa a escala continental de la verificaci\'o num\`erica d'aquesta. Estudis posteriors i m\'es profunds haurien de permetre elaborar mapes de menor escala.

La naturalesa dels nombres primers ens ha portat per molts racons diferents de les Matem\`atiques; en no imposar-nos restriccions en la forma de pensar, hem pogut gaudir del viatge i assolir els objectius que ens vam plantejar a l'inici del projecte i anar m\'es enll\`a, sobretot en el camp de la computaci\'o i la manipulaci\'o de grans volums de dades num\`eriques.

Una gran part dels coneixements b\`asics que hem hagut de fer servir han estat treballats en les assignatures de M\`etodes anal\'{\i}tics en teoria de nombres i d'An\`alisi harm\`onica i teoria del senyal, que s\'on optatives de quart curs del Grau de Ma\-te\-m\`a\-ti\-ques. Altres els hem hagut d'aprendre durant el desenvolupant del projecte. S'ha realitzat una tasca de recerca bibliogr\`afica important, consultant recursos antics i moderns, tant en format digital com en format paper.

\normalfont


\newpage

\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

\appendix
\chapter{Chapter 1}

%\section{Tensor contractions}

%\begin{definition}[Tensor contraction]
%    (https://math.stackexchange.com/questions/1792230/coordinate-free-notation-for-tensor-contraction)
%\end{definition}

%Let $T \in \mathbb{V}_1 \otimes \cdots \otimes \mathbb{V}_k \otimes \cdots \otimes \mathbb{V}_p \otimes \mathbb{W}_1^* \otimes \cdots
%\otimes \mathbb{W}_l \otimes \cdots \otimes \mathbb{W}_q$

 
%\chapter{Experimental results}
\end{document} 

