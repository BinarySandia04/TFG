@article{chenAutomaticDifferentiationSecond2020,
  title = {Automatic {{Differentiation}} for {{Second Renormalization}} of {{Tensor Networks}}},
  author = {Chen, Bin-Bin and Gao, Yuan and Guo, Yi-Bin and Liu, Yuzhi and Zhao, Hui-Hai and Liao, Hai-Jun and Wang, Lei and Xiang, Tao and Li, Wei and Xie, Z. Y.},
  date = {2020-06-23},
  journaltitle = {Physical Review B},
  shortjournal = {Phys. Rev. B},
  volume = {101},
  number = {22},
  eprint = {1912.02780},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  pages = {220409},
  issn = {2469-9950, 2469-9969},
  doi = {10.1103/PhysRevB.101.220409},
  url = {http://arxiv.org/abs/1912.02780},
  urldate = {2025-02-25},
  abstract = {Tensor renormalization group (TRG) constitutes an important methodology for accurate simulations of strongly correlated lattice models. Facilitated by the automatic differentiation technique widely used in deep learning, we propose a uniform framework of differentiable TRG (\$\textbackslash partial\$TRG) that can be applied to improve various TRG methods, in an automatic fashion. Essentially, \$\textbackslash partial\$TRG systematically extends the concept of second renormalization [PRL 103, 160601 (2009)] where the tensor environment is computed recursively in the backward iteration, in the sense that given the forward process of TRG, \$\textbackslash partial\$TRG automatically finds the gradient through backpropagation, with which one can deeply "train" the tensor networks. We benchmark \$\textbackslash partial\$TRG in solving the square-lattice Ising model, and demonstrate its power by simulating one- and two-dimensional quantum systems at finite temperature. The deep optimization as well as GPU acceleration renders \$\textbackslash partial\$TRG manybody simulations with high efficiency and accuracy.},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Physics - Computational Physics},
  file = {/home/arans/Zotero/storage/M3Y5D2CY/Chen et al. - 2020 - Automatic Differentiation for Second Renormalization of Tensor Networks.pdf;/home/arans/Zotero/storage/JXYXR4DJ/1912.html}
}

@article{chenTENSORRINGNETS2018,
  title = {{{TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING}}},
  author = {Chen, Xinqi and Hou, Ming and Zhou, Guoxu and Zhao, Qibin},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=BJxmXhRcK7},
  urldate = {2025-02-25},
  abstract = {Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks. Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed. To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL). TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers. Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers. Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single task’s performance, particularly favourable in scenarios where some of the tasks have insufficient data.},
  langid = {english},
  file = {/home/arans/Zotero/storage/IYXBYTE5/Chen et al. - 2018 - TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING.pdf}
}

@online{evenblyPracticalGuideNumerical2022,
  title = {A {{Practical Guide}} to the {{Numerical Implementation}} of {{Tensor Networks I}}: {{Contractions}}, {{Decompositions}} and {{Gauge Freedom}}},
  shorttitle = {A {{Practical Guide}} to the {{Numerical Implementation}} of {{Tensor Networks I}}},
  author = {Evenbly, Glen},
  date = {2022-02-04},
  eprint = {2202.02138},
  eprinttype = {arXiv},
  eprintclass = {quant-ph},
  doi = {10.48550/arXiv.2202.02138},
  url = {http://arxiv.org/abs/2202.02138},
  urldate = {2025-02-25},
  abstract = {We present an overview of the key ideas and skills necessary to begin implementing tensor network methods numerically, which is intended to facilitate the practical application of tensor network methods for researchers that are already versed with their theoretical foundations. These skills include an introduction to the contraction of tensor networks, to optimal tensor decompositions, and to the manipulation of gauge degrees of freedom in tensor networks. The topics presented are of key importance to many common tensor network algorithms such as DMRG, TEBD, TRG, PEPS and MERA.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Quantum Physics},
  file = {/home/arans/Zotero/storage/J3MS9XD2/Evenbly - 2022 - A Practical Guide to the Numerical Implementation of Tensor Networks I Contractions, Decompositions.pdf;/home/arans/Zotero/storage/F6BELNLL/2202.html}
}

@online{gaborReducedStorageDirect2024,
  title = {Reduced Storage Direct Tensor Ring Decomposition for Convolutional Neural Networks Compression},
  author = {Gabor, Mateusz and Zdunek, Rafał},
  date = {2024-08-05},
  eprint = {2405.10802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.10802},
  url = {http://arxiv.org/abs/2405.10802},
  urldate = {2025-02-24},
  abstract = {Convolutional neural networks (CNNs) are among the most widely used machine learning models for computer vision tasks, such as image classification. To improve the efficiency of CNNs, many CNNs compressing approaches have been developed. Low-rank methods approximate the original convolutional kernel with a sequence of smaller convolutional kernels, which leads to reduced storage and time complexities. In this study, we propose a novel low-rank CNNs compression method that is based on reduced storage direct tensor ring decomposition (RSDTR). The proposed method offers a higher circular mode permutation flexibility, and it is characterized by large parameter and FLOPS compression rates, while preserving a good classification accuracy of the compressed network. The experiments, performed on the CIFAR-10 and ImageNet datasets, clearly demonstrate the efficiency of RSDTR in comparison to other state-of-the-art CNNs compression approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/Y6MFC3FS/Gabor y Zdunek - 2024 - Reduced storage direct tensor ring decomposition for convolutional neural networks compression.pdf;/home/arans/Zotero/storage/TDKT76RA/2405.html}
}

@online{heScalableRobustTensor2023,
  title = {Scalable and {{Robust Tensor Ring Decomposition}} for {{Large-scale Data}}},
  author = {He, Yicong and Atia, George K.},
  date = {2023-05-15},
  eprint = {2305.09044},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.09044},
  url = {http://arxiv.org/abs/2305.09044},
  urldate = {2025-02-24},
  abstract = {Tensor ring (TR) decomposition has recently received increased attention due to its superior expressive performance for high-order tensors. However, the applicability of traditional TR decomposition algorithms to real-world applications is hindered by prevalent large data sizes, missing entries, and corruption with outliers. In this work, we propose a scalable and robust TR decomposition algorithm capable of handling large-scale tensor data with missing entries and gross corruptions. We first develop a novel auto-weighted steepest descent method that can adaptively fill the missing entries and identify the outliers during the decomposition process. Further, taking advantage of the tensor ring model, we develop a novel fast Gram matrix computation (FGMC) approach and a randomized subtensor sketching (RStS) strategy which yield significant reduction in storage and computational complexity. Experimental results demonstrate that the proposed method outperforms existing TR decomposition methods in the presence of outliers, and runs significantly faster than existing robust tensor completion algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/QLD2LQLH/He y Atia - 2023 - Scalable and Robust Tensor Ring Decomposition for Large-scale Data.pdf;/home/arans/Zotero/storage/Z4NM82JZ/2305.html}
}

@article{liaoDifferentiableProgrammingTensor2019,
  title = {Differentiable {{Programming Tensor Networks}}},
  author = {Liao, Hai-Jun and Liu, Jin-Guo and Wang, Lei and Xiang, Tao},
  date = {2019-09-05},
  journaltitle = {Physical Review X},
  shortjournal = {Phys. Rev. X},
  volume = {9},
  number = {3},
  eprint = {1903.09650},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  pages = {031041},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.9.031041},
  url = {http://arxiv.org/abs/1903.09650},
  urldate = {2025-02-24},
  abstract = {Differentiable programming is a fresh programming paradigm which composes parameterized algorithmic components and trains them using automatic differentiation (AD). The concept emerges from deep learning but is not only limited to training neural networks. We present theory and practice of programming tensor network algorithms in a fully differentiable way. By formulating the tensor network algorithm as a computation graph, one can compute higher order derivatives of the program accurately and efficiently using AD. We present essential techniques to differentiate through the tensor networks contractions, including stable AD for tensor decomposition and efficient backpropagation through fixed point iterations. As a demonstration, we compute the specific heat of the Ising model directly by taking the second order derivative of the free energy obtained in the tensor renormalization group calculation. Next, we perform gradient based variational optimization of infinite projected entangled pair states for quantum antiferromagnetic Heisenberg model and obtain start-of-the-art variational energy and magnetization with moderate efforts. Differentiable programming removes laborious human efforts in deriving and implementing analytical gradients for tensor network programs, which opens the door to more innovations in tensor network algorithms and applications.},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Quantum Physics},
  file = {/home/arans/Zotero/storage/T8TASW9M/Liao et al. - 2019 - Differentiable Programming Tensor Networks.pdf;/home/arans/Zotero/storage/8T8U272U/1903.html}
}

@inproceedings{malikSamplingBasedMethodTensor2021,
  title = {A {{Sampling-Based Method}} for {{Tensor Ring Decomposition}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Malik, Osman Asif and Becker, Stephen},
  date = {2021-07-01},
  pages = {7400--7411},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/malik21b.html},
  urldate = {2025-02-24},
  abstract = {We propose a sampling-based method for computing the tensor ring (TR) decomposition of a data tensor. The method uses leverage score sampled alternating least squares to fit the TR cores in an iterative fashion. By taking advantage of the special structure of TR tensors, we can efficiently estimate the leverage scores and attain a method which has complexity sublinear in the number of input tensor entries. We provide high-probability relative-error guarantees for the sampled least squares problems. We compare our proposal to existing methods in experiments on both synthetic and real data. Our method achieves substantial speedup—sometimes two or three orders of magnitude—over competing methods, while maintaining good accuracy. We also provide an example of how our method can be used for rapid feature extraction.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/arans/Zotero/storage/757ASAT6/Malik y Becker - 2021 - A Sampling-Based Method for Tensor Ring Decomposition.pdf;/home/arans/Zotero/storage/LHNSF2PE/Malik y Becker - 2021 - A Sampling-Based Method for Tensor Ring Decomposition.pdf}
}

@online{mickelinAlgorithmsComputingTensor2020,
  title = {On {{Algorithms}} for and {{Computing}} with the {{Tensor Ring Decomposition}}},
  author = {Mickelin, Oscar and Karaman, Sertac},
  date = {2020-02-10},
  eprint = {1807.02513},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1807.02513},
  url = {http://arxiv.org/abs/1807.02513},
  urldate = {2025-02-24},
  abstract = {Tensor decompositions such as the canonical format and the tensor train format have been widely utilized to reduce storage costs and operational complexities for high-dimensional data, achieving linear scaling with the input dimension instead of exponential scaling. In this paper, we investigate even lower storage-cost representations in the tensor ring format, which is an extension of the tensor train format with variable end-ranks. Firstly, we introduce two algorithms for converting a tensor in full format to tensor ring format with low storage cost. Secondly, we detail a rounding operation for tensor rings and show how this requires new definitions of common linear algebra operations in the format to obtain storage-cost savings. Lastly, we introduce algorithms for transforming the graph structure of graph-based tensor formats, with orders of magnitude lower complexity than existing literature. The efficiency of all algorithms is demonstrated on a number of numerical examples, and in certain cases, we demonstrate significantly higher compression ratios when compared to previous approaches to using the tensor ring format.},
  pubstate = {prepublished},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/NIR7VWQR/Mickelin y Karaman - 2020 - On Algorithms for and Computing with the Tensor Ring Decomposition.pdf;/home/arans/Zotero/storage/3VHN7VFP/1807.html}
}

@article{mishraIntelligentComputationalTechniques2024,
  title = {Intelligent Computational Techniques for Physical Object Properties Discovery, Detection, and Prediction: {{A}} Comprehensive Survey},
  shorttitle = {Intelligent Computational Techniques for Physical Object Properties Discovery, Detection, and Prediction},
  author = {Mishra, Shaili and Arora, Anuja},
  date = {2024-02-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {51},
  pages = {100609},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2023.100609},
  url = {https://www.sciencedirect.com/science/article/pii/S157401372300076X},
  urldate = {2025-02-25},
  abstract = {The exploding usage of physical object properties has greatly facilitated real-time applications such as robotics to perceive exactly as it appears in existence. Changes in the nature and properties of diverse real-time systems are associated with physical properties modification due to environmental factors. These physics-based object properties features attract the researchers’ attention while developing solutions to real-life problems. But, the detection and prediction of physical properties change are very diverse, covering many physics laws and object properties (material, shape, gravitational force, color, state change) which append complexity to these tasks. Instead of well-understood physics laws, elucidating physics laws requires substantial manual modeling with the help of standardized equations and associated factors. To adopt these physical laws to get instinctive and effective outcomes, researchers started applying computational models to learn changing property behavior as a substitute for using handcrafted and equipment-generated variable states. If physical properties detection challenges are not anticipated and required measures are not precluded, the upcoming computational model-driven physical object changing will not be able to serve appropriately. Therefore, this survey paper is drafted to demonstrate comprehensive theoretical and empirical studies of physical object properties detection and prediction. Furthermore, a generic paradigm is proposed to work in this direction along with characterization parameters of numerous physical object properties. A brief summarization of applicable machine learning, deep learning, and metaheuristic approaches is presented. An extensive list of released and openly available datasets for varying objects and parameters rendered for researchers. Additionally, performance measures regarding computational techniques for physical properties discovery and detection for quantitative evaluation of outcomes are also entailed. Finally, a few open research issues that need to be explored in the future are specified.},
  keywords = {Computational Intelligence,Deep learning,Machine learning,Meta-heuristic,Performance measures,Physical properties,Reinforcement learning},
  file = {/home/arans/Zotero/storage/VD6JUIJW/S157401372300076X.html}
}

@online{neillOverviewNeuralNetwork2020,
  title = {An {{Overview}} of {{Neural Network Compression}}},
  author = {Neill, James O'},
  date = {2020-08-01},
  eprint = {2006.03669},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.03669},
  url = {http://arxiv.org/abs/2006.03669},
  urldate = {2025-02-25},
  abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures\textbackslash footnote\{For an introduction to deep learning, see \textasciitilde\textbackslash citet\{goodfellow2016deep\}\}, namely, Recurrent Neural Networks\textasciitilde\textbackslash citep[(RNNs)][]\{rumelhart1985learning,hochreiter1997long\}, Convolutional Neural Networks\textasciitilde\textbackslash citep\{fukushima1980neocognitron\}\textasciitilde\textbackslash footnote\{For an up to date overview see\textasciitilde\textbackslash citet\{khan2019survey\}\} and Self-Attention based networks\textasciitilde\textbackslash citep\{vaswani2017attention\}\textbackslash footnote\{For a general overview of self-attention networks, see \textasciitilde\textbackslash citet\{chaudhari2019attentive\}.\},\textbackslash footnote\{For more detail and their use in natural language processing, see\textasciitilde\textbackslash citet\{hu2019introductory\}\}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/X7KW2L2H/Neill - 2020 - An Overview of Neural Network Compression.pdf;/home/arans/Zotero/storage/SVISX6KR/2006.html}
}

@online{obukhovTBasisCompactRepresentation2021,
  title = {T-{{Basis}}: A {{Compact Representation}} for {{Neural Networks}}},
  shorttitle = {T-{{Basis}}},
  author = {Obukhov, Anton and Rakhuba, Maxim and Georgoulis, Stamatios and Kanakis, Menelaos and Dai, Dengxin and Gool, Luc Van},
  date = {2021-07-13},
  eprint = {2007.06631},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.06631},
  url = {http://arxiv.org/abs/2007.06631},
  urldate = {2025-02-24},
  abstract = {We introduce T-Basis, a novel concept for a compact representation of a set of tensors, each of an arbitrary shape, which is often seen in Neural Networks. Each of the tensors in the set is modeled using Tensor Rings, though the concept applies to other Tensor Networks. Owing its name to the T-shape of nodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally shaped three-dimensional tensors, used to represent Tensor Ring nodes. Such representation allows us to parameterize the tensor set with a small number of parameters (coefficients of the T-Basis tensors), scaling logarithmically with each tensor's size in the set and linearly with the dimensionality of T-Basis. We evaluate the proposed approach on the task of neural network compression and demonstrate that it reaches high compression rates at acceptable performance drops. Finally, we analyze memory and operation requirements of the compressed networks and conclude that T-Basis networks are equally well suited for training and inference in resource-constrained environments and usage on the edge devices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/E5SFIX9L/Obukhov et al. - 2021 - T-Basis a Compact Representation for Neural Networks.pdf;/home/arans/Zotero/storage/G5MT2NQX/2007.html}
}

@online{taoScalableBayesianTensor2024,
  title = {Scalable {{Bayesian Tensor Ring Factorization}} for {{Multiway Data Analysis}}},
  author = {Tao, Zerui and Tanaka, Toshihisa and Zhao, Qibin},
  date = {2024-12-04},
  eprint = {2412.03321},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.03321},
  url = {http://arxiv.org/abs/2412.03321},
  urldate = {2025-02-24},
  abstract = {Tensor decompositions play a crucial role in numerous applications related to multi-way data analysis. By employing a Bayesian framework with sparsity-inducing priors, Bayesian Tensor Ring (BTR) factorization offers probabilistic estimates and an effective approach for automatically adapting the tensor ring rank during the learning process. However, previous BTR method employs an Automatic Relevance Determination (ARD) prior, which can lead to sub-optimal solutions. Besides, it solely focuses on continuous data, whereas many applications involve discrete data. More importantly, it relies on the Coordinate-Ascent Variational Inference (CAVI) algorithm, which is inadequate for handling large tensors with extensive observations. These limitations greatly limit its application scales and scopes, making it suitable only for small-scale problems, such as image/video completion. To address these issues, we propose a novel BTR model that incorporates a nonparametric Multiplicative Gamma Process (MGP) prior, known for its superior accuracy in identifying latent structures. To handle discrete data, we introduce the P\textbackslash 'olya-Gamma augmentation for closed-form updates. Furthermore, we develop an efficient Gibbs sampler for consistent posterior simulation, which reduces the computational complexity of previous VI algorithm by two orders, and an online EM algorithm that is scalable to extremely large tensors. To showcase the advantages of our model, we conduct extensive experiments on both simulation data and real-world applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/PUELZQNA/Tao et al. - 2024 - Scalable Bayesian Tensor Ring Factorization for Multiway Data Analysis.pdf;/home/arans/Zotero/storage/45CUHEIS/2412.html}
}

@online{wangWideCompressionTensor2018,
  title = {Wide {{Compression}}: {{Tensor Ring Nets}}},
  shorttitle = {Wide {{Compression}}},
  author = {Wang, Wenqi and Sun, Yifan and Eriksson, Brian and Wang, Wenlin and Aggarwal, Vaneet},
  date = {2018-02-25},
  eprint = {1802.09052},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.09052},
  url = {http://arxiv.org/abs/1802.09052},
  urldate = {2025-02-24},
  abstract = {Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep neural networks. Our results show that our TR-Nets approach \{is able to compress LeNet-5 by \$11\textbackslash times\$ without losing accuracy\}, and can compress the state-of-the-art Wide ResNet by \$243\textbackslash times\$ with only 2.3\textbackslash\% degradation in \{Cifar10 image classification\}. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/TG5JPJQP/Wang et al. - 2018 - Wide Compression Tensor Ring Nets.pdf;/home/arans/Zotero/storage/FIWUZQT4/1802.html}
}

@article{xieNeuralNetworkCompression2024,
  title = {Neural {{Network Compression Based}} on {{Tensor Ring Decomposition}}},
  author = {Xie, Kun and Liu, Can and Wang, Xin and Li, Xiaocan and Xie, Gaogang and Wen, Jigang and Li, Kenli},
  date = {2024},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--15},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2024.3383392},
  url = {https://ieeexplore.ieee.org/document/10510501},
  urldate = {2025-02-25},
  abstract = {Deep neural networks (DNNs) have made great breakthroughs and seen applications in many domains. However, the incomparable accuracy of DNNs is achieved with the cost of considerable memory consumption and high computational complexity, which restricts their deployment on conventional desktops and portable devices. To address this issue, low-rank factorization, which decomposes the neural network parameters into smaller sized matrices or tensors, has emerged as a promising technique for network compression. In this article, we propose leveraging the emerging tensor ring (TR) factorization to compress the neural network. We investigate the impact of both parameter tensor reshaping and TR decomposition (TRD) on the total number of compressed parameters. To achieve the maximal parameter compression, we propose an algorithm based on prime factorization that simultaneously identifies the optimal tensor reshaping and TRD. In addition, we discover that different execution orders of the core tensors result in varying computational complexities. To identify the optimal execution order, we construct a novel tree structure. Based on this structure, we propose a top-to-bottom splitting algorithm to schedule the execution of core tensors, thereby minimizing computational complexity. We have performed extensive experiments using three kinds of neural networks with three different datasets. The experimental results demonstrate that, compared with the three state-of-the-art algorithms for low-rank factorization, our algorithm can achieve better performance with much lower memory consumption and lower computational complexity.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Computational complexity,Kernel,Matrix decomposition,Neural network compression,Neural networks,Symbols,tensor ring (TR) factorization,Tensors},
  file = {/home/arans/Zotero/storage/EZFMK33Y/10510501.html}
}

@article{xuTRSTFFastAccurate2023,
  title = {{{TR-STF}}: A Fast and Accurate Tensor Ring Decomposition Algorithm via Defined Scaled Tri-Factorization},
  shorttitle = {{{TR-STF}}},
  author = {Xu, Ting and Huang, Ting-Zhu and Deng, Liang-Jian and Dou, Hong-Xia and Yokoya, Naoto},
  date = {2023-06-27},
  journaltitle = {Computational and Applied Mathematics},
  shortjournal = {Comp. Appl. Math.},
  volume = {42},
  number = {5},
  pages = {234},
  issn = {1807-0302},
  doi = {10.1007/s40314-023-02368-w},
  url = {https://doi.org/10.1007/s40314-023-02368-w},
  urldate = {2025-02-24},
  abstract = {This paper proposes an algorithm based on defined scaled tri-factorization (STF) for fast and accurate tensor ring (TR) decomposition. First, based on the fast tri-factorization approach, we define STF and design a corresponding algorithm that can more accurately represent various matrices while maintaining a similar level of computational time. Second, we apply sequential STFs to TR decomposition with theoretical proof and propose a stable (i.e., non-iterative) algorithm named TR-STF. It is a computationally more efficient algorithm than existing TR decomposition algorithms, which is beneficial when dealing with big data. Experiments on multiple randomly simulated data, highly oscillatory functions, and real-world data sets verify the effectiveness and high efficiency of the proposed TR-STF. For example, on the Pavia University data set, TR-STF is nearly 9240 and 39 times faster, respectively, and more accurate than algorithms based on alternating least squares and singular value decomposition. As an extension, we apply sequential STFs to tensor train (TT) decomposition and propose a non-iterative algorithm named TT-STF. Experimental results demonstrate the superiority of the proposed TT-STF compared with the state-of-the-art TT decomposition algorithm.},
  langid = {english},
  keywords = {68W99,Fast algorithm,Scaled tri-factorization,Tensor ring decomposition,Tensor train decomposition}
}

@online{yangCoMERAComputingMemoryEfficient2024,
  title = {{{CoMERA}}: {{Computing-}} and {{Memory-Efficient Training}} via {{Rank-Adaptive Tensor Optimization}}},
  shorttitle = {{{CoMERA}}},
  author = {Yang, Zi and Liu, Ziyue and Choudhary, Samridhi and Xie, Xinfeng and Gao, Cao and Kunzmann, Siegfried and Zhang, Zheng},
  date = {2024-12-02},
  eprint = {2405.14377},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.14377},
  url = {http://arxiv.org/abs/2405.14377},
  urldate = {2025-02-24},
  abstract = {Training large AI models such as LLMs and DLRMs costs massive GPUs and computing time. The high training cost has become only affordable to big tech companies, meanwhile also causing increasing concerns about the environmental impact. This paper presents CoMERA, a Computing- and Memory-Efficient training method via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive tensor-compressed (pre)-training via a multi-objective optimization formulation and improves the training to provide both a high compression ratio and excellent accuracy in the training process. Our optimized numerical computation (e.g., optimized tensorized embedding and tensor-network contractions) and GPU implementation eliminate part of the run-time overhead in the tensorized training on GPU. This leads to, for the first time, \$2-3\textbackslash times\$ speedup per training epoch compared with standard training. CoMERA also outperforms the recent GaLore in terms of both memory and computing efficiency. Specifically, CoMERA is \$2\textbackslash times\$ faster per training epoch and \$9\textbackslash times\$ more memory-efficient than GaLore on a tested six-encoder transformer with single-batch training. Our method also shows \$\textbackslash sim 2\textbackslash times\$ speedup than standard pre-training on a BERT-like code-generation LLM while achieving \$4.23\textbackslash times\$ compression ratio in pre-training. With further HPC optimization, CoMERA may reduce the pre-training cost of many other LLMs. An implementation of CoMERA is available at https://github.com/ziyangjoy/CoMERA.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/HBX6TQIZ/Yang et al. - 2024 - CoMERA Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization.pdf;/home/arans/Zotero/storage/YB8LGHYV/2405.html}
}

@online{yokotaVeryBasicsTensors2024,
  title = {Very {{Basics}} of {{Tensors}} with {{Graphical Notations}}: {{Unfolding}}, {{Calculations}}, and {{Decompositions}}},
  shorttitle = {Very {{Basics}} of {{Tensors}} with {{Graphical Notations}}},
  author = {Yokota, Tatsuya},
  date = {2024-11-25},
  eprint = {2411.16094},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.16094},
  url = {http://arxiv.org/abs/2411.16094},
  urldate = {2025-02-24},
  abstract = {Tensor network diagram (graphical notation) is a useful tool that graphically represents multiplications between multiple tensors using nodes and edges. Using the graphical notation, complex multiplications between tensors can be described simply and intuitively, and it also helps to understand the essence of tensor products. In fact, most of matrix/tensor products including inner product, outer product, Hadamard product, Kronecker product, and Khatri-Rao product can be written in graphical notation. These matrix/tensor operations are essential building blocks for the use of matrix/tensor decompositions in signal processing and machine learning. The purpose of this lecture note is to learn the very basics of tensors and how to represent them in mathematical symbols and graphical notation. Many papers using tensors omit these detailed definitions and explanations, which can be difficult for the reader. I hope this note will be of help to such readers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/9K26XGPK/Yokota - 2024 - Very Basics of Tensors with Graphical Notations Unfolding, Calculations, and Decompositions.pdf;/home/arans/Zotero/storage/CD88V9S2/2411.html}
}

@article{yuanRankMinimizationTensor2020,
  title = {Rank Minimization on Tensor Ring: An Efficient Approach for Tensor Decomposition and Completion},
  shorttitle = {Rank Minimization on Tensor Ring},
  author = {Yuan, Longhao and Li, Chao and Cao, Jianting and Zhao, Qibin},
  date = {2020-03-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {109},
  number = {3},
  pages = {603--622},
  issn = {1573-0565},
  doi = {10.1007/s10994-019-05846-7},
  url = {https://doi.org/10.1007/s10994-019-05846-7},
  urldate = {2025-02-24},
  abstract = {In recent studies, tensor ring decomposition (TRD) has become a promising model for tensor completion. However, TRD suffers from the rank selection problem due to the undetermined multilinear rank. For tensor decomposition with missing entries, the sub-optimal rank selection of traditional methods leads to the overfitting/underfitting problem. In this paper, we first explore the latent space of the TRD and theoretically prove the relationship between the TR-rank and the rank of the tensor unfoldings. Then, we propose two tensor completion models by imposing the different low-rank regularizations on the TR-factors, by which the TR-rank of the underlying tensor is minimized and the low-rank structures of the underlying tensor are exploited. By employing the alternating direction method of multipliers scheme, our algorithms obtain the TR factors and the underlying tensor simultaneously. In experiments of tensor completion tasks, our algorithms show robustness to rank selection and high computation efficiency, in comparison to traditional low-rank approximation algorithms.},
  langid = {english},
  keywords = {ADMM scheme,Artificial Intelligence,Structured nuclear norm,Tensor completion,Tensor ring decomposition},
  file = {/home/arans/Zotero/storage/R7SVXDP8/Yuan et al. - 2020 - Rank minimization on tensor ring an efficient approach for tensor decomposition and completion.pdf}
}

@online{yuPracticalSketchingBasedRandomized2022,
  title = {Practical {{Sketching-Based Randomized Tensor Ring Decomposition}}},
  author = {Yu, Yajie and Li, Hanyu},
  date = {2022-09-12},
  eprint = {2209.05647},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2209.05647},
  url = {http://arxiv.org/abs/2209.05647},
  urldate = {2025-02-25},
  abstract = {Based on sketching techniques, we propose two randomized algorithms for tensor ring (TR) decomposition. Specifically, by defining new tensor products and investigating their properties, we apply the Kronecker sub-sampled randomized Fourier transform and TensorSketch to the alternating least squares problems derived from the minimization problem of TR decomposition to devise the randomized algorithms. From the former, we find an algorithmic framework based on random projection for randomized TR decomposition. Theoretical results on sketch size and complexity analyses for the two algorithms are provided. We compare our proposals with the state-of-the-art method using both synthetic and real data. Numerical results show that they have quite decent performance in accuracy and computing time},
  pubstate = {prepublished},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/CQWTDZRS/Yu y Li - 2022 - Practical Sketching-Based Randomized Tensor Ring Decomposition.pdf;/home/arans/Zotero/storage/MI2VW9VF/2209.html}
}
