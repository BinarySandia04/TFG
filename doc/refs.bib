@article{mengApplicationLargeLanguage2024,
  title = {The Application of Large Language Models in Medicine: {{A}} Scoping Review},
  shorttitle = {The Application of Large Language Models in Medicine},
  author = {Meng, Xiangbin and Yan, Xiangyu and Zhang, Kuo and Liu, Da and Cui, Xiaojuan and Yang, Yaodong and Zhang, Muhan and Cao, Chunxia and Wang, Jingjia and Wang, Xuliang and Gao, Jun and Wang, Yuan-Geng-Shuo and Ji, Jia-ming and Qiu, Zifeng and Li, Muzi and Qian, Cheng and Guo, Tianze and Ma, Shuangquan and Wang, Zeying and Guo, Zexuan and Lei, Youlan and Shao, Chunli and Wang, Wenyao and Fan, Haojun and Tang, Yi-Da},
  date = {2024-05-17},
  journaltitle = {iScience},
  shortjournal = {iScience},
  volume = {27},
  number = {5},
  pages = {109713},
  issn = {2589-0042},
  doi = {10.1016/j.isci.2024.109713},
  url = {https://www.sciencedirect.com/science/article/pii/S2589004224009350},
  urldate = {2025-06-04},
  abstract = {This study systematically reviewed the application of large language models (LLMs) in medicine, analyzing 550 selected studies from a vast literature search. LLMs like ChatGPT transformed healthcare by enhancing diagnostics, medical writing, education, and project management. They assisted in drafting medical documents, creating training simulations, and streamlining research processes. Despite their growing utility in assisted diagnosis and improving doctor-patient communication, challenges persisted, including limitations in contextual understanding and the risk of over-reliance. The surge in LLM-related research indicated a focus on medical writing, diagnostics, and patient communication, but highlighted the need for careful integration, considering validation, ethical concerns, and the balance with traditional medical practice. Future research directions suggested a focus on multimodal LLMs, deeper algorithmic understanding, and ensuring responsible, effective use in healthcare.},
  keywords = {Artificial intelligence,Health informatics},
  file = {/home/arans/Zotero/storage/NKIS3HDQ/S2589004224009350.html}
}

@online{nieExploringRolesLarge2025,
  title = {Exploring the {{Roles}} of {{Large Language Models}} in {{Reshaping Transportation Systems}}: {{A Survey}}, {{Framework}}, and {{Roadmap}}},
  shorttitle = {Exploring the {{Roles}} of {{Large Language Models}} in {{Reshaping Transportation Systems}}},
  author = {Nie, Tong and Sun, Jian and Ma, Wei},
  date = {2025-03-27},
  eprint = {2503.21411},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2503.21411},
  url = {http://arxiv.org/abs/2503.21411},
  urldate = {2025-06-04},
  abstract = {Modern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/arans/Zotero/storage/DY26VV3U/Nie et al. - 2025 - Exploring the Roles of Large Language Models in Reshaping Transportation Systems A Survey, Framewor.pdf;/home/arans/Zotero/storage/5RB2Q2IM/2503.html}
}

@article{abd-alrazaqLargeLanguageModels2023,
  title = {Large {{Language Models}} in {{Medical Education}}: {{Opportunities}}, {{Challenges}}, and {{Future Directions}}},
  shorttitle = {Large {{Language Models}} in {{Medical Education}}},
  author = {Abd-alrazaq, Alaa and AlSaad, Rawan and Alhuwail, Dari and Ahmed, Arfan and Healy, Padraig Mark and Latifi, Syed and Aziz, Sarah and Damseh, Rafat and Alrazak, Sadam Alabed and Sheikh, Javaid},
  date = {2023-06-01},
  journaltitle = {JMIR Medical Education},
  volume = {9},
  number = {1},
  pages = {e48291},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/48291},
  url = {https://mededu.jmir.org/2023/1/e48291},
  urldate = {2025-06-04},
  abstract = {The integration of large language models (LLMs), such as those in the Generative Pre-trained Transformers (GPT) series, into medical education has the potential to transform learning experiences for students and elevate their knowledge, skills, and competence. Drawing on a wealth of professional and academic experience, we propose that LLMs hold promise for revolutionizing medical curriculum development, teaching methodologies, personalized study plans and learning materials, student assessments, and more. However, we also critically examine the challenges that such integration might pose by addressing issues of algorithmic bias, overreliance, plagiarism, misinformation, inequity, privacy, and copyright concerns in medical education. As we navigate the shift from an information-driven educational paradigm to an artificial intelligence (AI)–driven educational paradigm, we argue that it is paramount to understand both the potential and the pitfalls of LLMs in medical education. This paper thus offers our perspective on the opportunities and challenges of using LLMs in this context. We believe that the insights gleaned from this analysis will serve as a foundation for future recommendations and best practices in the field, fostering the responsible and effective use of AI technologies in medical education.},
  langid = {english},
  file = {/home/arans/Zotero/storage/IUZXBND3/Abd-alrazaq et al. - 2023 - Large Language Models in Medical Education Opportunities, Challenges, and Future Directions.pdf;/home/arans/Zotero/storage/KNRPSRMM/e48291.html}
}

@article{christandlTensorRankNot2018,
  title = {Tensor Rank Is Not Multiplicative under the Tensor Product},
  author = {Christandl, Matthias and Jensen, Asger Kjærulff and Zuiddam, Jeroen},
  date = {2018-04},
  journaltitle = {Linear Algebra and its Applications},
  shortjournal = {Linear Algebra and its Applications},
  volume = {543},
  eprint = {1705.09379},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {125--139},

  issn = {00243795},
  doi = {10.1016/j.laa.2017.12.020},
  url = {http://arxiv.org/abs/1705.09379},
  urldate = {2025-05-28},
  abstract = {The tensor rank of a tensor t is the smallest number r such that t can be decomposed as a sum of r simple tensors. Let s be a k-tensor and let t be an l-tensor. The tensor product of s and t is a (k + l)-tensor. Tensor rank is sub-multiplicative under the tensor product. We revisit the connection between restrictions and degenerations. A result of our study is that tensor rank is not in general multiplicative under the tensor product. This answers a question of Draisma and Saptharishi. Specifically, if a tensor t has border rank strictly smaller than its rank, then the tensor rank of t is not multiplicative under taking a sufficiently hight tensor product power. The "tensor Kronecker product" from algebraic complexity theory is related to our tensor product but different, namely it multiplies two k-tensors to get a k-tensor. Nonmultiplicativity of the tensor Kronecker product has been known since the work of Strassen. It remains an open question whether border rank and asymptotic rank are multiplicative under the tensor product. Interestingly, lower bounds on border rank obtained from generalised flattenings (including Young flattenings) multiply under the tensor product.},
  keywords = {Computer Science - Computational Complexity,Mathematics - Commutative Algebra,Quantum Physics},
  file = {/home/arans/Zotero/storage/NCTSG5WW/Christandl et al. - 2018 - Tensor rank is not multiplicative under the tensor product.pdf;/home/arans/Zotero/storage/TNEWSLUQ/1705.html}
}

@article{rohrig-zollnerPerformanceLowrankTensortrain2022,
  title = {Performance of the Low-Rank Tensor-Train {{SVD}} ({{TT-SVD}}) for Large Dense Tensors on Modern Multi-Core {{CPUs}}},
  author = {Röhrig-Zöllner, Melven and Thies, Jonas and Basermann, Achim},
  date = {2022-08},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  volume = {44},
  number = {4},
  eprint = {2102.00104},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {C287-C309},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/21M1395545},
  url = {http://arxiv.org/abs/2102.00104},
  urldate = {2025-05-28},
  abstract = {There are several factorizations of multi-dimensional tensors into lower-dimensional components, known as `tensor networks'. We consider the popular `tensor-train' (TT) format and ask: How efficiently can we compute a low-rank approximation from a full tensor on current multi-core CPUs? Compared to sparse and dense linear algebra, kernel libraries for multi-linear algebra are rare and typically not as well optimized. Linear algebra libraries like BLAS and LAPACK may provide the required operations in principle, but often at the cost of additional data movements for rearranging memory layouts. Furthermore, these libraries are typically optimized for the compute-bound case (e.g.\textbackslash{} square matrix operations) whereas low-rank tensor decompositions lead to memory bandwidth limited operations. We propose a `tensor-train singular value decomposition' (TT-SVD) algorithm based on two building blocks: a `Q-less tall-skinny QR' factorization, and a fused tall-skinny matrix-matrix multiplication and reshape operation. We analyze the performance of the resulting TT-SVD algorithm using the Roofline performance model. In addition, we present performance results for different algorithmic variants for shared-memory as well as distributed-memory architectures. Our experiments show that commonly used TT-SVD implementations suffer severe performance penalties. We conclude that a dedicated library for tensor factorization kernels would benefit the community: Computing a low-rank approximation can be as cheap as reading the data twice from main memory. As a consequence, an implementation that achieves realistic performance will move the limit at which one has to resort to randomized methods that only process part of the data.},
  keywords = {Computer Science - Mathematical Software,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/5H6JTNVK/Röhrig-Zöllner et al. - 2022 - Performance of the low-rank tensor-train SVD (TT-SVD) for large dense tensors on modern multi-core C.pdf;/home/arans/Zotero/storage/GMK4JIEC/2102.html}
}

@online{PyTorchDocumentationPyTorch,
  title = {{{PyTorch}} Documentation — {{PyTorch}} 2.7 Documentation},
  url = {https://docs.pytorch.org/docs/stable/index.html},
  urldate = {2025-06-04},
  file = {/home/arans/Zotero/storage/FRYSLU9E/index.html}
}

@online{liPermutationSearchTensor2022,
  title = {Permutation {{Search}} of {{Tensor Network Structures}} via {{Local Sampling}}},
  author = {Li, Chao and Zeng, Junhua and Tao, Zerui and Zhao, Qibin},
  date = {2022-06-14},
  eprint = {2206.06597},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.06597},
  url = {http://arxiv.org/abs/2206.06597},
  urldate = {2025-03-24},
  abstract = {Recent works put much effort into tensor network structure search (TN-SS), aiming to select suitable tensor network (TN) structures, involving the TN-ranks, formats, and so on, for the decomposition or learning tasks. In this paper, we consider a practical variant of TN-SS, dubbed TN permutation search (TN-PS), in which we search for good mappings from tensor modes onto TN vertices (core tensors) for compact TN representations. We conduct a theoretical investigation of TN-PS and propose a practically-efficient algorithm to resolve the problem. Theoretically, we prove the counting and metric properties of search spaces of TN-PS, analyzing for the first time the impact of TN structures on these unique properties. Numerically, we propose a novel meta-heuristic algorithm, in which the searching is done by randomly sampling in a neighborhood established in our theory, and then recurrently updating the neighborhood until convergence. Numerical results demonstrate that the new algorithm can reduce the required model size of TNs in extensive benchmarks, implying the improvement in the expressive power of TNs. Furthermore, the computational cost for the new algorithm is significantly less than that in\textasciitilde\textbackslash cite\{li2020evolutionary\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/DN5NA8XZ/Li et al. - 2022 - Permutation Search of Tensor Network Structures via Local Sampling.pdf;/home/arans/Zotero/storage/9STGY9YD/2206.html}
}

@article{penroseGeneralizedInverseMatrices1955,
  title = {A Generalized Inverse for Matrices},
  author = {Penrose, R.},
  date = {1955-07},
  journaltitle = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {51},
  number = {3},
  pages = {406--413},
  issn = {1469-8064, 0305-0041},
  doi = {10.1017/S0305004100030401},
  url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/generalized-inverse-for-matrices/5F4516D6B9989BB6563A4B267CC7D615},
  urldate = {2025-06-03},
  abstract = {This paper describes a generalization of the inverse of a non-singular matrix, as the unique solution of a certain set of equations. This generalized inverse exists for any (possibly rectangular) matrix whatsoever with complex elements. It is used here for solving linear matrix equations, and among other applications for finding an expression for the principal idempotent elements of a matrix. Also a new type of spectral decomposition is given.},
  langid = {english},
  file = {/home/arans/Zotero/storage/9IUNTW8M/Penrose - 1955 - A generalized inverse for matrices.pdf}
}

@inproceedings{golovinGradientlessDescentHighDimensional2019,
  title = {Gradientless {{Descent}}: {{High-Dimensional Zeroth-Order Optimization}}},
  shorttitle = {Gradientless {{Descent}}},
  author = {Golovin, Daniel and Karro, John and Kochanski, Greg and Lee, Chansoo and Song, Xingyou and Zhang, Qiuyi},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=Skep6TVYDB},
  urldate = {2025-06-03},
  abstract = {Zeroth-order optimization is the process of minimizing an objective \$f(x)\$, given oracle access to evaluations at adaptively chosen inputs \$x\$. In this paper, we present two simple yet powerful GradientLess Descent (GLD) algorithms that do not rely on an underlying gradient estimate and are numerically stable. We analyze our algorithm from a novel geometric perspective and we show that for \{\textbackslash it any monotone transform\} of a smooth and strongly convex objective with latent dimension \$k \textbackslash ge n\$, we present a novel analysis that shows convergence within an \$\textbackslash epsilon\$-ball of the optimum in \$O(kQ\textbackslash log(n)\textbackslash log(R/\textbackslash epsilon))\$ evaluations, where the input dimension is \$n\$, \$R\$ is the diameter of the input space and \$Q\$ is the condition number. Our rates are the first of its kind to be both 1) poly-logarithmically dependent on dimensionality and 2) invariant under monotone transformations. We further leverage our geometric perspective to show that our analysis is optimal. Both monotone invariance and its ability to utilize a low latent dimensionality are key to the empirical success of our algorithms, as demonstrated on synthetic and MuJoCo benchmarks.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/arans/Zotero/storage/MBBNIV7E/Golovin et al. - 2019 - Gradientless Descent High-Dimensional Zeroth-Order Optimization.pdf}
}


@online{biamonteTensorNetworksNutshell2017,
  title = {Tensor {{Networks}} in a {{Nutshell}}},
  author = {Biamonte, Jacob and Bergholm, Ville},
  date = {2017-07-31},
  eprint = {1708.00006},
  eprinttype = {arXiv},
  eprintclass = {quant-ph},
  doi = {10.48550/arXiv.1708.00006},
  url = {http://arxiv.org/abs/1708.00006},
  urldate = {2025-03-26},
  abstract = {Tensor network methods are taking a central role in modern quantum physics and beyond. They can provide an efficient approximation to certain classes of quantum states, and the associated graphical language makes it easy to describe and pictorially reason about quantum circuits, channels, protocols, open systems and more. Our goal is to explain tensor networks and some associated methods as quickly and as painlessly as possible. Beginning with the key definitions, the graphical tensor network language is presented through examples. We then provide an introduction to matrix product states. We conclude the tutorial with tensor contractions evaluating combinatorial counting problems. The first one counts the number of solutions for Boolean formulae, whereas the second is Penrose's tensor contraction algorithm, returning the number of \$3\$-edge-colorings of \$3\$-regular planar graphs.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,General Relativity and Quantum Cosmology,High Energy Physics - Theory,Mathematical Physics,Mathematics - Mathematical Physics,Quantum Physics},
  file = {/home/arans/Zotero/storage/NNFCNU6Z/Biamonte y Bergholm - 2017 - Tensor Networks in a Nutshell.pdf;/home/arans/Zotero/storage/ZMMWGFWD/1708.html}
}

@article{carliniRanksDerivedMultilinear2011,
  title = {Ranks Derived from Multilinear Maps},
  author = {Carlini, Enrico and Kleppe, Johannes},
  date = {2011-08-01},
  journaltitle = {Journal of Pure and Applied Algebra},
  shortjournal = {Journal of Pure and Applied Algebra},
  volume = {215},
  number = {8},
  pages = {1999--2004},
  issn = {0022-4049},
  doi = {10.1016/j.jpaa.2010.11.010},
  url = {https://www.sciencedirect.com/science/article/pii/S0022404910002616},
  urldate = {2025-03-31},
  abstract = {Let V1, V2 and V3 be vector spaces over any field k. An element T∈V1⊗V2⊗V3 induces for each i=1,2,3 a k-linear map Ti:Vi∗→⊗j≠iVj where Vi∗ is the dual vector space of Vi. We characterize all integer triplets (r1,r2,r3) such that there exists a tensor T with ri=rankTi, and we explain how these ranks are related to the higher secant varieties of various Segre varieties. We also study the case T∈V1⊗⋯⊗Vn with n{$>$}3, giving necessary conditions on the ranks of all induced linear maps.},
  file = {/home/arans/Zotero/storage/NDPR4IY2/S0022404910002616.html}
}

@article{chenAutomaticDifferentiationSecond2020,
  title = {Automatic {{Differentiation}} for {{Second Renormalization}} of {{Tensor Networks}}},
  author = {Chen, Bin-Bin and Gao, Yuan and Guo, Yi-Bin and Liu, Yuzhi and Zhao, Hui-Hai and Liao, Hai-Jun and Wang, Lei and Xiang, Tao and Li, Wei and Xie, Z. Y.},
  date = {2020-06-23},
  journaltitle = {Physical Review B},
  shortjournal = {Phys. Rev. B},
  volume = {101},
  number = {22},
  eprint = {1912.02780},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  pages = {220409},
  issn = {2469-9950, 2469-9969},
  doi = {10.1103/PhysRevB.101.220409},
  url = {http://arxiv.org/abs/1912.02780},
  urldate = {2025-02-25},
  abstract = {Tensor renormalization group (TRG) constitutes an important methodology for accurate simulations of strongly correlated lattice models. Facilitated by the automatic differentiation technique widely used in deep learning, we propose a uniform framework of differentiable TRG (\$\textbackslash partial\$TRG) that can be applied to improve various TRG methods, in an automatic fashion. Essentially, \$\textbackslash partial\$TRG systematically extends the concept of second renormalization [PRL 103, 160601 (2009)] where the tensor environment is computed recursively in the backward iteration, in the sense that given the forward process of TRG, \$\textbackslash partial\$TRG automatically finds the gradient through backpropagation, with which one can deeply "train" the tensor networks. We benchmark \$\textbackslash partial\$TRG in solving the square-lattice Ising model, and demonstrate its power by simulating one- and two-dimensional quantum systems at finite temperature. The deep optimization as well as GPU acceleration renders \$\textbackslash partial\$TRG manybody simulations with high efficiency and accuracy.},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Physics - Computational Physics},
  file = {/home/arans/Zotero/storage/M3Y5D2CY/Chen et al. - 2020 - Automatic Differentiation for Second Renormalization of Tensor Networks.pdf;/home/arans/Zotero/storage/JXYXR4DJ/1912.html}
}

@article{chenTENSORRINGNETS2018,
  title = {{{TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING}}},
  author = {Chen, Xinqi and Hou, Ming and Zhou, Guoxu and Zhao, Qibin},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=BJxmXhRcK7},
  urldate = {2025-02-25},
  abstract = {Recent deep multi-task learning (MTL) has been witnessed its success in alleviating data scarcity of some task by utilizing domain-specific knowledge from related tasks. Nonetheless, several major issues of deep MTL, including the effectiveness of sharing mechanisms, the efficiency of model complexity and the flexibility of network architectures, still remain largely unaddressed. To this end, we propose a novel generalized latent-subspace based knowledge sharing mechanism for linking task-specific models, namely tensor ring multi-task learning (TRMTL). TRMTL has a highly compact representation, and it is very effective in transferring task-invariant knowledge while being super flexible in learning task-specific features, successfully mitigating the dilemma of both negative-transfer in lower layers and under-transfer in higher layers. Under our TRMTL, it is feasible for each task to have heterogenous input data dimensionality or distinct feature sizes at different hidden layers. Experiments on a variety of datasets demonstrate our model is capable of significantly improving each single task’s performance, particularly favourable in scenarios where some of the tasks have insufficient data.},
  langid = {english},
  file = {/home/arans/Zotero/storage/IYXBYTE5/Chen et al. - 2018 - TENSOR RING NETS ADAPTED DEEP MULTI-TASK LEARNING.pdf}
}

@incollection{dodsonTensorsMultilinearForms1991,
  title = {Tensors and {{Multilinear Forms}}},
  booktitle = {Tensor {{Geometry}}: {{The Geometric Viewpoint}} and Its {{Uses}}},
  author = {Dodson, Christopher Terence John and Poston, Timothy},
  editor = {Dodson, Christopher Terence John and Poston, Timothy},
  date = {1991},
  pages = {98--113},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-10514-2_6},
  url = {https://doi.org/10.1007/978-3-642-10514-2_6},
  urldate = {2025-03-28},
  abstract = {Starting with a vector space X, we already have several spaces derived from it, such as the dual space X* and the spaces L2(X;R) and L2(X*;R) of bilinear forms on X and X*. We shall now produce some more. Fortunately, rather than adding more spaces to an ad hoc list, all as different as, say X* and L2(X; R) seem from each other, our new construction gives a general framework in which all the spaces so far considered occur as special cases. (This gathering of apparently very different things into one grand structure where they appear as examples is common in mathematics — both because it is often a very powerful tool and because many mathematicians have great difficulty in remembering facts they can’t deduce from a framework, like the atomic weight of copper or the date of the battle of Pondicherry. This deficiency is often what pushed them to the subject and away from chemistry or history in the first place, at school.)},
  isbn = {978-3-642-10514-2},
  langid = {english},
  keywords = {Bilinear Form,Linear Functional,Summation Convention,Tensor Product,Vector Space},
  file = {/home/arans/Zotero/storage/7GPBIJGE/Dodson y Poston - 1991 - Tensors and Multilinear Forms.pdf}
}

@online{evenblyPracticalGuideNumerical2022,
  title = {A {{Practical Guide}} to the {{Numerical Implementation}} of {{Tensor Networks I}}: {{Contractions}}, {{Decompositions}} and {{Gauge Freedom}}},
  shorttitle = {A {{Practical Guide}} to the {{Numerical Implementation}} of {{Tensor Networks I}}},
  author = {Evenbly, Glen},
  date = {2022-02-04},
  eprint = {2202.02138},
  eprinttype = {arXiv},
  eprintclass = {quant-ph},
  doi = {10.48550/arXiv.2202.02138},
  url = {http://arxiv.org/abs/2202.02138},
  urldate = {2025-02-25},
  abstract = {We present an overview of the key ideas and skills necessary to begin implementing tensor network methods numerically, which is intended to facilitate the practical application of tensor network methods for researchers that are already versed with their theoretical foundations. These skills include an introduction to the contraction of tensor networks, to optimal tensor decompositions, and to the manipulation of gauge degrees of freedom in tensor networks. The topics presented are of key importance to many common tensor network algorithms such as DMRG, TEBD, TRG, PEPS and MERA.},
  pubstate = {prepublished},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Quantum Physics},
  file = {/home/arans/Zotero/storage/J3MS9XD2/Evenbly - 2022 - A Practical Guide to the Numerical Implementation of Tensor Networks I Contractions, Decompositions.pdf;/home/arans/Zotero/storage/F6BELNLL/2202.html}
}

@online{gaborReducedStorageDirect2024,
  title = {Reduced Storage Direct Tensor Ring Decomposition for Convolutional Neural Networks Compression},
  author = {Gabor, Mateusz and Zdunek, Rafał},
  date = {2024-08-05},
  eprint = {2405.10802},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.10802},
  url = {http://arxiv.org/abs/2405.10802},
  urldate = {2025-02-24},
  abstract = {Convolutional neural networks (CNNs) are among the most widely used machine learning models for computer vision tasks, such as image classification. To improve the efficiency of CNNs, many CNNs compressing approaches have been developed. Low-rank methods approximate the original convolutional kernel with a sequence of smaller convolutional kernels, which leads to reduced storage and time complexities. In this study, we propose a novel low-rank CNNs compression method that is based on reduced storage direct tensor ring decomposition (RSDTR). The proposed method offers a higher circular mode permutation flexibility, and it is characterized by large parameter and FLOPS compression rates, while preserving a good classification accuracy of the compressed network. The experiments, performed on the CIFAR-10 and ImageNet datasets, clearly demonstrate the efficiency of RSDTR in comparison to other state-of-the-art CNNs compression approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/Y6MFC3FS/Gabor y Zdunek - 2024 - Reduced storage direct tensor ring decomposition for convolutional neural networks compression.pdf;/home/arans/Zotero/storage/TDKT76RA/2405.html}
}

@online{guoTensorNetworkStructure2025,
  title = {Tensor {{Network Structure Search Using Program Synthesis}}},
  author = {Guo, Zheng and Deshpande, Aditya and Kiedrowski, Brian and Wang, Xinyu and Gorodetsky, Alex},
  date = {2025-02-22},
  eprint = {2502.02711},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.02711},
  url = {http://arxiv.org/abs/2502.02711},
  urldate = {2025-05-12},
  abstract = {Tensor networks provide a powerful framework for compressing multi-dimensional data. The optimal tensor network structure for a given data tensor depends on both the inherent data properties and the specific optimality criteria, making tensor network structure search a crucial research problem. Existing solutions typically involve sampling and validating numerous candidate structures; this is computationally expensive, limiting their practical applications. We address this challenge by formulating tensor network structure search as a program synthesis problem and proposing a highly efficient validation method that is based on constraint solving. Specifically, we design a domain specific language: it builds the correspondence between programs and network structures, and uses a novel idea of output-directed splits to compress the search space without hindering the expressiveness. We then propose a synthesis algorithm that can prioritize promising candidates through constraint solving. \% Experimental results show that our approach improves search speed by \$10\textbackslash times\$ and achieves compression ratios by \$1.5\textbackslash times\$ to \$3\textbackslash times\$ better than state-of-the-art. Notably, our approach scales to larger tensors that are out of reach by prior work. Finally, we demonstrate that the discovered topologies generalize to data from the same source, achieving compression ratios up to \$ 2.4\textbackslash times\$ better than hierarchical Tuckers while maintaining the runtime around \$110\$ seconds.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Programming Languages},
  file = {/home/arans/Zotero/storage/M5NFVT8X/Guo et al. - 2025 - Tensor Network Structure Search Using Program Synthesis.pdf;/home/arans/Zotero/storage/BQWGQLCR/2502.html}
}

@inproceedings{hashemizadehAdaptiveLearningTensor2020,
  title = {Adaptive {{Learning}} of {{Tensor Network Structures}}},
  author = {Hashemizadeh, Meraj and Liu, M. and Miller, Jacob and Rabusseau, Guillaume},
  date = {2020-08-12},
  url = {https://www.semanticscholar.org/paper/88d63592664fe2962a9c2e20c56fe499d4809bfa},
  urldate = {2025-05-17},
  abstract = {Tensor Networks (TN) offer a powerful framework to efficiently represent very high-dimensional objects. TN have recently shown their potential for machine learning applications and offer a unifying view of common tensor decomposition models such as Tucker, tensor train (TT) and tensor ring (TR). However, identifying the best tensor network structure from data for a given task is challenging. In this work, we leverage the TN formalism to develop a generic and efficient adaptive algorithm to jointly learn the structure and the parameters of a TN from data. Our method is based on a simple greedy approach starting from a rank one tensor and successively identifying the most promising tensor network edges for small rank increments. Our algorithm can adaptively identify TN structures with small number of parameters that effectively optimize any differentiable objective function. Experiments on tensor decomposition, tensor completion and model compression tasks demonstrate the effectiveness of the proposed algorithm. In particular, our method outperforms the state-of-the-art evolutionary topology search [Li and Sun, 2020] for tensor decomposition of images (while being orders of magnitude faster) and finds efficient tensor network structures to compress neural networks outperforming popular TT based approaches [Novikov et al., 2015].},
  file = {/home/arans/Zotero/storage/N9HCKLEK/Hashemizadeh et al. - 2020 - Adaptive Learning of Tensor Network Structures.pdf}
}

@online{heScalableRobustTensor2023,
  title = {Scalable and {{Robust Tensor Ring Decomposition}} for {{Large-scale Data}}},
  author = {He, Yicong and Atia, George K.},
  date = {2023-05-15},
  eprint = {2305.09044},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.09044},
  url = {http://arxiv.org/abs/2305.09044},
  urldate = {2025-02-24},
  abstract = {Tensor ring (TR) decomposition has recently received increased attention due to its superior expressive performance for high-order tensors. However, the applicability of traditional TR decomposition algorithms to real-world applications is hindered by prevalent large data sizes, missing entries, and corruption with outliers. In this work, we propose a scalable and robust TR decomposition algorithm capable of handling large-scale tensor data with missing entries and gross corruptions. We first develop a novel auto-weighted steepest descent method that can adaptively fill the missing entries and identify the outliers during the decomposition process. Further, taking advantage of the tensor ring model, we develop a novel fast Gram matrix computation (FGMC) approach and a randomized subtensor sketching (RStS) strategy which yield significant reduction in storage and computational complexity. Experimental results demonstrate that the proposed method outperforms existing TR decomposition methods in the presence of outliers, and runs significantly faster than existing robust tensor completion algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/QLD2LQLH/He y Atia - 2023 - Scalable and Robust Tensor Ring Decomposition for Large-scale Data.pdf;/home/arans/Zotero/storage/Z4NM82JZ/2305.html}
}

@online{hillarMostTensorProblems2013,
  title = {Most Tensor Problems Are {{NP-hard}}},
  author = {Hillar, Christopher and Lim, Lek-Heng},
  date = {2013-07-01},
  eprint = {0911.1393},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.0911.1393},
  url = {http://arxiv.org/abs/0911.1393},
  urldate = {2025-03-31},
  abstract = {We prove that multilinear (tensor) analogues of many efficiently computable problems in numerical linear algebra are NP-hard. Our list here includes: determining the feasibility of a system of bilinear equations, deciding whether a 3-tensor possesses a given eigenvalue, singular value, or spectral norm; approximating an eigenvalue, eigenvector, singular vector, or the spectral norm; and determining the rank or best rank-1 approximation of a 3-tensor. Furthermore, we show that restricting these problems to symmetric tensors does not alleviate their NP-hardness. We also explain how deciding nonnegative definiteness of a symmetric 4-tensor is NP-hard and how computing the combinatorial hyperdeterminant of a 4-tensor is NP-, \#P-, and VNP-hard. We shall argue that our results provide another view of the boundary separating the computational tractability of linear/convex problems from the intractability of nonlinear/nonconvex ones.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/7Q8M2TRM/Hillar y Lim - 2013 - Most tensor problems are NP-hard.pdf;/home/arans/Zotero/storage/MXJ5FRSY/0911.html}
}

@online{landsbergGeometryTensorNetwork2012,
  title = {On the Geometry of Tensor Network States},
  author = {Landsberg, J. M. and Qi, Yang and Ye, Ke},
  date = {2012-01-09},
  eprint = {1105.4449},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1105.4449},
  url = {http://arxiv.org/abs/1105.4449},
  urldate = {2025-03-25},
  abstract = {We answer a question of L. Grasedyck that arose in quantum information theory, showing that the limit of tensors in a space of tensor network states need not be a tensor network state. We also give geometric descriptions of spaces of tensor networks states corresponding to trees and loops. Grasedyck's question has a surprising connection to the area of Geometric Complexity Theory, in that the result is equivalent to the statement that the boundary of the Mulmuley-Sohoni type variety associated to matrix multiplication is strictly larger than the projections and re-labelings of matrix multiplication. Tensor Network States are also related to graphical models in algebraic statistics.},
  pubstate = {prepublished},
  keywords = {Mathematics - Algebraic Geometry},
  file = {/home/arans/Zotero/storage/HW8P9XX4/Landsberg et al. - 2012 - On the geometry of tensor network states.pdf;/home/arans/Zotero/storage/NDPKSBSL/1105.html}
}

@online{liAlternatingLocalEnumeration2023,
  title = {Alternating {{Local Enumeration}} ({{TnALE}}): {{Solving Tensor Network Structure Search}} with {{Fewer Evaluations}}},
  shorttitle = {Alternating {{Local Enumeration}} ({{TnALE}})},
  author = {Li, Chao and Zeng, Junhua and Li, Chunmei and Caiafa, Cesar and Zhao, Qibin},
  date = {2023-05-29},
  eprint = {2304.12875},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.12875},
  url = {http://arxiv.org/abs/2304.12875},
  urldate = {2025-03-24},
  abstract = {Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS\textasciitilde\textbackslash cite\{li2022permutation\} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \textbackslash emph\{greatly\} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \textbackslash emph\{reached\} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that \$\textbackslash Omega(2\textasciicircum N)\$ evaluations are typically required in TNLS for \textbackslash emph\{reaching\} the objective reduction in the neighborhood, while ideally \$O(N\textasciicircum 2R)\$ evaluations are sufficient in TnALE, where \$N\$ denotes the tensor order and \$R\$ reflects the \textbackslash emph\{``low-rankness''\} of the neighborhood. Experimental results verify that TnALE can find practically good TN-ranks and permutations with vastly fewer evaluations than the state-of-the-art algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/VY77TAB6/Li et al. - 2023 - Alternating Local Enumeration (TnALE) Solving Tensor Network Structure Search with Fewer Evaluation.pdf;/home/arans/Zotero/storage/FHR5VXAJ/2304.html}
}

@online{liAlternatingLocalEnumeration2023a,
  title = {Alternating {{Local Enumeration}} ({{TnALE}}): {{Solving Tensor Network Structure Search}} with {{Fewer Evaluations}}},
  shorttitle = {Alternating {{Local Enumeration}} ({{TnALE}})},
  author = {Li, Chao and Zeng, Junhua and Li, Chunmei and Caiafa, Cesar and Zhao, Qibin},
  date = {2023-05-29},
  eprint = {2304.12875},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.12875},
  url = {http://arxiv.org/abs/2304.12875},
  urldate = {2025-05-15},
  abstract = {Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS\textasciitilde\textbackslash cite\{li2022permutation\} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \textbackslash emph\{greatly\} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \textbackslash emph\{reached\} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that \$\textbackslash Omega(2\textasciicircum N)\$ evaluations are typically required in TNLS for \textbackslash emph\{reaching\} the objective reduction in the neighborhood, while ideally \$O(N\textasciicircum 2R)\$ evaluations are sufficient in TnALE, where \$N\$ denotes the tensor order and \$R\$ reflects the \textbackslash emph\{``low-rankness''\} of the neighborhood. Experimental results verify that TnALE can find practically good TN-ranks and permutations with vastly fewer evaluations than the state-of-the-art algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/J4TKYP77/Li et al. - 2023 - Alternating Local Enumeration (TnALE) Solving Tensor Network Structure Search with Fewer Evaluation.pdf;/home/arans/Zotero/storage/2BPBZC5Z/2304.html}
}

@article{liaoDifferentiableProgrammingTensor2019,
  title = {Differentiable {{Programming Tensor Networks}}},
  author = {Liao, Hai-Jun and Liu, Jin-Guo and Wang, Lei and Xiang, Tao},
  date = {2019-09-05},
  journaltitle = {Physical Review X},
  shortjournal = {Phys. Rev. X},
  volume = {9},
  number = {3},
  eprint = {1903.09650},
  eprinttype = {arXiv},
  eprintclass = {cond-mat},
  pages = {031041},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.9.031041},
  url = {http://arxiv.org/abs/1903.09650},
  urldate = {2025-02-24},
  abstract = {Differentiable programming is a fresh programming paradigm which composes parameterized algorithmic components and trains them using automatic differentiation (AD). The concept emerges from deep learning but is not only limited to training neural networks. We present theory and practice of programming tensor network algorithms in a fully differentiable way. By formulating the tensor network algorithm as a computation graph, one can compute higher order derivatives of the program accurately and efficiently using AD. We present essential techniques to differentiate through the tensor networks contractions, including stable AD for tensor decomposition and efficient backpropagation through fixed point iterations. As a demonstration, we compute the specific heat of the Ising model directly by taking the second order derivative of the free energy obtained in the tensor renormalization group calculation. Next, we perform gradient based variational optimization of infinite projected entangled pair states for quantum antiferromagnetic Heisenberg model and obtain start-of-the-art variational energy and magnetization with moderate efforts. Differentiable programming removes laborious human efforts in deriving and implementing analytical gradients for tensor network programs, which opens the door to more innovations in tensor network algorithms and applications.},
  keywords = {Condensed Matter - Strongly Correlated Electrons,Quantum Physics},
  file = {/home/arans/Zotero/storage/T8TASW9M/Liao et al. - 2019 - Differentiable Programming Tensor Networks.pdf;/home/arans/Zotero/storage/8T8U272U/1903.html}
}

@inproceedings{liEvolutionaryTopologySearch2020,
  title = {Evolutionary {{Topology Search}} for {{Tensor Network Decomposition}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Li, Chao and Sun, Zhun},
  date = {2020-11-21},
  pages = {5947--5957},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/li20l.html},
  urldate = {2025-03-24},
  abstract = {Tensor network (TN) decomposition is a promising framework to represent extremely high-dimensional problems with few parameters. However, it is challenging to search the (near-)optimal topological structures for TN decomposition, since the number of candidate solutions exponentially grows with increasing the order of a tensor. In this paper, we claim that the issue can be practically tackled by evolutionary algorithms in an affordable manner. We encode the complex topological structures into binary strings, and develop a simple genetic meta-algorithm to search the optimal topology on Hamming space. The experimental results by both synthetic and real-world data demonstrate that our method can effectively discover the ground-truth topology or even better structures with a small number of generations, and significantly boost the representational power of TN decomposition compared with well-known tensor-train (TT) or tensor-ring (TR) models.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/arans/Zotero/storage/TS4RXZBT/Li y Sun - 2020 - Evolutionary Topology Search for Tensor Network Decomposition.pdf}
}

@inproceedings{liEvolutionaryTopologySearch2020a,
  title = {Evolutionary {{Topology Search}} for {{Tensor Network Decomposition}}: An {{Extended Abstract}}},
  shorttitle = {Evolutionary {{Topology Search}} for {{Tensor Network Decomposition}}},
  author = {Li, Chao and Sun, Zhun and Zeng, Junhua and Zhao, Qibin},
  date = {2020},
  url = {https://www.semanticscholar.org/paper/Evolutionary-Topology-Search-for-Tensor-Network-an-Li-Sun/8878e7d70b76b99d939fd68079b5e894cba370b6},
  urldate = {2025-03-24},
  abstract = {Tensor network (TN) decomposition is a promising framework to represent extremely high-dimensional problems with few parameters. However, it remains challenging to search the (near-)optimal topological structures for TN decomposition, since the number of candidate solutions exponentially grows with increasing the order of a tensor. In this work, we claim that the issue can be practically tackled by genetic algorithms (GAs) in an affordable manner, and the key is to encode the complex topological structures into fixed-length binary strings, a.k.a. , chromosomes in the context of GA. The experimental results on natural images demonstrate that, in the decomposition task, GA is able to discover more efficient topologys than the well-known TN models within a small number of generations.},
  file = {/home/arans/Zotero/storage/UDVKKIQ7/Li et al. - 2020 - Evolutionary Topology Search for Tensor Network Decomposition an Extended Abstract.pdf}
}

@online{liHeuristicRankSelection2021,
  title = {Heuristic {{Rank Selection}} with {{Progressively Searching Tensor Ring Network}}},
  author = {Li, Nannan and Pan, Yu and Chen, Yaran and Ding, Zixiang and Zhao, Dongbin and Xu, Zenglin},
  date = {2021-05-30},
  eprint = {2009.10580},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.10580},
  url = {http://arxiv.org/abs/2009.10580},
  urldate = {2025-05-20},
  abstract = {Recently, Tensor Ring Networks (TRNs) have been applied in deep networks, achieving remarkable successes in compression ratio and accuracy. Although highly related to the performance of TRNs, rank selection is seldom studied in previous works and usually set to equal in experiments. Meanwhile, there is not any heuristic method to choose the rank, and an enumerating way to find appropriate rank is extremely time-consuming. Interestingly, we discover that part of the rank elements is sensitive and usually aggregate in a narrow region, namely an interest region. Therefore, based on the above phenomenon, we propose a novel progressive genetic algorithm named Progressively Searching Tensor Ring Network Search (PSTRN), which has the ability to find optimal rank precisely and efficiently. Through the evolutionary phase and progressive phase, PSTRN can converge to the interest region quickly and harvest good performance. Experimental results show that PSTRN can significantly reduce the complexity of seeking rank, compared with the enumerating method. Furthermore, our method is validated on public benchmarks like MNIST, CIFAR10/100, UCF11 and HMDB51, achieving the state-of-the-art performance.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/arans/Zotero/storage/R4F26AV6/Li et al. - 2021 - Heuristic Rank Selection with Progressively Searching Tensor Ring Network.pdf;/home/arans/Zotero/storage/E9WPG8PI/2009.html}
}

@article{liHybridFullyConnected2023,
  title = {Hybrid {{Fully Connected Tensorized Compression Network}} for {{Hyperspectral Image Classification}}},
  author = {Li, Heng-Chao and Lin, Zhi-Xin and Ma, Tian-Yu and Zhao, Xi-Le and Plaza, Antonio and Emery, William J.},
  date = {2023},
  journaltitle = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {61},
  pages = {1--16},
  issn = {1558-0644},
  doi = {10.1109/TGRS.2023.3241193},
  url = {https://ieeexplore.ieee.org/document/10032776},
  urldate = {2025-03-04},
  abstract = {Deep learning models, such as convolutional neural networks (CNNs), have made significant progress in hyperspectral image (HSI) classification. However, these models require a large number of parameters, which occupy a lot of storage space and suffer from overfitting, thus resulting in performance loss. To solve the above problems, in this article, we propose a new compression network [namely, a Hybrid Fully Connected Tensorized Compression Network (HybridFCTCN)] by considering the high dimensionality of HSI data. First, using the low-rank fully connected tensor network decomposition (FCTND), three novel units, i.e., FCTN-FC, FCTNConv2D, and FCTNConv3D, are designed to compress the weight tensor of standard fully connected (FC) layer and kernel tensor of convolutional layer, reducing their parameters. In the novel units, the intrinsic correlation of the decomposed factors is adequately exploited by the FC structures, which enhances their feature extraction and classification abilities. Then, benefiting from the hybrid network backbone composed of the FCTNConv3D and FCTNConv2D units, HybridFCTCN can extract more discriminative features with fewer parameters, while it has great generalization capability and robustness, enabling better HSI classification. Finally, the rank of above-designed units is defined, and its determination is discussed to facilitate the application of the proposed model. Extensive experiments on three widely used HSI datasets reveal that the proposed model achieves state-of-the-art classification performance for different training sample sizes with a very small number of parameters.},
  eventtitle = {{{IEEE Transactions}} on {{Geoscience}} and {{Remote Sensing}}},
  keywords = {Convolution,Convolutional neural networks (CNNs),Correlation,Feature extraction,fully connected tensor network decomposition (FCTND),hyperspectral image (HSI) classification,Hyperspectral imaging,Kernel,network compression,Standards,Tensors},
  file = {/home/arans/Zotero/storage/AASX2SE7/Li et al. - 2023 - Hybrid Fully Connected Tensorized Compression Network for Hyperspectral Image Classification.pdf;/home/arans/Zotero/storage/SLGWWKUE/10032776.html}
}

@online{malikSamplingBasedDecompositionAlgorithms2022,
  title = {Sampling-{{Based Decomposition Algorithms}} for {{Arbitrary Tensor Networks}}},
  author = {Malik, Osman Asif and Bharadwaj, Vivek and Murray, Riley},
  date = {2022-10-07},
  eprint = {2210.03828},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2210.03828},
  url = {http://arxiv.org/abs/2210.03828},
  urldate = {2025-04-30},
  abstract = {We show how to develop sampling-based alternating least squares (ALS) algorithms for decomposition of tensors into any tensor network (TN) format. Provided the TN format satisfies certain mild assumptions, resulting algorithms will have input sublinear per-iteration cost. Unlike most previous works on sampling-based ALS methods for tensor decomposition, the sampling in our framework is done according to the exact leverage score distribution of the design matrices in the ALS subproblems. We implement and test two tensor decomposition algorithms that use our sampling framework in a feature extraction experiment where we compare them against a number of other decomposition algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/PVC47HIP/Malik et al. - 2022 - Sampling-Based Decomposition Algorithms for Arbitrary Tensor Networks.pdf;/home/arans/Zotero/storage/Q2NR56XM/2210.html}
}

@online{guoTensorNetworkStructure2025,
  title = {Tensor {{Network Structure Search Using Program Synthesis}}},
  author = {Guo, Zheng and Deshpande, Aditya and Kiedrowski, Brian and Wang, Xinyu and Gorodetsky, Alex},
  date = {2025-02-22},
  eprint = {2502.02711},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2502.02711},
  url = {http://arxiv.org/abs/2502.02711},
  urldate = {2025-05-12},
  abstract = {Tensor networks provide a powerful framework for compressing multi-dimensional data. The optimal tensor network structure for a given data tensor depends on both the inherent data properties and the specific optimality criteria, making tensor network structure search a crucial research problem. Existing solutions typically involve sampling and validating numerous candidate structures; this is computationally expensive, limiting their practical applications. We address this challenge by formulating tensor network structure search as a program synthesis problem and proposing a highly efficient validation method that is based on constraint solving. Specifically, we design a domain specific language: it builds the correspondence between programs and network structures, and uses a novel idea of output-directed splits to compress the search space without hindering the expressiveness. We then propose a synthesis algorithm that can prioritize promising candidates through constraint solving. \% Experimental results show that our approach improves search speed by \$10\textbackslash times\$ and achieves compression ratios by \$1.5\textbackslash times\$ to \$3\textbackslash times\$ better than state-of-the-art. Notably, our approach scales to larger tensors that are out of reach by prior work. Finally, we demonstrate that the discovered topologies generalize to data from the same source, achieving compression ratios up to \$ 2.4\textbackslash times\$ better than hierarchical Tuckers while maintaining the runtime around \$110\$ seconds.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Programming Languages},
  file = {/home/arans/Zotero/storage/M5NFVT8X/Guo et al. - 2025 - Tensor Network Structure Search Using Program Synthesis.pdf;/home/arans/Zotero/storage/BQWGQLCR/2502.html}
}


@inproceedings{malikSamplingBasedMethodTensor2021,
  title = {A {{Sampling-Based Method}} for {{Tensor Ring Decomposition}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Malik, Osman Asif and Becker, Stephen},
  date = {2021-07-01},
  pages = {7400--7411},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/malik21b.html},
  urldate = {2025-02-24},
  abstract = {We propose a sampling-based method for computing the tensor ring (TR) decomposition of a data tensor. The method uses leverage score sampled alternating least squares to fit the TR cores in an iterative fashion. By taking advantage of the special structure of TR tensors, we can efficiently estimate the leverage scores and attain a method which has complexity sublinear in the number of input tensor entries. We provide high-probability relative-error guarantees for the sampled least squares problems. We compare our proposal to existing methods in experiments on both synthetic and real data. Our method achieves substantial speedup—sometimes two or three orders of magnitude—over competing methods, while maintaining good accuracy. We also provide an example of how our method can be used for rapid feature extraction.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/arans/Zotero/storage/757ASAT6/Malik y Becker - 2021 - A Sampling-Based Method for Tensor Ring Decomposition.pdf;/home/arans/Zotero/storage/LHNSF2PE/Malik y Becker - 2021 - A Sampling-Based Method for Tensor Ring Decomposition.pdf}
}

@online{mickelinAlgorithmsComputingTensor2020,
  title = {On {{Algorithms}} for and {{Computing}} with the {{Tensor Ring Decomposition}}},
  author = {Mickelin, Oscar and Karaman, Sertac},
  date = {2020-02-10},
  eprint = {1807.02513},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1807.02513},
  url = {http://arxiv.org/abs/1807.02513},
  urldate = {2025-02-24},
  abstract = {Tensor decompositions such as the canonical format and the tensor train format have been widely utilized to reduce storage costs and operational complexities for high-dimensional data, achieving linear scaling with the input dimension instead of exponential scaling. In this paper, we investigate even lower storage-cost representations in the tensor ring format, which is an extension of the tensor train format with variable end-ranks. Firstly, we introduce two algorithms for converting a tensor in full format to tensor ring format with low storage cost. Secondly, we detail a rounding operation for tensor rings and show how this requires new definitions of common linear algebra operations in the format to obtain storage-cost savings. Lastly, we introduce algorithms for transforming the graph structure of graph-based tensor formats, with orders of magnitude lower complexity than existing literature. The efficiency of all algorithms is demonstrated on a number of numerical examples, and in certain cases, we demonstrate significantly higher compression ratios when compared to previous approaches to using the tensor ring format.},
  pubstate = {prepublished},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/NIR7VWQR/Mickelin y Karaman - 2020 - On Algorithms for and Computing with the Tensor Ring Decomposition.pdf;/home/arans/Zotero/storage/3VHN7VFP/1807.html}
}

@article{mishraIntelligentComputationalTechniques2024,
  title = {Intelligent Computational Techniques for Physical Object Properties Discovery, Detection, and Prediction: {{A}} Comprehensive Survey},
  shorttitle = {Intelligent Computational Techniques for Physical Object Properties Discovery, Detection, and Prediction},
  author = {Mishra, Shaili and Arora, Anuja},
  date = {2024-02-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {51},
  pages = {100609},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2023.100609},
  url = {https://www.sciencedirect.com/science/article/pii/S157401372300076X},
  urldate = {2025-02-25},
  abstract = {The exploding usage of physical object properties has greatly facilitated real-time applications such as robotics to perceive exactly as it appears in existence. Changes in the nature and properties of diverse real-time systems are associated with physical properties modification due to environmental factors. These physics-based object properties features attract the researchers’ attention while developing solutions to real-life problems. But, the detection and prediction of physical properties change are very diverse, covering many physics laws and object properties (material, shape, gravitational force, color, state change) which append complexity to these tasks. Instead of well-understood physics laws, elucidating physics laws requires substantial manual modeling with the help of standardized equations and associated factors. To adopt these physical laws to get instinctive and effective outcomes, researchers started applying computational models to learn changing property behavior as a substitute for using handcrafted and equipment-generated variable states. If physical properties detection challenges are not anticipated and required measures are not precluded, the upcoming computational model-driven physical object changing will not be able to serve appropriately. Therefore, this survey paper is drafted to demonstrate comprehensive theoretical and empirical studies of physical object properties detection and prediction. Furthermore, a generic paradigm is proposed to work in this direction along with characterization parameters of numerous physical object properties. A brief summarization of applicable machine learning, deep learning, and metaheuristic approaches is presented. An extensive list of released and openly available datasets for varying objects and parameters rendered for researchers. Additionally, performance measures regarding computational techniques for physical properties discovery and detection for quantitative evaluation of outcomes are also entailed. Finally, a few open research issues that need to be explored in the future are specified.},
  keywords = {Computational Intelligence,Deep learning,Machine learning,Meta-heuristic,Performance measures,Physical properties,Reinforcement learning},
  file = {/home/arans/Zotero/storage/VD6JUIJW/S157401372300076X.html}
}

@online{neillOverviewNeuralNetwork2020,
  title = {An {{Overview}} of {{Neural Network Compression}}},
  author = {Neill, James O'},
  date = {2020-08-01},
  eprint = {2006.03669},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.03669},
  url = {http://arxiv.org/abs/2006.03669},
  urldate = {2025-02-25},
  abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures\textbackslash footnote\{For an introduction to deep learning, see \textasciitilde\textbackslash citet\{goodfellow2016deep\}\}, namely, Recurrent Neural Networks\textasciitilde\textbackslash citep[(RNNs)][]\{rumelhart1985learning,hochreiter1997long\}, Convolutional Neural Networks\textasciitilde\textbackslash citep\{fukushima1980neocognitron\}\textasciitilde\textbackslash footnote\{For an up to date overview see\textasciitilde\textbackslash citet\{khan2019survey\}\} and Self-Attention based networks\textasciitilde\textbackslash citep\{vaswani2017attention\}\textbackslash footnote\{For a general overview of self-attention networks, see \textasciitilde\textbackslash citet\{chaudhari2019attentive\}.\},\textbackslash footnote\{For more detail and their use in natural language processing, see\textasciitilde\textbackslash citet\{hu2019introductory\}\}. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/X7KW2L2H/Neill - 2020 - An Overview of Neural Network Compression.pdf;/home/arans/Zotero/storage/SVISX6KR/2006.html}
}

@online{novikovTensorizingNeuralNetworks2015,
  title = {Tensorizing {{Neural Networks}}},
  author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
  date = {2015-12-20},
  eprint = {1509.06569},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1509.06569},
  url = {http://arxiv.org/abs/1509.06569},
  urldate = {2025-03-05},
  abstract = {Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/arans/Zotero/storage/ULH6GF4L/Novikov et al. - 2015 - Tensorizing Neural Networks.pdf;/home/arans/Zotero/storage/7LH3A3QN/1509.html}
}

@online{obukhovTBasisCompactRepresentation2021,
  title = {T-{{Basis}}: A {{Compact Representation}} for {{Neural Networks}}},
  shorttitle = {T-{{Basis}}},
  author = {Obukhov, Anton and Rakhuba, Maxim and Georgoulis, Stamatios and Kanakis, Menelaos and Dai, Dengxin and Gool, Luc Van},
  date = {2021-07-13},
  eprint = {2007.06631},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2007.06631},
  url = {http://arxiv.org/abs/2007.06631},
  urldate = {2025-02-24},
  abstract = {We introduce T-Basis, a novel concept for a compact representation of a set of tensors, each of an arbitrary shape, which is often seen in Neural Networks. Each of the tensors in the set is modeled using Tensor Rings, though the concept applies to other Tensor Networks. Owing its name to the T-shape of nodes in diagram notation of Tensor Rings, T-Basis is simply a list of equally shaped three-dimensional tensors, used to represent Tensor Ring nodes. Such representation allows us to parameterize the tensor set with a small number of parameters (coefficients of the T-Basis tensors), scaling logarithmically with each tensor's size in the set and linearly with the dimensionality of T-Basis. We evaluate the proposed approach on the task of neural network compression and demonstrate that it reaches high compression rates at acceptable performance drops. Finally, we analyze memory and operation requirements of the compressed networks and conclude that T-Basis networks are equally well suited for training and inference in resource-constrained environments and usage on the edge devices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/E5SFIX9L/Obukhov et al. - 2021 - T-Basis a Compact Representation for Neural Networks.pdf;/home/arans/Zotero/storage/G5MT2NQX/2007.html}
}

@article{orusTensorNetworksComplex2019,
  title = {Tensor Networks for Complex Quantum Systems},
  author = {Orús, Román},
  date = {2019-09},
  journaltitle = {Nature Reviews Physics},
  shortjournal = {Nat Rev Phys},
  volume = {1},
  number = {9},
  pages = {538--550},
  publisher = {Nature Publishing Group},
  issn = {2522-5820},
  doi = {10.1038/s42254-019-0086-7},
  url = {https://www.nature.com/articles/s42254-019-0086-7},
  urldate = {2025-05-14},
  abstract = {Originally developed in the context of condensed-matter physics and based on renormalization group ideas, tensor networks have been revived thanks to quantum information theory and the progress in understanding the role of entanglement in quantum many-body systems. Moreover, tensor network states have turned out to play a key role in other scientific disciplines. In this context, here I provide an overview of the basic concepts and key developments in the field. I briefly discuss the most important tensor network structures and algorithms, together with an outline of advances related to global and gauge symmetries, fermions, topological order, classification of phases, entanglement Hamiltonians, holografic duality, artificial intelligence, the 2D Hubbard model, 2D quantum antiferromagnets, conformal field theory, quantum chemistry, disordered systems and many-body localization.},
  langid = {english},
  keywords = {Condensed-matter physics,Quantum information,Theoretical physics},
  file = {/home/arans/Zotero/storage/TC2ICCAW/Orús - 2019 - Tensor networks for complex quantum systems.pdf}
}

@article{oseledetsTensorTrainDecomposition2011,
  title = {Tensor-{{Train Decomposition}}},
  author = {Oseledets, I. V.},
  date = {2011-01},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  volume = {33},
  number = {5},
  pages = {2295--2317},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {1064-8275},
  doi = {10.1137/090752286},
  url = {https://epubs.siam.org/doi/10.1137/090752286},
  urldate = {2025-03-24},
  abstract = {Tensors arise naturally in high-dimensional problems in chemistry, financial mathematics, and many other areas. The numerical treatment of such problems is difficult due to the curse of dimensionality: the number of unknowns and the computational complexity grow exponentially with the dimension of the problem. To break the curse of dimensionality, low-parametric representations, or formats, have to be used. In this paper we make use of the TT-format (tensor-train format) which is one of the most effective stable representations of high-dimensional tensors. Basic linear algebra operations in the TT-format are now well developed. Our goal is to provide a “black-box” type of solver for linear systems where both the matrix and the right-hand side are in the TT-format. An efficient DMRG (density matrix renormalization group) method is proposed, and several tricks are employed to make it work. The numerical experiments confirm the effectiveness of our approach.}
}

@article{panagakisTensorMethodsComputer2021,
  title = {Tensor {{Methods}} in {{Computer Vision}} and {{Deep Learning}}},
  author = {Panagakis, Yannis and Kossaifi, Jean and Chrysos, Grigorios G. and Oldfield, James and Nicolaou, Mihalis A. and Anandkumar, Anima and Zafeiriou, Stefanos},
  date = {2021-05},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {109},
  number = {5},
  eprint = {2107.03436},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {863--890},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2021.3074329},
  url = {http://arxiv.org/abs/2107.03436},
  urldate = {2025-03-09},
  abstract = {Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks. This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase of tensor methods, especially in deep learning architectures, and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of the paper and implementing them, step-by-step with TensorLy.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/arans/Zotero/storage/F54KSHQT/Panagakis et al. - 2021 - Tensor Methods in Computer Vision and Deep Learning.pdf;/home/arans/Zotero/storage/N2KKT4RH/2107.html}
}

@article{panTedNetPytorchToolkit2022,
  title = {{{TedNet}}: {{A Pytorch Toolkit}} for {{Tensor Decomposition Networks}}},
  shorttitle = {{{TedNet}}},
  author = {Pan, Yu and Wang, Maolin and Xu, Zenglin},
  date = {2022-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {469},
  eprint = {2104.05018},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {234--238},
  issn = {09252312},
  doi = {10.1016/j.neucom.2021.10.064},
  url = {http://arxiv.org/abs/2104.05018},
  urldate = {2025-03-07},
  abstract = {Tensor Decomposition Networks (TDNs) prevail for their inherent compact architectures. To give more researchers a flexible way to exploit TDNs, we present a Pytorch toolkit named TedNet. TedNet implements 5 kinds of tensor decomposition(i.e., CANDECOMP/PARAFAC (CP), Block-Term Tucker (BTT), Tucker-2, Tensor Train (TT) and Tensor Ring (TR) on traditional deep neural layers, the convolutional layer and the fully-connected layer. By utilizing the basic layers, it is simple to construct a variety of TDNs. TedNet is available at https://github.com/tnbar/tednet.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/VZIBHP3J/Pan et al. - 2022 - TedNet A Pytorch Toolkit for Tensor Decomposition Networks.pdf;/home/arans/Zotero/storage/SQ86QPH8/2104.html}
}

@incollection{romanTensorProducts2008,
  title = {Tensor {{Products}}},
  booktitle = {Advanced {{Linear Algebra}}},
  author = {Roman, Steven},
  editor = {Roman, Steven},
  date = {2008},
  pages = {355--409},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-0-387-72831-5_14},
  url = {https://doi.org/10.1007/978-0-387-72831-5_14},
  urldate = {2025-03-25},
  isbn = {978-0-387-72831-5},
  langid = {english},
  file = {/home/arans/Zotero/storage/XYZBU59B/Roman - 2008 - Tensor Products.pdf}
}

@article{schindlerAlgorithmsTensorNetwork2020,
  title = {Algorithms for {{Tensor Network Contraction Ordering}}},
  author = {Schindler, Frank and Jermyn, Adam S.},
  date = {2020-09-01},
  journaltitle = {Machine Learning: Science and Technology},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  volume = {1},
  number = {3},
  eprint = {2001.08063},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {035001},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/ab94c5},
  url = {http://arxiv.org/abs/2001.08063},
  urldate = {2025-04-01},
  abstract = {Contracting tensor networks is often computationally demanding. Well-designed contraction sequences can dramatically reduce the contraction cost. We explore the performance of simulated annealing and genetic algorithms, two common discrete optimization techniques, to this ordering problem. We benchmark their performance as well as that of the commonly-used greedy search on physically relevant tensor networks. Where computationally feasible, we also compare them with the optimal contraction sequence obtained by an exhaustive search. We find that the algorithms we consider consistently outperform a greedy search given equal computational resources, with an advantage that scales with tensor network size. We compare the obtained contraction sequences and identify signs of highly non-local optimization, with the more sophisticated algorithms sacrificing run-time early in the contraction for better overall performance.},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Physics - Computational Physics,Quantum Physics},
  file = {/home/arans/Zotero/storage/TRC8TKKJ/Schindler y Jermyn - 2020 - Algorithms for Tensor Network Contraction Ordering.pdf;/home/arans/Zotero/storage/7QUGXYLM/2001.html}
}

@online{sharipovQuickIntroductionTensor2004,
  title = {Quick Introduction to Tensor Analysis},
  author = {Sharipov, Ruslan},
  date = {2004-03-16},
  eprint = {math/0403252},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.math/0403252},
  url = {http://arxiv.org/abs/math/0403252},
  urldate = {2025-03-25},
  abstract = {I wrote this book in a "do-it-yourself" style so that I give only a draft of tensor theory, which includes formulating definitions and theorems and giving basic ideas and formulas. All other work such as proving consistence of definitions, deriving formulas, proving theorems or completing details to proofs is left to the reader in the form of numerous exercises. I hope that this style makes learning the subject really quick and more effective for understanding and memorizing.},
  pubstate = {prepublished},
  keywords = {Mathematical Physics,Mathematics - History and Overview,Mathematics - Mathematical Physics},
  file = {/home/arans/Zotero/storage/DCI93IDE/Sharipov - 2004 - Quick introduction to tensor analysis.pdf;/home/arans/Zotero/storage/SUCI5QKJ/0403252.html}
}

@online{taoScalableBayesianTensor2024,
  title = {Scalable {{Bayesian Tensor Ring Factorization}} for {{Multiway Data Analysis}}},
  author = {Tao, Zerui and Tanaka, Toshihisa and Zhao, Qibin},
  date = {2024-12-04},
  eprint = {2412.03321},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2412.03321},
  url = {http://arxiv.org/abs/2412.03321},
  urldate = {2025-02-24},
  abstract = {Tensor decompositions play a crucial role in numerous applications related to multi-way data analysis. By employing a Bayesian framework with sparsity-inducing priors, Bayesian Tensor Ring (BTR) factorization offers probabilistic estimates and an effective approach for automatically adapting the tensor ring rank during the learning process. However, previous BTR method employs an Automatic Relevance Determination (ARD) prior, which can lead to sub-optimal solutions. Besides, it solely focuses on continuous data, whereas many applications involve discrete data. More importantly, it relies on the Coordinate-Ascent Variational Inference (CAVI) algorithm, which is inadequate for handling large tensors with extensive observations. These limitations greatly limit its application scales and scopes, making it suitable only for small-scale problems, such as image/video completion. To address these issues, we propose a novel BTR model that incorporates a nonparametric Multiplicative Gamma Process (MGP) prior, known for its superior accuracy in identifying latent structures. To handle discrete data, we introduce the P\textbackslash 'olya-Gamma augmentation for closed-form updates. Furthermore, we develop an efficient Gibbs sampler for consistent posterior simulation, which reduces the computational complexity of previous VI algorithm by two orders, and an online EM algorithm that is scalable to extremely large tensors. To showcase the advantages of our model, we conduct extensive experiments on both simulation data and real-world applications.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/PUELZQNA/Tao et al. - 2024 - Scalable Bayesian Tensor Ring Factorization for Multiway Data Analysis.pdf;/home/arans/Zotero/storage/45CUHEIS/2412.html}
}

@article{wangSVDbasedAlgorithmsFullyconnected2024,
  title = {{{SVD-based}} Algorithms for Fully-Connected Tensor Network Decomposition},
  author = {Wang, Mengyu and Li, Hanyu},
  date = {2024-06-03},
  journaltitle = {Computational and Applied Mathematics},
  shortjournal = {Comp. Appl. Math.},
  volume = {43},
  number = {5},
  pages = {265},
  issn = {1807-0302},
  doi = {10.1007/s40314-024-02772-w},
  url = {https://doi.org/10.1007/s40314-024-02772-w},
  urldate = {2025-03-20},
  abstract = {The popular fully-connected tensor network (FCTN) decomposition has achieved successful applications in many fields. A standard method to this decomposition is the alternating least squares. However, it often converges slowly and suffers from issues of numerical stability. In this work, we investigate the SVD-based algorithms for FCTN decomposition to tackle the aforementioned deficiencies. On the basis of a result about FCTN-ranks, a deterministic algorithm, namely FCTN-SVD, is first proposed, which can approximate the FCTN decomposition under a fixed accuracy. Then, we present the randomized version of the algorithm. Both synthetic and real data are used to test our algorithms. Numerical results show that they perform much better than the existing methods, and the randomized algorithm can indeed yield acceleration on FCTN-SVD. Moreover, we also apply our algorithms to tensor-on-vector regression and achieve quite decent performance.},
  langid = {english},
  keywords = {15A69,49M27,68W20,68W25,Alternating least squares,Fully-connected tensor network decomposition,Randomized algorithm,Sketching,SVD},
  file = {/home/arans/Zotero/storage/BAKG65DK/Wang y Li - 2024 - SVD-based algorithms for fully-connected tensor network decomposition.pdf}
}

@online{wangTensorNetworksMeet2023,
  title = {Tensor {{Networks Meet Neural Networks}}: {{A Survey}} and {{Future Perspectives}}},
  shorttitle = {Tensor {{Networks Meet Neural Networks}}},
  author = {Wang, Maolin and Pan, Yu and Xu, Zenglin and Yang, Xiangli and Li, Guangxi and Cichocki, Andrzej},
  date = {2023-05-08},
  eprint = {2302.09019},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.09019},
  url = {http://arxiv.org/abs/2302.09019},
  urldate = {2025-03-09},
  abstract = {Tensor networks (TNs) and neural networks (NNs) are two fundamental data modeling approaches. TNs were introduced to solve the curse of dimensionality in large-scale tensors by converting an exponential number of dimensions to polynomial complexity. As a result, they have attracted significant attention in the fields of quantum physics and machine learning. Meanwhile, NNs have displayed exceptional performance in various applications, e.g., computer vision, natural language processing, and robotics research. Interestingly, although these two types of networks originate from different observations, they are inherently linked through the common multilinearity structure underlying both TNs and NNs, thereby motivating a significant number of intellectual developments regarding combinations of TNs and NNs. In this paper, we refer to these combinations as tensorial neural networks (TNNs), and present an introduction to TNNs in three primary aspects: network compression, information fusion, and quantum circuit simulation. Furthermore, this survey also explores methods for improving TNNs, examines flexible toolboxes for implementing TNNs, and documents TNN development while highlighting potential future directions. To the best of our knowledge, this is the first comprehensive survey that bridges the connections among NNs, TNs, and quantum circuits. We provide a curated list of TNNs at \textbackslash url\{https://github.com/tnbar/awesome-tensorial-neural-networks\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/EWMTFVHQ/Wang et al. - 2023 - Tensor Networks Meet Neural Networks A Survey and Future Perspectives.pdf;/home/arans/Zotero/storage/34RZIB8N/2302.html}
}

@online{wangWideCompressionTensor2018,
  title = {Wide {{Compression}}: {{Tensor Ring Nets}}},
  shorttitle = {Wide {{Compression}}},
  author = {Wang, Wenqi and Sun, Yifan and Eriksson, Brian and Wang, Wenlin and Aggarwal, Vaneet},
  date = {2018-02-25},
  eprint = {1802.09052},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.09052},
  url = {http://arxiv.org/abs/1802.09052},
  urldate = {2025-02-24},
  abstract = {Deep neural networks have demonstrated state-of-the-art performance in a variety of real-world applications. In order to obtain performance gains, these networks have grown larger and deeper, containing millions or even billions of parameters and over a thousand layers. The trade-off is that these large architectures require an enormous amount of memory, storage, and computation, thus limiting their usability. Inspired by the recent tensor ring factorization, we introduce Tensor Ring Networks (TR-Nets), which significantly compress both the fully connected layers and the convolutional layers of deep neural networks. Our results show that our TR-Nets approach \{is able to compress LeNet-5 by \$11\textbackslash times\$ without losing accuracy\}, and can compress the state-of-the-art Wide ResNet by \$243\textbackslash times\$ with only 2.3\textbackslash\% degradation in \{Cifar10 image classification\}. Overall, this compression scheme shows promise in scientific computing and deep learning, especially for emerging resource-constrained devices such as smartphones, wearables, and IoT devices.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/TG5JPJQP/Wang et al. - 2018 - Wide Compression Tensor Ring Nets.pdf;/home/arans/Zotero/storage/FIWUZQT4/1802.html}
}

@article{whiteDensityMatrixFormulation1992,
  title = {Density Matrix Formulation for Quantum Renormalization Groups},
  author = {White, Steven R.},
  date = {1992-11-09},
  journaltitle = {Physical Review Letters},
  shortjournal = {Phys. Rev. Lett.},
  volume = {69},
  number = {19},
  pages = {2863--2866},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.69.2863},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.69.2863},
  urldate = {2025-03-27},
  abstract = {A generalization of the numerical renormalization-group procedure used first by Wilson for the Kondo problem is presented. It is shown that this formulation is optimal in a certain sense. As a demonstration of the effectiveness of this approach, results from numerical real-space renormalization-group calculations for Heisenberg chains are presented.},
  file = {/home/arans/Zotero/storage/EFCDIQDD/PhysRevLett.69.html}
}

@book{wilsonIntroductionGraphTheory2009,
  title = {Introduction to Graph Theory},
  author = {Wilson, Robin J.},
  date = {2009},
  edition = {4. ed., [Nachdr.]},
  publisher = {Prentice Hall},
  location = {Harlow Munich},
  isbn = {978-0-582-24993-6},
  langid = {english},
  pagetotal = {171}
}

@article{xieNeuralNetworkCompression2024,
  title = {Neural {{Network Compression Based}} on {{Tensor Ring Decomposition}}},
  author = {Xie, Kun and Liu, Can and Wang, Xin and Li, Xiaocan and Xie, Gaogang and Wen, Jigang and Li, Kenli},
  date = {2024},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--15},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2024.3383392},
  url = {https://ieeexplore.ieee.org/document/10510501},
  urldate = {2025-02-25},
  abstract = {Deep neural networks (DNNs) have made great breakthroughs and seen applications in many domains. However, the incomparable accuracy of DNNs is achieved with the cost of considerable memory consumption and high computational complexity, which restricts their deployment on conventional desktops and portable devices. To address this issue, low-rank factorization, which decomposes the neural network parameters into smaller sized matrices or tensors, has emerged as a promising technique for network compression. In this article, we propose leveraging the emerging tensor ring (TR) factorization to compress the neural network. We investigate the impact of both parameter tensor reshaping and TR decomposition (TRD) on the total number of compressed parameters. To achieve the maximal parameter compression, we propose an algorithm based on prime factorization that simultaneously identifies the optimal tensor reshaping and TRD. In addition, we discover that different execution orders of the core tensors result in varying computational complexities. To identify the optimal execution order, we construct a novel tree structure. Based on this structure, we propose a top-to-bottom splitting algorithm to schedule the execution of core tensors, thereby minimizing computational complexity. We have performed extensive experiments using three kinds of neural networks with three different datasets. The experimental results demonstrate that, compared with the three state-of-the-art algorithms for low-rank factorization, our algorithm can achieve better performance with much lower memory consumption and lower computational complexity.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Computational complexity,Kernel,Matrix decomposition,Neural network compression,Neural networks,Symbols,tensor ring (TR) factorization,Tensors},
  file = {/home/arans/Zotero/storage/EZFMK33Y/10510501.html}
}

@article{xuTRSTFFastAccurate2023,
  title = {{{TR-STF}}: A Fast and Accurate Tensor Ring Decomposition Algorithm via Defined Scaled Tri-Factorization},
  shorttitle = {{{TR-STF}}},
  author = {Xu, Ting and Huang, Ting-Zhu and Deng, Liang-Jian and Dou, Hong-Xia and Yokoya, Naoto},
  date = {2023-06-27},
  journaltitle = {Computational and Applied Mathematics},
  shortjournal = {Comp. Appl. Math.},
  volume = {42},
  number = {5},
  pages = {234},
  issn = {1807-0302},
  doi = {10.1007/s40314-023-02368-w},
  url = {https://doi.org/10.1007/s40314-023-02368-w},
  urldate = {2025-02-24},
  abstract = {This paper proposes an algorithm based on defined scaled tri-factorization (STF) for fast and accurate tensor ring (TR) decomposition. First, based on the fast tri-factorization approach, we define STF and design a corresponding algorithm that can more accurately represent various matrices while maintaining a similar level of computational time. Second, we apply sequential STFs to TR decomposition with theoretical proof and propose a stable (i.e., non-iterative) algorithm named TR-STF. It is a computationally more efficient algorithm than existing TR decomposition algorithms, which is beneficial when dealing with big data. Experiments on multiple randomly simulated data, highly oscillatory functions, and real-world data sets verify the effectiveness and high efficiency of the proposed TR-STF. For example, on the Pavia University data set, TR-STF is nearly 9240 and 39 times faster, respectively, and more accurate than algorithms based on alternating least squares and singular value decomposition. As an extension, we apply sequential STFs to tensor train (TT) decomposition and propose a non-iterative algorithm named TT-STF. Experimental results demonstrate the superiority of the proposed TT-STF compared with the state-of-the-art TT decomposition algorithm.},
  langid = {english},
  keywords = {68W99,Fast algorithm,Scaled tri-factorization,Tensor ring decomposition,Tensor train decomposition}
}

@online{yangCoMERAComputingMemoryEfficient2024,
  title = {{{CoMERA}}: {{Computing-}} and {{Memory-Efficient Training}} via {{Rank-Adaptive Tensor Optimization}}},
  shorttitle = {{{CoMERA}}},
  author = {Yang, Zi and Liu, Ziyue and Choudhary, Samridhi and Xie, Xinfeng and Gao, Cao and Kunzmann, Siegfried and Zhang, Zheng},
  date = {2024-12-02},
  eprint = {2405.14377},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.14377},
  url = {http://arxiv.org/abs/2405.14377},
  urldate = {2025-02-24},
  abstract = {Training large AI models such as LLMs and DLRMs costs massive GPUs and computing time. The high training cost has become only affordable to big tech companies, meanwhile also causing increasing concerns about the environmental impact. This paper presents CoMERA, a Computing- and Memory-Efficient training method via Rank-Adaptive tensor optimization. CoMERA achieves rank-adaptive tensor-compressed (pre)-training via a multi-objective optimization formulation and improves the training to provide both a high compression ratio and excellent accuracy in the training process. Our optimized numerical computation (e.g., optimized tensorized embedding and tensor-network contractions) and GPU implementation eliminate part of the run-time overhead in the tensorized training on GPU. This leads to, for the first time, \$2-3\textbackslash times\$ speedup per training epoch compared with standard training. CoMERA also outperforms the recent GaLore in terms of both memory and computing efficiency. Specifically, CoMERA is \$2\textbackslash times\$ faster per training epoch and \$9\textbackslash times\$ more memory-efficient than GaLore on a tested six-encoder transformer with single-batch training. Our method also shows \$\textbackslash sim 2\textbackslash times\$ speedup than standard pre-training on a BERT-like code-generation LLM while achieving \$4.23\textbackslash times\$ compression ratio in pre-training. With further HPC optimization, CoMERA may reduce the pre-training cost of many other LLMs. An implementation of CoMERA is available at https://github.com/ziyangjoy/CoMERA.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/HBX6TQIZ/Yang et al. - 2024 - CoMERA Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization.pdf;/home/arans/Zotero/storage/YB8LGHYV/2405.html}
}

@online{yangLatentMatricesTensor2022,
  title = {Latent {{Matrices}} for {{Tensor Network Decomposition}} and to {{Tensor Completion}}},
  author = {Yang, Peilin and Sun, Weijun and Zhao, Qibin and Zhou, Guoxu},
  date = {2022-10-19},
  eprint = {2210.03392},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.03392},
  url = {http://arxiv.org/abs/2210.03392},
  urldate = {2025-03-20},
  abstract = {The prevalent fully-connected tensor network (FCTN) has achieved excellent success to compress data. However, the FCTN decomposition suffers from slow computational speed when facing higher-order and large-scale data. Naturally, there arises an interesting question: can a new model be proposed that decomposes the tensor into smaller ones and speeds up the computation of the algorithm? This work gives a positive answer by formulating a novel higher-order tensor decomposition model that utilizes latent matrices based on the tensor network structure, which can decompose a tensor into smaller-scale data than the FCTN decomposition, hence we named it Latent Matrices for Tensor Network Decomposition (LMTN). Furthermore, three optimization algorithms, LMTN-PAM, LMTN-SVD and LMTN-AR, have been developed and applied to the tensor-completion task. In addition, we provide proofs of theoretical convergence and complexity analysis for these algorithms. Experimental results show that our algorithm has the effectiveness in both deep learning dataset compression and higher-order tensor completion, and that our LMTN-SVD algorithm is 3-6 times faster than the FCTN-PAM algorithm and only a 1.8 points accuracy drop.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning},
  file = {/home/arans/Zotero/storage/ALVFQIX2/Yang et al. - 2022 - Latent Matrices for Tensor Network Decomposition and to Tensor Completion.pdf;/home/arans/Zotero/storage/L82QSRA9/2210.html}
}

@online{yeTensorNetworkRanks2019,
  title = {Tensor Network Ranks},
  author = {Ye, Ke and Lim, Lek-Heng},
  date = {2019-02-09},
  eprint = {1801.02662},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1801.02662},
  url = {http://arxiv.org/abs/1801.02662},
  urldate = {2025-03-24},
  abstract = {In problems involving approximation, completion, denoising, dimension reduction, estimation, interpolation, modeling, order reduction, regression, etc, we argue that the near-universal practice of assuming that a function, matrix, or tensor (which we will see are all the same object in this context) has \textbackslash emph\{low rank\} may be ill-justified. There are many natural instances where the object in question has high rank with respect to the classical notions of rank: matrix rank, tensor rank, multilinear rank --- the latter two being the most straightforward generalizations of the former. To remedy this, we show that one may vastly expand these classical notions of ranks: Given any undirected graph \$G\$, there is a notion of \$G\$-rank associated with \$G\$, which provides us with as many different kinds of ranks as there are undirected graphs. In particular, the popular tensor network states in physics (e.g., \textbackslash textsc\{mps\}, \textbackslash textsc\{ttns\}, \textbackslash textsc\{peps\}) may be regarded as functions of a specific \$G\$-rank for various choices of \$G\$. Among other things, we will see that a function, matrix, or tensor may have very high matrix, tensor, or multilinear rank and yet very low \$G\$-rank for some \$G\$. In fact the difference is in the orders of magnitudes and the gaps between \$G\$-ranks and these classical ranks are arbitrarily large for some important objects in computer science, mathematics, and physics. Furthermore, we show that there is a \$G\$ such that almost every tensor has \$G\$-rank exponentially lower than its rank or the dimension of its ambient space.},
  pubstate = {prepublished},
  keywords = {Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/9K8HJWC5/Ye y Lim - 2019 - Tensor network ranks.pdf;/home/arans/Zotero/storage/JWF2D3NC/1801.html}
}

@online{yokotaVeryBasicsTensors2024,
  title = {Very {{Basics}} of {{Tensors}} with {{Graphical Notations}}: {{Unfolding}}, {{Calculations}}, and {{Decompositions}}},
  shorttitle = {Very {{Basics}} of {{Tensors}} with {{Graphical Notations}}},
  author = {Yokota, Tatsuya},
  date = {2024-11-25},
  eprint = {2411.16094},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2411.16094},
  url = {http://arxiv.org/abs/2411.16094},
  urldate = {2025-02-24},
  abstract = {Tensor network diagram (graphical notation) is a useful tool that graphically represents multiplications between multiple tensors using nodes and edges. Using the graphical notation, complex multiplications between tensors can be described simply and intuitively, and it also helps to understand the essence of tensor products. In fact, most of matrix/tensor products including inner product, outer product, Hadamard product, Kronecker product, and Khatri-Rao product can be written in graphical notation. These matrix/tensor operations are essential building blocks for the use of matrix/tensor decompositions in signal processing and machine learning. The purpose of this lecture note is to learn the very basics of tensors and how to represent them in mathematical symbols and graphical notation. Many papers using tensors omit these detailed definitions and explanations, which can be difficult for the reader. I hope this note will be of help to such readers.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/home/arans/Zotero/storage/9K26XGPK/Yokota - 2024 - Very Basics of Tensors with Graphical Notations Unfolding, Calculations, and Decompositions.pdf;/home/arans/Zotero/storage/CD88V9S2/2411.html}
}

@article{yuanRankMinimizationTensor2020,
  title = {Rank Minimization on Tensor Ring: An Efficient Approach for Tensor Decomposition and Completion},
  shorttitle = {Rank Minimization on Tensor Ring},
  author = {Yuan, Longhao and Li, Chao and Cao, Jianting and Zhao, Qibin},
  date = {2020-03-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {109},
  number = {3},
  pages = {603--622},
  issn = {1573-0565},
  doi = {10.1007/s10994-019-05846-7},
  url = {https://doi.org/10.1007/s10994-019-05846-7},
  urldate = {2025-02-24},
  abstract = {In recent studies, tensor ring decomposition (TRD) has become a promising model for tensor completion. However, TRD suffers from the rank selection problem due to the undetermined multilinear rank. For tensor decomposition with missing entries, the sub-optimal rank selection of traditional methods leads to the overfitting/underfitting problem. In this paper, we first explore the latent space of the TRD and theoretically prove the relationship between the TR-rank and the rank of the tensor unfoldings. Then, we propose two tensor completion models by imposing the different low-rank regularizations on the TR-factors, by which the TR-rank of the underlying tensor is minimized and the low-rank structures of the underlying tensor are exploited. By employing the alternating direction method of multipliers scheme, our algorithms obtain the TR factors and the underlying tensor simultaneously. In experiments of tensor completion tasks, our algorithms show robustness to rank selection and high computation efficiency, in comparison to traditional low-rank approximation algorithms.},
  langid = {english},
  keywords = {ADMM scheme,Artificial Intelligence,Structured nuclear norm,Tensor completion,Tensor ring decomposition},
  file = {/home/arans/Zotero/storage/R7SVXDP8/Yuan et al. - 2020 - Rank minimization on tensor ring an efficient approach for tensor decomposition and completion.pdf}
}

@online{yuPracticalSketchingBasedRandomized2022,
  title = {Practical {{Sketching-Based Randomized Tensor Ring Decomposition}}},
  author = {Yu, Yajie and Li, Hanyu},
  date = {2022-09-12},
  eprint = {2209.05647},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.2209.05647},
  url = {http://arxiv.org/abs/2209.05647},
  urldate = {2025-02-25},
  abstract = {Based on sketching techniques, we propose two randomized algorithms for tensor ring (TR) decomposition. Specifically, by defining new tensor products and investigating their properties, we apply the Kronecker sub-sampled randomized Fourier transform and TensorSketch to the alternating least squares problems derived from the minimization problem of TR decomposition to devise the randomized algorithms. From the former, we find an algorithmic framework based on random projection for randomized TR decomposition. Theoretical results on sketch size and complexity analyses for the two algorithms are provided. We compare our proposals with the state-of-the-art method using both synthetic and real data. Numerical results show that they have quite decent performance in accuracy and computing time},
  pubstate = {prepublished},
  keywords = {Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/CQWTDZRS/Yu y Li - 2022 - Practical Sketching-Based Randomized Tensor Ring Decomposition.pdf;/home/arans/Zotero/storage/MI2VW9VF/2209.html}
}

@article{zengBayesianTensorNetwork2024,
  title = {Bayesian Tensor Network Structure Search and Its Application to Tensor Completion},
  author = {Zeng, Junhua and Zhou, Guoxu and Qiu, Yuning and Li, Chao and Zhao, Qibin},
  date = {2024-07-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {175},
  pages = {106290},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2024.106290},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608024002144},
  urldate = {2025-05-20},
  abstract = {Tensor network (TN) has demonstrated remarkable efficacy in the compact representation of high-order data. In contrast to the TN methods with pre-determined structures, the recently introduced tensor network structure search (TNSS) methods automatically learn a compact TN structure from the data, gaining increasing attention. Nonetheless, TNSS requires time-consuming manual adjustments of the penalty parameters that control the model complexity to achieve better performance, especially in the presence of missing or noisy data. To provide an effective solution to this problem, in this paper, we propose a parameters tuning-free TNSS algorithm based on Bayesian modeling, aiming at conducting TNSS in a fully data-driven manner. Specifically, the uncertainty in the data corruption is well-incorporated in the prior setting of the probabilistic model. For TN structure determination, we reframe it as a rank learning problem of the fully-connected tensor network (FCTN), integrating the generalized inverse Gaussian (GIG) distribution for low-rank promotion. To eliminate the need for hyperparameter tuning, we adopt a fully Bayesian approach and propose an efficient Markov chain Monte Carlo (MCMC) algorithm for posterior distribution sampling. Compared with the previous TNSS method, experiment results demonstrate the proposed algorithm can effectively and efficiently find the latent TN structures of the data under various missing and noise conditions and achieves the best recovery results. Furthermore, our method exhibits superior performance in tensor completion with real-world data compared to other state-of-the-art tensor-decomposition-based completion methods.},
  keywords = {Bayesian modeling,Tensor completion,Tensor network decomposition,Tensor network structure search},
  file = {/home/arans/Zotero/storage/DU6DG8WR/S0893608024002144.html}
}

@article{zengBayesianTensorNetwork2024a,
  title = {Bayesian Tensor Network Structure Search and Its Application to Tensor Completion},
  author = {Zeng, Junhua and Zhou, Guoxu and Qiu, Yuning and Li, Chao and Zhao, Qibin},
  date = {2024-07},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {175},
  pages = {106290},
  issn = {08936080},
  doi = {10.1016/j.neunet.2024.106290},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608024002144},
  urldate = {2025-05-20},
  abstract = {Tensor network (TN) has demonstrated remarkable efficacy in the compact representation of high-order data. In contrast to the TN methods with pre-determined structures, the recently introduced tensor network structure search (TNSS) methods automatically learn a compact TN structure from the data, gaining increasing attention. Nonetheless, TNSS requires time-consuming manual adjustments of the penalty parameters that control the model complexity to achieve better performance, especially in the presence of missing or noisy data. To provide an effective solution to this problem, in this paper, we propose a parameters tuning-free TNSS algorithm based on Bayesian modeling, aiming at conducting TNSS in a fully data-driven manner. Specifically, the uncertainty in the data corruption is well-incorporated in the prior setting of the probabilistic model. For TN structure determination, we reframe it as a rank learning problem of the fully-connected tensor network (FCTN), integrating the generalized inverse Gaussian (GIG) distribution for low-rank promotion. To eliminate the need for hyperparameter tuning, we adopt a fully Bayesian approach and propose an efficient Markov chain Monte Carlo (MCMC) algorithm for posterior distribution sampling. Compared with the previous TNSS method, experiment results demonstrate the proposed algorithm can effectively and efficiently find the latent TN structures of the data under various missing and noise conditions and achieves the best recovery results. Furthermore, our method exhibits superior performance in tensor completion with real-world data compared to other state-of-the-art tensor-decomposition-based completion methods.},
  langid = {english},
  file = {/home/arans/Zotero/storage/5Q9YIAFL/Zeng et al. - 2024 - Bayesian tensor network structure search and its application to tensor completion.pdf}
}

@online{zhaoTensorRingDecomposition2016,
  title = {Tensor {{Ring Decomposition}}},
  author = {Zhao, Qibin and Zhou, Guoxu and Xie, Shengli and Zhang, Liqing and Cichocki, Andrzej},
  date = {2016-06-17},
  eprint = {1606.05535},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1606.05535},
  url = {http://arxiv.org/abs/1606.05535},
  urldate = {2025-03-04},
  abstract = {Tensor networks have in recent years emerged as the powerful tools for solving the large-scale optimization problems. One of the most popular tensor network is tensor train (TT) decomposition that acts as the building blocks for the complicated tensor networks. However, the TT decomposition highly depends on permutations of tensor dimensions, due to its strictly sequential multilinear products over latent cores, which leads to difficulties in finding the optimal TT representation. In this paper, we introduce a fundamental tensor decomposition model to represent a large dimensional tensor by a circular multilinear products over a sequence of low dimensional cores, which can be graphically interpreted as a cyclic interconnection of 3rd-order tensors, and thus termed as tensor ring (TR) decomposition. The key advantage of TR model is the circular dimensional permutation invariance which is gained by employing the trace operation and treating the latent cores equivalently. TR model can be viewed as a linear combination of TT decompositions, thus obtaining the powerful and generalized representation abilities. For optimization of latent cores, we present four different algorithms based on the sequential SVDs, ALS scheme, and block-wise ALS techniques. Furthermore, the mathematical properties of TR model are investigated, which shows that the basic multilinear algebra can be performed efficiently by using TR representaions and the classical tensor decompositions can be conveniently transformed into the TR representation. Finally, the experiments on both synthetic signals and real-world datasets were conducted to evaluate the performance of different algorithms.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Data Structures and Algorithms,Computer Science - Numerical Analysis},
  file = {/home/arans/Zotero/storage/YCNI4PLW/Zhao et al. - 2016 - Tensor Ring Decomposition.pdf;/home/arans/Zotero/storage/BD6X7K7B/1606.html}
}

@article{zhengFullyConnectedTensorNetwork2021,
  title = {Fully-{{Connected Tensor Network Decomposition}} and {{Its Application}} to {{Higher-Order Tensor Completion}}},
  author = {Zheng, Yu-Bang and Huang, Ting-Zhu and Zhao, Xi-Le and Zhao, Qibin and Jiang, Tai-Xiang},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {12},
  pages = {11071--11078},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i12.17321},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17321},
  urldate = {2025-03-04},
  abstract = {The popular tensor train (TT) and tensor ring (TR) decompositions have achieved promising results in science and engineering. However, TT and TR decompositions only establish an operation between adjacent two factors and are highly sensitive to the permutation of tensor modes, leading to an inadequate and inflexible representation. In this paper, we propose a generalized tensor decomposition, which decomposes an Nth-order tensor into a set of Nth-order factors and establishes an operation between any two factors. Since it can be graphically interpreted as a fully-connected network, we named it fully-connected tensor network (FCTN) decomposition. The superiorities of the FCTN decomposition lie in the outstanding capability for characterizing adequately the intrinsic correlations between any two modes of tensors and the essential invariance for transposition. Furthermore, we employ the FCTN decomposition to one representative task, i.e., tensor completion, and develop an efficient solving algorithm based on proximal alternating minimization. Theoretically, we prove the convergence of the developed algorithm, i.e., the sequence obtained by it globally converges to a critical point. Experimental results substantiate that the proposed method compares favorably to the state-of-the-art methods based on other tensor decompositions.},
  issue = {12},
  langid = {english},
  keywords = {Applications},
  file = {/home/arans/Zotero/storage/DBK847TT/Zheng et al. - 2021 - Fully-Connected Tensor Network Decomposition and Its Application to Higher-Order Tensor Completion.pdf}
}

@online{zhouTensorStarTensor2024,
  title = {Tensor {{Star Tensor Decomposition}} and {{Its Applications}} to {{Higher-order Compression}} and {{Completion}}},
  author = {Zhou, Wuyang and Zheng, Yu-Bang and Zhao, Qibin and Mandic, Danilo},
  date = {2024-09-06},
  eprint = {2403.10481},
  eprinttype = {arXiv},
  eprintclass = {eess},
  doi = {10.48550/arXiv.2403.10481},
  url = {http://arxiv.org/abs/2403.10481},
  urldate = {2025-03-24},
  abstract = {A novel tensor decomposition framework, termed Tensor Star (TS) decomposition, is proposed which represents a new type of tensor network decomposition based on tensor contractions. This is achieved by connecting the core tensors in a ring shape, whereby the core tensors act as skip connections between the factor tensors and allow for direct correlation characterisation between any two arbitrary dimensions. Uniquely, this makes it possible to decompose an order-\$N\$ tensor into \$N\$ order-\$3\$ factor tensors \$\textbackslash\{\textbackslash mathcal\{G\}\_\{k\}\textbackslash\}\_\{k=1\}\textasciicircum\{N\}\$ and \$N\$ order-\$4\$ core tensors \$\textbackslash\{\textbackslash mathcal\{C\}\_\{k\}\textbackslash\}\_\{k=1\}\textasciicircum\{N\}\$, which are arranged in a star shape. Unlike the class of Tensor Train (TT) decompositions, these factor tensors are not directly connected to one another. The so obtained core tensors also enable consecutive factor tensors to have different latent ranks. In this way, the TS decomposition alleviates the "curse of dimensionality" and controls the "curse of ranks", exhibiting a storage complexity which scales linearly with the number of dimensions and as the fourth power of the ranks.},
  pubstate = {prepublished},
  keywords = {Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/arans/Zotero/storage/GMHGJRJC/Zhou et al. - 2024 - Tensor Star Tensor Decomposition and Its Applications to Higher-order Compression and Completion.pdf;/home/arans/Zotero/storage/CQLCFDBY/2403.html}
}

@article{rohrig-zollnerPerformanceLowrankTensortrain2022,
  title = {Performance of the Low-Rank Tensor-Train {{SVD}} ({{TT-SVD}}) for Large Dense Tensors on Modern Multi-Core {{CPUs}}},
  author = {Röhrig-Zöllner, Melven and Thies, Jonas and Basermann, Achim},
  date = {2022-08},
  journaltitle = {SIAM Journal on Scientific Computing},
  shortjournal = {SIAM J. Sci. Comput.},
  volume = {44},
  number = {4},
  eprint = {2102.00104},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {C287-C309},
  issn = {1064-8275, 1095-7197},
  doi = {10.1137/21M1395545},
  url = {http://arxiv.org/abs/2102.00104},
  urldate = {2025-05-28},
  abstract = {There are several factorizations of multi-dimensional tensors into lower-dimensional components, known as `tensor networks'. We consider the popular `tensor-train' (TT) format and ask: How efficiently can we compute a low-rank approximation from a full tensor on current multi-core CPUs? Compared to sparse and dense linear algebra, kernel libraries for multi-linear algebra are rare and typically not as well optimized. Linear algebra libraries like BLAS and LAPACK may provide the required operations in principle, but often at the cost of additional data movements for rearranging memory layouts. Furthermore, these libraries are typically optimized for the compute-bound case (e.g.\textbackslash{} square matrix operations) whereas low-rank tensor decompositions lead to memory bandwidth limited operations. We propose a `tensor-train singular value decomposition' (TT-SVD) algorithm based on two building blocks: a `Q-less tall-skinny QR' factorization, and a fused tall-skinny matrix-matrix multiplication and reshape operation. We analyze the performance of the resulting TT-SVD algorithm using the Roofline performance model. In addition, we present performance results for different algorithmic variants for shared-memory as well as distributed-memory architectures. Our experiments show that commonly used TT-SVD implementations suffer severe performance penalties. We conclude that a dedicated library for tensor factorization kernels would benefit the community: Computing a low-rank approximation can be as cheap as reading the data twice from main memory. As a consequence, an implementation that achieves realistic performance will move the limit at which one has to resort to randomized methods that only process part of the data.},
  keywords = {Computer Science - Mathematical Software,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  file = {/home/arans/Zotero/storage/5H6JTNVK/Röhrig-Zöllner et al. - 2022 - Performance of the low-rank tensor-train SVD (TT-SVD) for large dense tensors on modern multi-core C.pdf;/home/arans/Zotero/storage/GMK4JIEC/2102.html}
}

  @article{christandlTensorRankNot2018,
  title = {Tensor Rank Is Not Multiplicative under the Tensor Product},
  author = {Christandl, Matthias and Jensen, Asger Kjærulff and Zuiddam, Jeroen},
  date = {2018-04},
  journaltitle = {Linear Algebra and its Applications},
  shortjournal = {Linear Algebra and its Applications},
  volume = {543},
  eprint = {1705.09379},
  eprinttype = {arXiv},
  eprintclass = {math},
  pages = {125--139},
  issn = {00243795},
  doi = {10.1016/j.laa.2017.12.020},
  url = {http://arxiv.org/abs/1705.09379},
  urldate = {2025-05-28},
  abstract = {The tensor rank of a tensor t is the smallest number r such that t can be decomposed as a sum of r simple tensors. Let s be a k-tensor and let t be an l-tensor. The tensor product of s and t is a (k + l)-tensor. Tensor rank is sub-multiplicative under the tensor product. We revisit the connection between restrictions and degenerations. A result of our study is that tensor rank is not in general multiplicative under the tensor product. This answers a question of Draisma and Saptharishi. Specifically, if a tensor t has border rank strictly smaller than its rank, then the tensor rank of t is not multiplicative under taking a sufficiently hight tensor product power. The "tensor Kronecker product" from algebraic complexity theory is related to our tensor product but different, namely it multiplies two k-tensors to get a k-tensor. Nonmultiplicativity of the tensor Kronecker product has been known since the work of Strassen. It remains an open question whether border rank and asymptotic rank are multiplicative under the tensor product. Interestingly, lower bounds on border rank obtained from generalised flattenings (including Young flattenings) multiply under the tensor product.},
  keywords = {Computer Science - Computational Complexity,Mathematics - Commutative Algebra,Quantum Physics},
  file = {/home/arans/Zotero/storage/NCTSG5WW/Christandl et al. - 2018 - Tensor rank is not multiplicative under the tensor product.pdf;/home/arans/Zotero/storage/TNEWSLUQ/1705.html}
}

@article{schindlerAlgorithmsTensorNetwork2020,
  title = {Algorithms for {{Tensor Network Contraction Ordering}}},
  author = {Schindler, Frank and Jermyn, Adam S.},
  date = {2020-09-01},
  journaltitle = {Machine Learning: Science and Technology},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  volume = {1},
  number = {3},
  eprint = {2001.08063},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {035001},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/ab94c5},
  url = {http://arxiv.org/abs/2001.08063},
  urldate = {2025-04-01},
  abstract = {Contracting tensor networks is often computationally demanding. Well-designed contraction sequences can dramatically reduce the contraction cost. We explore the performance of simulated annealing and genetic algorithms, two common discrete optimization techniques, to this ordering problem. We benchmark their performance as well as that of the commonly-used greedy search on physically relevant tensor networks. Where computationally feasible, we also compare them with the optimal contraction sequence obtained by an exhaustive search. We find that the algorithms we consider consistently outperform a greedy search given equal computational resources, with an advantage that scales with tensor network size. We compare the obtained contraction sequences and identify signs of highly non-local optimization, with the more sophisticated algorithms sacrificing run-time early in the contraction for better overall performance.},
  keywords = {Computer Science - Neural and Evolutionary Computing,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Physics - Computational Physics,Quantum Physics},
  file = {/home/arans/Zotero/storage/TRC8TKKJ/Schindler y Jermyn - 2020 - Algorithms for Tensor Network Contraction Ordering.pdf;/home/arans/Zotero/storage/7QUGXYLM/2001.html}
}

