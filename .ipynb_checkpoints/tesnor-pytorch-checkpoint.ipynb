{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca940fc4-50cb-4c46-b974-56ae84eb00d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "from tn import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abc58e-1093-41e3-935c-681bdabde8a8",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e360cc-5d40-446b-b597-80a9d16ed8f7",
   "metadata": {},
   "source": [
    "## Tensor Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c935d1-04f2-438f-a541-6c861143d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNLinearLayer(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape, G, R, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.layer = TN(G, in_shape + out_shape, R)\n",
    "        self.n_info = {}\n",
    "        self.bias = bias\n",
    "        self.dim_x = np.prod(in_shape)\n",
    "        self.dim_y = np.prod(out_shape)\n",
    "        self.n_info[\"ori_params\"] = self.dim_x * self.dim_y\n",
    "        self.n_info[\"t_params\"] = self.layer.get_tn_size()\n",
    "        self.in_shape = in_shape\n",
    "\n",
    "        self.bias_param = nn.Parameter(torch.randn(self.dim_y))\n",
    "        if self.bias:\n",
    "            self.bias_param = nn.Parameter(torch.randn(self.dim_y))\n",
    "    def forward(self, x):\n",
    "        r = self.layer(torch.reshape(x, (x.numel() // self.dim_x, self.dim_x)))\n",
    "        r += self.bias_param\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26989e2-c55f-41a7-a1b1-addb6b60f62d",
   "metadata": {},
   "source": [
    "## Model definitions\n",
    "We test different tensor decompositions methods by training a fully connected network model over the MNIST dataset.\n",
    "The test model has layers [784, 320, 100, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10f3b39-d795-4c26-8621-01a270b45cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LeNet-300-100 model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(784, 320),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(320, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10), # Returns logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class LeNetTN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_info = {}\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        G1 = nx.Graph()\n",
    "        G2 = nx.Graph()\n",
    "        G3 = nx.Graph()\n",
    "        G1.add_edges_from([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 1]])\n",
    "        G2.add_edges_from([[1, 2], [2, 3], [3, 4], [4, 5], [5, 1]])\n",
    "        G3.add_edges_from([[1, 2], [2, 3], [3, 1]])\n",
    "        \n",
    "        self.l1 = TNLinearLayer([7,7,4,4], [5,8,8], G1, [14,14,14,14,14,14,14], bias=True)\n",
    "        self.l2 = TNLinearLayer([5,8,8], [10, 10], G2, [10,10,10,10,10], bias=True)\n",
    "        self.l3 = TNLinearLayer([10,10], [10], G3, [7,7,7], bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.ReLU(),\n",
    "            self.l3, # Returns logits\n",
    "        )\n",
    "        self.n_info[\"ori_params\"] = sum([x.n_info[\"ori_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"t_params\"] = sum([x.n_info[\"t_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"cr\"] = self.n_info[\"ori_params\"] / self.n_info[\"t_params\"]\n",
    "\n",
    "        print(\"LeNet TN ---\")\n",
    "        print(\"Original params: \" + str(self.n_info[\"ori_params\"]))\n",
    "        print(\"TN params: \" + str(self.n_info[\"t_params\"]))\n",
    "        print(\"Compression ratio: \" + str(self.n_info[\"cr\"]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "class LeNetTNALE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_info = {}\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        G1 = nx.Graph()\n",
    "        G2 = nx.Graph()\n",
    "        G3 = nx.Graph()\n",
    "        G1.add_edges_from([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7], [7, 1], [1, 4], [2, 5]])\n",
    "        G2.add_edges_from([[1, 2], [2, 3], [3, 4], [4, 5], [5, 1]])\n",
    "        G3.add_edges_from([[1, 2], [2, 3], [3, 1]])\n",
    "        R1 = [17,17,17,17,17,17,17, 10, 10]\n",
    "        R2 = [10,10,10,10,10]\n",
    "        R3 = [7,7,7]\n",
    "\n",
    "        O1 = torch.randn([7,7,4,4,5,8,8]).to(device)\n",
    "        O2 = torch.randn([5,8,8, 10, 10]).to(device)\n",
    "        O3 = torch.randn([10,10, 10]).to(device)\n",
    "        print(\"Searching ranks of layer 1...\")\n",
    "        G1, R1 = TN.tn_ale(G1, R1, 5, 1, O1, print_iters=True)\n",
    "        G1, R1 = TN.tn_ale(G1, R1, 2, 2, O1, print_iters=True)\n",
    "        print(\"Searching ranks of layer 2...\")\n",
    "        G2, R2 = TN.tn_ale(G2, R2, 5, 1, O2, print_iters=True)\n",
    "        G2, R2 = TN.tn_ale(G2, R2, 2, 2, O2, print_iters=True)\n",
    "        print(\"Searching ranks of layer 3...\")\n",
    "        G3, R3 = TN.tn_ale(G3, R3, 5, 1, O3, print_iters=True)\n",
    "        G3, R3 = TN.tn_ale(G3, R3, 2, 2, O3, print_iters=True)\n",
    "        \n",
    "        \n",
    "        self.l1 = TNLinearLayer([7,7,4,4], [5,8,8], G1, R1, bias=True)\n",
    "        self.l2 = TNLinearLayer([5,8,8], [10, 10], G2, R2, bias=True)\n",
    "        self.l3 = TNLinearLayer([10,10], [10], G3, R3, bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.ReLU(),\n",
    "            self.l3, # Returns logits\n",
    "        )\n",
    "        self.n_info[\"ori_params\"] = sum([x.n_info[\"ori_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"t_params\"] = sum([x.n_info[\"t_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"cr\"] = self.n_info[\"ori_params\"] / self.n_info[\"t_params\"]\n",
    "\n",
    "        print(\"LeNet TR ---\")\n",
    "        print(\"Original params: \" + str(self.n_info[\"ori_params\"]))\n",
    "        print(\"TN params: \" + str(self.n_info[\"t_params\"]))\n",
    "        print(\"Compression ratio: \" + str(self.n_info[\"cr\"]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc65f0c-c2f4-43a3-87d6-c1e3fabe7061",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11912b64-a0fd-4f58-9369-9f0d46f9f6e2",
   "metadata": {},
   "source": [
    "## Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baeda3e7-08ab-400c-8369-54c6a9be7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_set(model, validation_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for x_val, y_val in validation_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            outputs = model(x_val)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def show_example_batch(train_data):\n",
    "    # Get the first batch\n",
    "    train_loader, valid_loader = train_data\n",
    "    \n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    plot_size=20\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(images[idx].numpy()), cmap='gray')\n",
    "        # print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(labels[idx].item()))\n",
    "    plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "def get_train_data():\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                  ])\n",
    "    # Download and load the training data\n",
    "    trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "    validation_size = 0.10\n",
    "    num_train = len(trainset)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    # Calculate the number of data points in the validation set\n",
    "    split = int(np.floor(validation_size * num_train))\n",
    "    print(split)\n",
    "    \n",
    "    # Train_idx => Imatges per entrenar\n",
    "    # Valid_idx => Imatges per verificar i comprovar el model\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # Create data samplers\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ced89-f2f6-4338-8993-4f7ba4987481",
   "metadata": {},
   "source": [
    "## Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f612b704-3e69-4085-88ec-d8892c90f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_data, learning_rate=1.2e-3, batch_size=60, epochs=10):\n",
    "    losses = []\n",
    "    accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    grad_norms = []\n",
    "    print(\"Training model\")\n",
    "    print(model)\n",
    "\n",
    "    train_loader, valid_loader = train_data\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    #for param in model.parameters():\n",
    "    #    print(param)\n",
    "    # Validation Calculation\n",
    "    val_check_iter = 3\n",
    "    iterations_per_epoch = len(train_loader)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            # Move data to the device\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = nn.CrossEntropyLoss()(output, y_batch)\n",
    "            loss.backward()\n",
    "    \n",
    "    \n",
    "            # Calculate and store gradient norm\n",
    "            total_grad_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "            grad_norms.append(total_grad_norm.item())\n",
    "    \n",
    "            optimizer.step()\n",
    "    \n",
    "            # Store the loss\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "            # Store the accuracy \n",
    "            _, argmax = torch.max(output, 1)\n",
    "            accuracy = (y_batch == argmax.squeeze()).float().mean()\n",
    "            accuracy_list.append(accuracy)\n",
    "    \n",
    "            if i % int(len(train_loader) / 50) == 0:\n",
    "                print(\".\", end='')\n",
    "    \n",
    "            if i % int(len(train_loader) / val_check_iter) == 0:\n",
    "                # Calculate validation accuracy at the end of each epoch\n",
    "                val_accuracy = evaluate_validation_set(model, valid_loader, device)\n",
    "                val_accuracy_list.append(val_accuracy)\n",
    "                \n",
    "                # Calculate average loss and accuracy over an epoch\n",
    "                avg_loss = torch.mean(torch.tensor(losses[-len(train_loader):]))\n",
    "                avg_accuracy = torch.mean(torch.tensor(accuracy_list[-len(train_loader):]))\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss.item():.4f}, Accuracy: {avg_accuracy.item():.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    infos = {\n",
    "        \"losses\": losses,\n",
    "        \"accuracy_list\": accuracy_list,\n",
    "        \"grad_norms\": grad_norms,\n",
    "        \"val_accuracy_list\": val_accuracy_list,\n",
    "        \"iterations_per_epoch\": iterations_per_epoch,\n",
    "        \"model_name\": model.__class__.__name__\n",
    "    }\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fcab5-cf45-4c20-8593-b7e9abfcd4f7",
   "metadata": {},
   "source": [
    "## Show training results function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7043f520-0bff-463f-b0c1-3a0fa929151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_infos(training_infos, epochs):\n",
    "    \n",
    "    average_window = 200\n",
    "    infos = []\n",
    "    for training_info in training_infos:\n",
    "        losses = torch.tensor(training_info[\"losses\"]).cpu().numpy()\n",
    "        accuracy_list = torch.tensor(training_info[\"accuracy_list\"]).cpu().numpy()\n",
    "        grad_norms = torch.tensor(training_info[\"grad_norms\"]).cpu().numpy()\n",
    "        val_accuracy_list = training_info[\"val_accuracy_list\"]\n",
    "        iterations_per_epoch = training_info[\"iterations_per_epoch\"]\n",
    "\n",
    "        average_losses = [np.mean(losses[i-average_window:i]) for i in range(0, len(losses), average_window)]\n",
    "        avg_accuracy_list = [np.mean(accuracy_list[i-average_window:i]) for i in range(0, len(accuracy_list), average_window)]\n",
    "        avg_grad_norms = [np.mean(grad_norms[i-average_window:i]) for i in range(0, len(grad_norms), average_window)]\n",
    "\n",
    "        iterations = np.arange(len(average_losses)) * average_window\n",
    "        x_iterations = np.arange(0, iterations_per_epoch * epochs,(iterations_per_epoch * epochs) / len(val_accuracy_list))\n",
    "        infos.append({\n",
    "            \"name\": training_info[\"model_name\"],\n",
    "            \"iterations\": iterations,\n",
    "            \"average_losses\": average_losses,\n",
    "            \"avg_grad_norms\": avg_grad_norms,\n",
    "            \"avg_accuracy_list\": avg_accuracy_list,\n",
    "            \"val_accuracy_list\": val_accuracy_list,\n",
    "            \"x_iterations\": x_iterations\n",
    "        })\n",
    "    \n",
    "    # Plotting the loss curve (average, with max and min as error bands)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"average_losses\"], label=info[\"name\"])\n",
    "    \n",
    "    plt.ylim(0, .5)\n",
    "    \n",
    "    plt.title('Training Loss Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the gradient norm curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"avg_grad_norms\"], label=info[\"name\"])\n",
    "    plt.title('Gradient Norm Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the accuracy curve (average, with max and min as error bars)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"avg_accuracy_list\"], label=info[\"name\"])\n",
    "    plt.title('Training Accuracy Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zoom to 0.9 to 1.0 range\n",
    "    plt.ylim(0.9, 1.0)\n",
    "    \n",
    "    # Plotting the validation accuracy curve\n",
    "    x_iterations = np.arange(0, iterations_per_epoch * epochs,(iterations_per_epoch * epochs) / len(val_accuracy_list))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"x_iterations\"], info[\"val_accuracy_list\"], label=info[\"name\"])\n",
    "    plt.title('Validation Accuracy Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zoom to 0.94 to 1.0 range\n",
    "    plt.ylim(0.94, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4144a0d-3761-4162-9c2e-17e37491c3e3",
   "metadata": {},
   "source": [
    "## Model test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a81ff2-c7a1-4b87-a3fb-362e79806cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    print(model)\n",
    "    \n",
    "    # Download and load the test data\n",
    "    testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "    test_loader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "    \n",
    "            output = model(x_batch)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, y_batch).item()\n",
    "    \n",
    "            _, argmax = torch.max(output, 1)\n",
    "            correct += (y_batch == argmax.squeeze()).float().sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Test loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9505057-0ea4-4929-a781-4c51c973d4aa",
   "metadata": {},
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a917c5-f686-4961-9b3e-45ee043d097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching ranks of layer 1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 2\u001b[0m       LeNetTNALE(), LeNetTN(), LeNet()\n\u001b[0;32m      3\u001b[0m ]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m      5\u001b[0m     x\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[3], line 76\u001b[0m, in \u001b[0;36mLeNetTNALE.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m O3 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching ranks of layer 1...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 76\u001b[0m G1, R1 \u001b[38;5;241m=\u001b[39m TN\u001b[38;5;241m.\u001b[39mtn_ale(G1, R1, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, O1, print_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     77\u001b[0m G1, R1 \u001b[38;5;241m=\u001b[39m TN\u001b[38;5;241m.\u001b[39mtn_ale(G1, R1, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, O1, print_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearching ranks of layer 2...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\TFG\\tn.py:245\u001b[0m, in \u001b[0;36mTN.tn_ale\u001b[1;34m(G0, R0, radius, iters, objective, print_iters, max_rank)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m Rp[k] \u001b[38;5;241m>\u001b[39m max_rank:\n\u001b[0;32m    244\u001b[0m     Rp[k] \u001b[38;5;241m=\u001b[39m max_rank\n\u001b[1;32m--> 245\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m TN\u001b[38;5;241m.\u001b[39mevaluate_structure(G, Rp, objective)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m h \u001b[38;5;241m>\u001b[39m evaluation:\n\u001b[0;32m    247\u001b[0m     p \u001b[38;5;241m=\u001b[39m (G, Rp)\n",
      "File \u001b[1;32m~\\Desktop\\TFG\\tn.py:281\u001b[0m, in \u001b[0;36mTN.evaluate_structure\u001b[1;34m(G, R, objective, tuning_param)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_structure\u001b[39m(G, R, objective, tuning_param\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# We do one iteration of ALS):\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     tn \u001b[38;5;241m=\u001b[39m TN(G, objective\u001b[38;5;241m.\u001b[39mshape, R)\n\u001b[1;32m--> 281\u001b[0m     err \u001b[38;5;241m=\u001b[39m TN\u001b[38;5;241m.\u001b[39mals(tn, objective, \u001b[38;5;241m0\u001b[39m, iter_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    282\u001b[0m     icr \u001b[38;5;241m=\u001b[39m tn\u001b[38;5;241m.\u001b[39mget_tn_size() \u001b[38;5;241m/\u001b[39m objective\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m icr \u001b[38;5;241m+\u001b[39m err \u001b[38;5;241m*\u001b[39m tuning_param\n",
      "File \u001b[1;32m~\\Desktop\\TFG\\tn.py:398\u001b[0m, in \u001b[0;36mTN.als\u001b[1;34m(tn, t, err, iter_num, print_iters)\u001b[0m\n\u001b[0;32m    396\u001b[0m ATA \u001b[38;5;241m=\u001b[39m cont_mat\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m cont_mat \u001b[38;5;241m+\u001b[39m lambda_reg \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(cont_mat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39mcont_mat\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mcont_mat\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    397\u001b[0m ATb \u001b[38;5;241m=\u001b[39m cont_mat\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m obj_mat\n\u001b[1;32m--> 398\u001b[0m core \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msolve(ATA, ATb)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;66;03m# core = torch.linalg.pinv(cont_mat) @ obj_mat\u001b[39;00m\n\u001b[0;32m    401\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [\n",
    "      LeNetTNALE(), LeNetTN(), LeNet()\n",
    "]\n",
    "for x in models:\n",
    "    x.to(device)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31d6d9-be51-45cc-85b1-01721ee79c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_train_data()\n",
    "epochs = 20\n",
    "training_infos = []\n",
    "show_example_batch(get_train_data())\n",
    "for model in models:\n",
    "    print(model.parameters())\n",
    "    training_infos.append(train_model(model, device, train_data, epochs=epochs))\n",
    "show_training_infos(training_infos, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7f98d-2f87-4a38-8bce-9bac7ffb69fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
