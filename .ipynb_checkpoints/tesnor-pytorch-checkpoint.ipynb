{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca940fc4-50cb-4c46-b974-56ae84eb00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import tednet.tednet.tnn.tensor_ring as tednet_tr\n",
    "import tednet.tednet.tnn.tensor_train as tednet_tt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abc58e-1093-41e3-935c-681bdabde8a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e360cc-5d40-446b-b597-80a9d16ed8f7",
   "metadata": {},
   "source": [
    "## Tensor Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c935d1-04f2-438f-a541-6c861143d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRLinearLayer(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape, ranks, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.n_info = {}\n",
    "        self.layer = tednet_tr.TRLinear(in_shape, out_shape, ranks, bias=bias)\n",
    "        self.n_info[\"ori_params\"] = self.layer.tn_info[\"ori_params\"]\n",
    "        self.n_info[\"t_params\"] = self.layer.tn_info[\"t_params\"]\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class TTLinearLayer(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape, ranks, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.n_info = {}\n",
    "        self.layer = tednet_tt.TTLinear(in_shape, out_shape, ranks, bias=bias)\n",
    "        self.n_info[\"ori_params\"] = self.layer.tn_info[\"ori_params\"]\n",
    "        self.n_info[\"t_params\"] = self.layer.tn_info[\"t_params\"]\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26989e2-c55f-41a7-a1b1-addb6b60f62d",
   "metadata": {},
   "source": [
    "## Model definitions\n",
    "We test different tensor decompositions methods by training a fully connected network model over the MNIST dataset.\n",
    "The test model has layers [784, 320, 100, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10f3b39-d795-4c26-8621-01a270b45cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LeNet-300-100 model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(784, 320),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(320, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10), # Returns logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class LeNetTR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetTR, self).__init__()\n",
    "        self.n_info = {}\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = TRLinearLayer([7,7,4,4], [5,8,8], [7,7,4,4,5,8,8], bias=True)\n",
    "        self.l2 = TRLinearLayer([5,8,8], [10, 10], [5,5,5,5,5], bias=True)\n",
    "        self.l3 = TRLinearLayer([10,10], [10], [5,5,5], bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.ReLU(),\n",
    "            self.l3, # Returns logits\n",
    "        )\n",
    "        self.n_info[\"ori_params\"] = sum([x.n_info[\"ori_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"t_params\"] = sum([x.n_info[\"t_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"cr\"] = self.n_info[\"ori_params\"] / self.n_info[\"t_params\"]\n",
    "\n",
    "        print(\"LeNet TR ---\")\n",
    "        print(\"Original params: \" + str(self.n_info[\"ori_params\"]))\n",
    "        print(\"TN params: \" + str(self.n_info[\"t_params\"]))\n",
    "        print(\"Compression ratio: \" + str(self.n_info[\"cr\"]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "class LeNetTT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetTT, self).__init__()\n",
    "        self.n_info = {}\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = TTLinearLayer([7,7,4,4], [5,4,4,4], [4,4,4], bias=True)\n",
    "        self.l2 = TTLinearLayer([5,4,4,4], [5,5,2,2], [4,4,4], bias=True)\n",
    "        self.l3 = TTLinearLayer([5,5,2,2], [1,1,1,10], [4,4,4], bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.ReLU(),\n",
    "            self.l3, # Returns logits\n",
    "        )\n",
    "        self.n_info[\"ori_params\"] = sum([x.n_info[\"ori_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"t_params\"] = sum([x.n_info[\"t_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"cr\"] = self.n_info[\"ori_params\"] / self.n_info[\"t_params\"]\n",
    "\n",
    "        print(\"LeNet TT ---\")\n",
    "        print(\"Original params: \" + str(self.n_info[\"ori_params\"]))\n",
    "        print(\"TN params: \" + str(self.n_info[\"t_params\"]))\n",
    "        print(\"Compression ratio: \" + str(self.n_info[\"cr\"]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc65f0c-c2f4-43a3-87d6-c1e3fabe7061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11912b64-a0fd-4f58-9369-9f0d46f9f6e2",
   "metadata": {},
   "source": [
    "## Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baeda3e7-08ab-400c-8369-54c6a9be7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_set(model, validation_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for x_val, y_val in validation_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            outputs = model(x_val)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def show_example_batch(train_data):\n",
    "    # Get the first batch\n",
    "    train_loader, valid_loader = train_data\n",
    "    \n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    plot_size=20\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(images[idx].numpy()), cmap='gray')\n",
    "        # print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(labels[idx].item()))\n",
    "    plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "def get_train_data():\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                  ])\n",
    "    # Download and load the training data\n",
    "    trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "    validation_size = 0.10\n",
    "    num_train = len(trainset)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    # Calculate the number of data points in the validation set\n",
    "    split = int(np.floor(validation_size * num_train))\n",
    "    print(split)\n",
    "    \n",
    "    # Train_idx => Imatges per entrenar\n",
    "    # Valid_idx => Imatges per verificar i comprovar el model\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # Create data samplers\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ced89-f2f6-4338-8993-4f7ba4987481",
   "metadata": {},
   "source": [
    "## Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f612b704-3e69-4085-88ec-d8892c90f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_data, learning_rate=1.2e-3, batch_size=60, epochs=10):\n",
    "    losses = []\n",
    "    accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    grad_norms = []\n",
    "    print(\"Training model\")\n",
    "    print(model)\n",
    "\n",
    "    train_loader, valid_loader = train_data\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Validation Calculation\n",
    "    val_check_iter = 3\n",
    "    iterations_per_epoch = len(train_loader)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            # Move data to the device\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = nn.CrossEntropyLoss()(output, y_batch)\n",
    "            loss.backward()\n",
    "    \n",
    "    \n",
    "            # Calculate and store gradient norm\n",
    "            total_grad_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "            grad_norms.append(total_grad_norm.item())\n",
    "    \n",
    "            optimizer.step()\n",
    "    \n",
    "            # Store the loss\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "            # Store the accuracy \n",
    "            _, argmax = torch.max(output, 1)\n",
    "            accuracy = (y_batch == argmax.squeeze()).float().mean()\n",
    "            accuracy_list.append(accuracy)\n",
    "    \n",
    "            if i % int(len(train_loader) / 50) == 0:\n",
    "                print(\".\", end='')\n",
    "    \n",
    "            if i % int(len(train_loader) / val_check_iter) == 0:\n",
    "                # Calculate validation accuracy at the end of each epoch\n",
    "                val_accuracy = evaluate_validation_set(model, valid_loader, device)\n",
    "                val_accuracy_list.append(val_accuracy)\n",
    "                \n",
    "                # Calculate average loss and accuracy over an epoch\n",
    "                avg_loss = torch.mean(torch.tensor(losses[-len(train_loader):]))\n",
    "                avg_accuracy = torch.mean(torch.tensor(accuracy_list[-len(train_loader):]))\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss.item():.4f}, Accuracy: {avg_accuracy.item():.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    infos = {\n",
    "        losses: losses,\n",
    "        accuracy_list: accuracy_list,\n",
    "        grad_norms: grad_norms,\n",
    "        val_accuracy_list: val_accuracy_list,\n",
    "        iterations_per_epoch: iterations_per_epoch,\n",
    "        model_name: model.__class__.__name__\n",
    "    }\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fcab5-cf45-4c20-8593-b7e9abfcd4f7",
   "metadata": {},
   "source": [
    "## Show training results function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043f520-0bff-463f-b0c1-3a0fa929151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_infos(training_infos, epochs):\n",
    "    \n",
    "    average_window = 200\n",
    "    infos = []\n",
    "    for training_info in training_infos:\n",
    "        losses = torch.tensor(training_info[\"losses\"]).cpu().numpy()\n",
    "        accuracy_list = torch.tensor(training_info[\"accuracy_list\"]).cpu().numpy()\n",
    "        grad_norms = torch.tensor(training_info[\"grad_norms\"]).cpu().numpy()\n",
    "        val_accuracy_list = training_info[\"val_accuracy_list\"]\n",
    "        iterations_per_epoch = training_info[\"iterations_per_epoch\"]\n",
    "\n",
    "        average_losses = [np.mean(losses[i-average_window:i]) for i in range(0, len(losses), average_window)]\n",
    "        avg_accuracy_list = [np.mean(accuracy_list[i-average_window:i]) for i in range(0, len(accuracy_list), average_window)]\n",
    "        avg_grad_norms = [np.mean(grad_norms[i-average_window:i]) for i in range(0, len(grad_norms), average_window)]\n",
    "\n",
    "        iterations = np.arange(len(average_losses)) * average_window\n",
    "        x_iterations = np.arange(0, iterations_per_epoch * epochs,(iterations_per_epoch * epochs) / len(val_accuracy_list))\n",
    "        infos.append({\n",
    "            name: training_info[\"model_name\"],\n",
    "            iterations: iterations,\n",
    "            average_losses: average_losses,\n",
    "            avg_grad_norms: avg_grad_norms,\n",
    "            avg_accuracy_list: avg_accuracy_list,\n",
    "            val_accuracy_list: val_accuracy_list,\n",
    "            x_iterations: x_iterations\n",
    "        })\n",
    "    \n",
    "    # Plotting the loss curve (average, with max and min as error bands)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"average_losses\"], label=info[\"model_name\"])\n",
    "    \n",
    "    plt.ylim(0, .5)\n",
    "    \n",
    "    plt.title('Training Loss Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the gradient norm curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"avg_grad_norms\"], label=info[\"model_name\"])\n",
    "    plt.title('Gradient Norm Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the accuracy curve (average, with max and min as error bars)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"avg_accuracy_list\"], label=info[\"model_name\"])\n",
    "    plt.title('Training Accuracy Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zoom to 0.9 to 1.0 range\n",
    "    plt.ylim(0.9, 1.0)\n",
    "    \n",
    "    # Plotting the validation accuracy curve\n",
    "    x_iterations = np.arange(0, iterations_per_epoch * epochs,(iterations_per_epoch * epochs) / len(val_accuracy_list))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"x_iterations\"], info[\"val_accuracy_list\"], label=info[\"model_name\"])\n",
    "    plt.title('Validation Accuracy Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zoom to 0.94 to 1.0 range\n",
    "    plt.ylim(0.94, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4144a0d-3761-4162-9c2e-17e37491c3e3",
   "metadata": {},
   "source": [
    "## Model test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a81ff2-c7a1-4b87-a3fb-362e79806cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    print(model)\n",
    "    \n",
    "    # Download and load the test data\n",
    "    testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "    test_loader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "    \n",
    "            output = model(x_batch)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, y_batch).item()\n",
    "    \n",
    "            _, argmax = torch.max(output, 1)\n",
    "            correct += (y_batch == argmax.squeeze()).float().sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Test loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9505057-0ea4-4929-a781-4c51c973d4aa",
   "metadata": {},
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16a917c5-f686-4961-9b3e-45ee043d097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet TR ---\n",
      "Original params: 283880\n",
      "TN params: 3618\n",
      "Compression ratio: 78.46323935876174\n",
      "compression_ration is:  276.2995594713656\n",
      "compression_ration is:  55.172413793103445\n",
      "compression_ration is:  4.716981132075472\n",
      "LeNet TT ---\n",
      "Original params: 283880\n",
      "TN params: 1700\n",
      "Compression ratio: 166.98823529411766\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LeNet(), LeNetTR(), LeNetTT()\n",
    "]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for x in models:\n",
    "    x.to(device)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d31d6d9-be51-45cc-85b1-01721ee79c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "Training model\n",
      "LeNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=320, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=320, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      ".....................................................Epoch 1/20, Loss: 0.2325, Accuracy: 0.9285, Val Accuracy: 0.9585\n",
      "....................................................Epoch 2/20, Loss: 0.0963, Accuracy: 0.9704, Val Accuracy: 0.9710\n",
      "....................................................Epoch 3/20, Loss: 0.0647, Accuracy: 0.9797, Val Accuracy: 0.9753\n",
      "....................................................Epoch 4/20, Loss: 0.0515, Accuracy: 0.9838, Val Accuracy: 0.9745\n",
      "....................................................Epoch 5/20, Loss: 0.0422, Accuracy: 0.9860, Val Accuracy: 0.9785\n",
      "....................................................Epoch 6/20, Loss: 0.0352, Accuracy: 0.9890, Val Accuracy: 0.9727\n",
      "....................................................Epoch 7/20, Loss: 0.0304, Accuracy: 0.9896, Val Accuracy: 0.9723\n",
      "....................................................Epoch 8/20, Loss: 0.0273, Accuracy: 0.9910, Val Accuracy: 0.9663\n",
      "....................................................Epoch 9/20, Loss: 0.0258, Accuracy: 0.9911, Val Accuracy: 0.9830\n",
      "....."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m training_infos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m----> 5\u001b[0m     training_infos\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m show_training_infos(training_infos, epochs)\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, device, train_data, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m iterations_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move data to the device\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torchvision/datasets/mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torchvision/transforms/functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/torchvision/transforms/_functional_tensor.py:921\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    919\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    920\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 921\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (std \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstd evaluated to zero after conversion to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, leading to division by zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = get_train_data()\n",
    "epochs = 20\n",
    "training_infos = []\n",
    "for model in models:\n",
    "    training_infos.append(train_model(model, device, train_data, epochs=epochs))\n",
    "show_training_infos(training_infos, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
