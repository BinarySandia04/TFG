{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e27c376-71cb-4dc5-a2ab-ffe845773b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb89c2d-c06c-417f-a3cd-ae747300bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# large figures\n",
    "rcParams['figure.figsize'] = 8, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6752a726-6778-4e64-b8a5-6f2f850f3d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:04<00:00, 2.01MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\train-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 300kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:05<00:00, 296kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST\\raw\n",
      "\n",
      "Training dataset shape:  torch.Size([60000, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arans\\miniconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST from Pytorch\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "\n",
    "# Define the size of the validation set\n",
    "validation_size = 0.10\n",
    "\n",
    "# Get the number of data points\n",
    "num_train = len(trainset)\n",
    "\n",
    "# List of indices from 0 to num_train\n",
    "indices = list(range(num_train))\n",
    "\n",
    "# Shuffle the indices\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Calculate the number of data points in the validation set\n",
    "split = int(np.floor(validation_size * num_train))\n",
    "\n",
    "# Split indices into training and validation sets\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Create data samplers\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "# Get the first batch\n",
    "dataiter = iter(train_loader)\n",
    "\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx].numpy()), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))\n",
    "plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "\n",
    "# Print shape of training dataset\n",
    "print(\"Training dataset shape: \", trainset.train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6641ba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for training\n",
    "learning_rate = 1.2e-3\n",
    "batch_size = 60\n",
    "epochs = 50 \n",
    "\n",
    "# Validation Calculation\n",
    "val_check_iter = 3\n",
    "iterations_per_epoch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205e27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_set(model, validation_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for x_val, y_val in validation_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            outputs = model(x_val)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b900c41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([48, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      ".torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n",
      "torch.Size([64, 784])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m/\u001b[39m val_check_iter) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Calculate validation accuracy at the end of each epoch\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     val_accuracy \u001b[38;5;241m=\u001b[39m evaluate_validation_set(model, valid_loader, device)\n\u001b[0;32m     72\u001b[0m     val_accuracy_list\u001b[38;5;241m.\u001b[39mappend(val_accuracy)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# Calculate average loss and accuracy over an epoch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36mevaluate_validation_set\u001b[1;34m(model, validation_loader, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient computation\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x_val, y_val \u001b[38;5;129;01min\u001b[39;00m validation_loader:\n\u001b[0;32m      7\u001b[0m         x_val \u001b[38;5;241m=\u001b[39m x_val\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m         y_val \u001b[38;5;241m=\u001b[39m y_val\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mnormalize(tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mnormalize(tensor, mean\u001b[38;5;241m=\u001b[39mmean, std\u001b[38;5;241m=\u001b[39mstd, inplace\u001b[38;5;241m=\u001b[39minplace)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39msub_(mean)\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the LeNet-300-100 model\n",
    "class LeNet300100(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet300100, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(784, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10), # Returns logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = LeNet300100()\n",
    "\n",
    "# Move the model to the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Print the model summary (We'll use torchsummary for a detailed summary)\n",
    "#from torchinfo import summary\n",
    "#print(summary(model, input_size=(784,)))\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize lists to store losses and gradient norms\n",
    "losses = []\n",
    "accuracy_list = []\n",
    "val_accuracy_list = []\n",
    "grad_norms = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Move data to the device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = nn.CrossEntropyLoss()(output, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # Calculate and store gradient norm\n",
    "        total_grad_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "        grad_norms.append(total_grad_norm.item())\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Store the accuracy \n",
    "        _, argmax = torch.max(output, 1)\n",
    "        accuracy = (y_batch == argmax.squeeze()).float().mean()\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "        if i % int(len(train_loader) / 50) == 0:\n",
    "            print(\".\", end='')\n",
    "\n",
    "        if i % int(len(train_loader) / val_check_iter) == 0:\n",
    "            # Calculate validation accuracy at the end of each epoch\n",
    "            val_accuracy = evaluate_validation_set(model, valid_loader, device)\n",
    "            val_accuracy_list.append(val_accuracy)\n",
    "            \n",
    "            # Calculate average loss and accuracy over an epoch\n",
    "            avg_loss = torch.mean(torch.tensor(losses[-len(train_loader):]))\n",
    "            avg_accuracy = torch.mean(torch.tensor(accuracy_list[-len(train_loader):]))\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss.item():.4f}, Accuracy: {avg_accuracy.item():.4f}, Val Accuracy: {val_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee712373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the tensor to CPU and convert to numpy datatype\n",
    "losses = torch.tensor(losses).cpu().numpy()\n",
    "accuracy_list = torch.tensor(accuracy_list).cpu().numpy()\n",
    "grad_norms = torch.tensor(grad_norms).cpu().numpy()\n",
    "\n",
    "# Create averages of size 200\n",
    "average_window = 200\n",
    "average_losses = [np.mean(losses[i-average_window:i]) for i in range(0, len(losses), average_window)]\n",
    "avg_accuracy_list = [np.mean(accuracy_list[i-average_window:i]) for i in range(0, len(accuracy_list), average_window)]\n",
    "avg_grad_norms = [np.mean(grad_norms[i-average_window:i]) for i in range(0, len(grad_norms), average_window)]\n",
    "\n",
    "# Create an x-axis variable for plotting\n",
    "iterations = np.arange(len(average_losses)) * average_window\n",
    "\n",
    "# Plotting the loss curve (average, with max and min as error bands)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iterations, average_losses, label='Average')\n",
    "\n",
    "plt.ylim(0, .5)\n",
    "\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the gradient norm curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(iterations, avg_grad_norms, label='Average')\n",
    "plt.title('Gradient Norm Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting the accuracy curve (average, with max and min as error bars)\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iterations, avg_accuracy_list, label='Average')\n",
    "plt.title('Training Accuracy Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Zoom to 0.94 to 1.0 range\n",
    "plt.ylim(0.94, 1.0)\n",
    "\n",
    "# Plotting the validation accuracy curve\n",
    "x_iterations = np.arange(0, iterations_per_epoch * epochs,(iterations_per_epoch * epochs) /len(val_accuracy_list))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_iterations, val_accuracy_list, label='Average')\n",
    "plt.title('Validation Accuracy Curve')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Zoom to 0.94 to 1.0 range\n",
    "plt.ylim(0.94, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "print(model)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        output = model(x_batch)\n",
    "        test_loss += nn.CrossEntropyLoss()(output, y_batch).item()\n",
    "\n",
    "        _, argmax = torch.max(output, 1)\n",
    "        correct += (y_batch == argmax.squeeze()).float().sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "print(f'Test loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29834166-b5f9-48f8-8c8a-3c38802236dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(\"NAME:\", name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fa8bc",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
