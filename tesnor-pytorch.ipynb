{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca940fc4-50cb-4c46-b974-56ae84eb00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import numpy as np\n",
    "import tednet.tednet.tnn.tensor_ring as tednet_tr\n",
    "import tednet.tednet.tnn.tensor_train as tednet_tt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abc58e-1093-41e3-935c-681bdabde8a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e360cc-5d40-446b-b597-80a9d16ed8f7",
   "metadata": {},
   "source": [
    "## Tensor Layer definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40c935d1-04f2-438f-a541-6c861143d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRLinearLayer(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape, ranks, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.n_info = {}\n",
    "        self.layer = tednet_tr.TRLinear(in_shape, out_shape, ranks, bias=bias)\n",
    "        self.n_info[\"ori_params\"] = self.layer.tn_info[\"ori_params\"]\n",
    "        self.n_info[\"t_params\"] = self.layer.tn_info[\"t_params\"]\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class TTLinearLayer(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape, ranks, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.n_info = {}\n",
    "        self.layer = tednet_tt.TTLinear(in_shape, out_shape, ranks, bias=bias)\n",
    "        self.n_info[\"ori_params\"] = self.layer.tn_info[\"ori_params\"]\n",
    "        self.n_info[\"t_params\"] = self.layer.tn_info[\"t_params\"]\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26989e2-c55f-41a7-a1b1-addb6b60f62d",
   "metadata": {},
   "source": [
    "## Model definitions\n",
    "We test different tensor decompositions methods by training a fully connected network model over the MNIST dataset.\n",
    "The test model has layers [784, 320, 100, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b10f3b39-d795-4c26-8621-01a270b45cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LeNet-300-100 model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(784, 320),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(320, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 10), # Returns logits\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class LeNetTR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetTR, self).__init__()\n",
    "        self.n_info = {}\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = TRLinearLayer([7,7,4,4], [5,8,8], [7,7,4,4,5,8,8], bias=True)\n",
    "        self.l2 = TRLinearLayer([5,8,8], [10, 10], [5,5,5,5,5], bias=True)\n",
    "        self.l3 = TRLinearLayer([10,10], [10], [5,5,5], bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.ReLU(),\n",
    "            self.l3, # Returns logits\n",
    "        )\n",
    "        self.n_info[\"ori_params\"] = sum([x.n_info[\"ori_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"t_params\"] = sum([x.n_info[\"t_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"cr\"] = self.n_info[\"ori_params\"] / self.n_info[\"t_params\"]\n",
    "\n",
    "        print(\"LeNet TR ---\")\n",
    "        print(\"Original params: \" + str(self.n_info[\"ori_params\"]))\n",
    "        print(\"TN params: \" + str(self.n_info[\"t_params\"]))\n",
    "        print(\"Compression ratio: \" + str(self.n_info[\"cr\"]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        \n",
    "class LeNetTT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNetTT, self).__init__()\n",
    "        self.n_info = {}\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = TTLinearLayer([7,7,4,4], [5,4,4,4], [4,4,4], bias=True)\n",
    "        self.l2 = TTLinearLayer([5,4,4,4], [5,5,2,2], [4,4,4], bias=True)\n",
    "        self.l3 = TTLinearLayer([5,5,2,2], [1,1,1,10], [4,4,4], bias=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.ReLU(),\n",
    "            self.l3, # Returns logits\n",
    "        )\n",
    "        self.n_info[\"ori_params\"] = sum([x.n_info[\"ori_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"t_params\"] = sum([x.n_info[\"t_params\"] for x in [self.l1, self.l2, self.l3]])\n",
    "        self.n_info[\"cr\"] = self.n_info[\"ori_params\"] / self.n_info[\"t_params\"]\n",
    "\n",
    "        print(\"LeNet TT ---\")\n",
    "        print(\"Original params: \" + str(self.n_info[\"ori_params\"]))\n",
    "        print(\"TN params: \" + str(self.n_info[\"t_params\"]))\n",
    "        print(\"Compression ratio: \" + str(self.n_info[\"cr\"]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc65f0c-c2f4-43a3-87d6-c1e3fabe7061",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11912b64-a0fd-4f58-9369-9f0d46f9f6e2",
   "metadata": {},
   "source": [
    "## Training helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baeda3e7-08ab-400c-8369-54c6a9be7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_set(model, validation_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for x_val, y_val in validation_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "\n",
    "            outputs = model(x_val)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_val.size(0)\n",
    "            correct += (predicted == y_val).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def show_example_batch(train_data):\n",
    "    # Get the first batch\n",
    "    train_loader, valid_loader = train_data\n",
    "    \n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Plot the images in the batch, along with the corresponding labels\n",
    "    fig = plt.figure(figsize=(25, 4))\n",
    "    plot_size=20\n",
    "    for idx in np.arange(plot_size):\n",
    "        ax = fig.add_subplot(2, int(plot_size/2), idx+1, xticks=[], yticks=[])\n",
    "        ax.imshow(np.squeeze(images[idx].numpy()), cmap='gray')\n",
    "        # print out the correct label for each image\n",
    "        # .item() gets the value contained in a Tensor\n",
    "        ax.set_title(str(labels[idx].item()))\n",
    "    plt.imshow(images[0].numpy().squeeze(), cmap='gray_r')\n",
    "def get_train_data():\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                  transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                                  ])\n",
    "    # Download and load the training data\n",
    "    trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "    validation_size = 0.10\n",
    "    num_train = len(trainset)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    # Calculate the number of data points in the validation set\n",
    "    split = int(np.floor(validation_size * num_train))\n",
    "    print(split)\n",
    "    \n",
    "    # Train_idx => Imatges per entrenar\n",
    "    # Valid_idx => Imatges per verificar i comprovar el model\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # Create data samplers\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=valid_sampler)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ced89-f2f6-4338-8993-4f7ba4987481",
   "metadata": {},
   "source": [
    "## Train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f612b704-3e69-4085-88ec-d8892c90f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, train_data, learning_rate=1.2e-3, batch_size=60, epochs=10):\n",
    "    losses = []\n",
    "    accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    grad_norms = []\n",
    "    print(\"Training model\")\n",
    "    print(model)\n",
    "\n",
    "    train_loader, valid_loader = train_data\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Validation Calculation\n",
    "    val_check_iter = 3\n",
    "    iterations_per_epoch = len(train_loader)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            # Move data to the device\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_batch)\n",
    "            loss = nn.CrossEntropyLoss()(output, y_batch)\n",
    "            loss.backward()\n",
    "    \n",
    "    \n",
    "            # Calculate and store gradient norm\n",
    "            total_grad_norm = torch.sqrt(sum(p.grad.norm()**2 for p in model.parameters() if p.grad is not None))\n",
    "            grad_norms.append(total_grad_norm.item())\n",
    "    \n",
    "            optimizer.step()\n",
    "    \n",
    "            # Store the loss\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "            # Store the accuracy \n",
    "            _, argmax = torch.max(output, 1)\n",
    "            accuracy = (y_batch == argmax.squeeze()).float().mean()\n",
    "            accuracy_list.append(accuracy)\n",
    "    \n",
    "            if i % int(len(train_loader) / 50) == 0:\n",
    "                print(\".\", end='')\n",
    "    \n",
    "            if i % int(len(train_loader) / val_check_iter) == 0:\n",
    "                # Calculate validation accuracy at the end of each epoch\n",
    "                val_accuracy = evaluate_validation_set(model, valid_loader, device)\n",
    "                val_accuracy_list.append(val_accuracy)\n",
    "                \n",
    "                # Calculate average loss and accuracy over an epoch\n",
    "                avg_loss = torch.mean(torch.tensor(losses[-len(train_loader):]))\n",
    "                avg_accuracy = torch.mean(torch.tensor(accuracy_list[-len(train_loader):]))\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss.item():.4f}, Accuracy: {avg_accuracy.item():.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "    infos = {\n",
    "        losses: losses,\n",
    "        accuracy_list: accuracy_list,\n",
    "        grad_norms: grad_norms,\n",
    "        val_accuracy_list: val_accuracy_list,\n",
    "        iterations_per_epoch: iterations_per_epoch,\n",
    "        model_name: model.__class__.__name__\n",
    "    }\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fcab5-cf45-4c20-8593-b7e9abfcd4f7",
   "metadata": {},
   "source": [
    "## Show training results function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7043f520-0bff-463f-b0c1-3a0fa929151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_training_infos(training_infos, epochs):\n",
    "    \n",
    "    average_window = 200\n",
    "    infos = []\n",
    "    for training_info in training_infos:\n",
    "        losses = torch.tensor(training_info[\"losses\"]).cpu().numpy()\n",
    "        accuracy_list = torch.tensor(training_info[\"accuracy_list\"]).cpu().numpy()\n",
    "        grad_norms = torch.tensor(training_info[\"grad_norms\"]).cpu().numpy()\n",
    "        val_accuracy_list = training_info[\"val_accuracy_list\"]\n",
    "        iterations_per_epoch = training_info[\"iterations_per_epoch\"]\n",
    "\n",
    "        average_losses = [np.mean(losses[i-average_window:i]) for i in range(0, len(losses), average_window)]\n",
    "        avg_accuracy_list = [np.mean(accuracy_list[i-average_window:i]) for i in range(0, len(accuracy_list), average_window)]\n",
    "        avg_grad_norms = [np.mean(grad_norms[i-average_window:i]) for i in range(0, len(grad_norms), average_window)]\n",
    "\n",
    "        iterations = np.arange(len(average_losses)) * average_window\n",
    "        x_iterations = np.arange(0, iterations_per_epoch * epochs,(iterations_per_epoch * epochs) / len(val_accuracy_list))\n",
    "        infos.append({\n",
    "            name: training_info[\"model_name\"],\n",
    "            iterations: iterations,\n",
    "            average_losses: average_losses,\n",
    "            avg_grad_norms: avg_grad_norms,\n",
    "            avg_accuracy_list: avg_accuracy_list,\n",
    "            val_accuracy_list: val_accuracy_list,\n",
    "            x_iterations: x_iterations\n",
    "        })\n",
    "    \n",
    "    # Plotting the loss curve (average, with max and min as error bands)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"average_losses\"], label=info[\"model_name\"])\n",
    "    \n",
    "    plt.ylim(0, .5)\n",
    "    \n",
    "    plt.title('Training Loss Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the gradient norm curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"avg_grad_norms\"], label=info[\"model_name\"])\n",
    "    plt.title('Gradient Norm Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plotting the accuracy curve (average, with max and min as error bars)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"iterations\"], info[\"avg_accuracy_list\"], label=info[\"model_name\"])\n",
    "    plt.title('Training Accuracy Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zoom to 0.9 to 1.0 range\n",
    "    plt.ylim(0.9, 1.0)\n",
    "    \n",
    "    # Plotting the validation accuracy curve\n",
    "    x_iterations = np.arange(0, iterations_per_epoch * epochs,(iterations_per_epoch * epochs) / len(val_accuracy_list))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for info in infos:\n",
    "        plt.plot(info[\"x_iterations\"], info[\"val_accuracy_list\"], label=info[\"model_name\"])\n",
    "    plt.title('Validation Accuracy Curve Average')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Zoom to 0.94 to 1.0 range\n",
    "    plt.ylim(0.94, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4144a0d-3761-4162-9c2e-17e37491c3e3",
   "metadata": {},
   "source": [
    "## Model test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a81ff2-c7a1-4b87-a3fb-362e79806cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def test_model(model):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    print(model)\n",
    "    \n",
    "    # Download and load the test data\n",
    "    testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "    test_loader = DataLoader(testset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "    \n",
    "            output = model(x_batch)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, y_batch).item()\n",
    "    \n",
    "            _, argmax = torch.max(output, 1)\n",
    "            correct += (y_batch == argmax.squeeze()).float().sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print(f'Test loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9505057-0ea4-4929-a781-4c51c973d4aa",
   "metadata": {},
   "source": [
    "# Actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16a917c5-f686-4961-9b3e-45ee043d097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet TR ---\n",
      "Original params: 283880\n",
      "TN params: 3618\n",
      "Compression ratio: 78.46323935876174\n",
      "compression_ration is:  276.2995594713656\n",
      "compression_ration is:  55.172413793103445\n",
      "compression_ration is:  4.716981132075472\n",
      "LeNet TT ---\n",
      "Original params: 283880\n",
      "TN params: 1700\n",
      "Compression ratio: 166.98823529411766\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LeNet(), LeNetTR(), LeNetTT()\n",
    "]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for x in models:\n",
    "    x.to(device)\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31d6d9-be51-45cc-85b1-01721ee79c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "Training model\n",
      "LeNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=320, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=320, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      ".....................................................Epoch 1/20, Loss: 0.2291, Accuracy: 0.9300, Val Accuracy: 0.9597\n",
      "....................................................Epoch 2/20, Loss: 0.0928, Accuracy: 0.9715, Val Accuracy: 0.9673\n",
      "....................................................Epoch 3/20, Loss: 0.0681, Accuracy: 0.9780, Val Accuracy: 0.9675\n",
      "....................................................Epoch 4/20, Loss: 0.0516, Accuracy: 0.9836, Val Accuracy: 0.9730\n",
      "....................................................Epoch 5/20, Loss: 0.0412, Accuracy: 0.9862, Val Accuracy: 0.9738\n",
      "....................................................Epoch 6/20, Loss: 0.0342, Accuracy: 0.9881, Val Accuracy: 0.9778\n",
      "....................................................Epoch 7/20, Loss: 0.0315, Accuracy: 0.9894, Val Accuracy: 0.9773\n",
      "....................................................Epoch 8/20, Loss: 0.0241, Accuracy: 0.9920, Val Accuracy: 0.9725\n",
      "....................................................Epoch 9/20, Loss: 0.0253, Accuracy: 0.9917, Val Accuracy: 0.9765\n",
      "....................................................Epoch 10/20, Loss: 0.0197, Accuracy: 0.9932, Val Accuracy: 0.9795\n",
      "...................................................."
     ]
    }
   ],
   "source": [
    "train_data = get_train_data()\n",
    "epochs = 20\n",
    "training_infos = []\n",
    "for model in models:\n",
    "    training_infos.append(train_model(model, device, train_data, epochs=epochs))\n",
    "show_training_infos(training_infos, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
